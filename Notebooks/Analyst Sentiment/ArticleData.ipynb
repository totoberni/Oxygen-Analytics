{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a1008709",
   "metadata": {},
   "source": [
    "# Advanced Financial NLP Pipeline: AAPL, NVDA, GOOGL\n",
    "\n",
    "This notebook implements the multi-feature sentiment analysis pipeline as defined in `plan.md`. We will analyze `AAPL`, `NVDA`, and `GOOGL` for the most recent 12-month period available through the Finnhub API.\n",
    "\n",
    "### Notebook Workflow\n",
    "\n",
    "1.  **Phase 1: Environment Setup**\n",
    "    - Import libraries (`finnhub`, `pandas`, `transformers`, etc.).\n",
    "    - Initialize the Finnhub API client.\n",
    "\n",
    "2.  **Phase 2: Data Acquisition**\n",
    "    - Fetch company news (headlines, summaries) from the `company-news` endpoint.\n",
    "    - Fetch insider transaction sentiment (MSPR) from the `stock/insider-sentiment` endpoint.\n",
    "\n",
    "3.  **Phase 3: NLP Sentiment Analysis**\n",
    "    - Load a pre-trained `FinBERT` model for sentiment analysis.\n",
    "    - Calculate and apply sentiment scores to news headlines and summaries.\n",
    "\n",
    "4.  **Phase 4: Data Aggregation & Consolidation**\n",
    "    - Merge the news and insider sentiment data into a single DataFrame.\n",
    "    - Resample the combined data into a final quarterly format.\n",
    "    - Generate the aggregated DataFrame containing mean sentiment scores, news volume, and insider sentiment metrics."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c9349dd",
   "metadata": {},
   "source": [
    "## Phase 1: Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a90467d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0: Library Installation\n",
      "------------------------------\n",
      "Required libraries installed successfully.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.2.1 -> 25.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "print(\"1.0: Library Installation\")\n",
    "print(\"-\"*30)\n",
    "# To ensure packages install into the correct kernel environment, we explicitly use\n",
    "# the 'sys.executable' to call pip. This avoids issues where '!pip' might\n",
    "# point to a different Python installation.\n",
    "import sys\n",
    "\n",
    "# NOTE: To run this on GColab, you can also use the %pip magic command instead of !{sys.executable} -m pip\n",
    "\n",
    "# Consolidated installation of all required libraries\n",
    "!{sys.executable} -m pip install finnhub-python pandas seaborn matplotlib numpy datasets kaggle python-dotenv --quiet\n",
    "\n",
    "print(\"Required libraries installed successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7e71de03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.1: Library Imports\n",
      "------------------------------\n",
      "Core libraries imported successfully.\n"
     ]
    }
   ],
   "source": [
    "print(\"1.1: Library Imports\")\n",
    "print(\"-\"*30)\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import finnhub as fi\n",
    "import matplotlib.pyplot as plt\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "import json\n",
    "from datetime import datetime\n",
    "\n",
    "print(\"Core libraries imported successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0ae308c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.2: Finnhub Client Initialization\n",
      "------------------------------\n",
      "Finnhub client initialized.\n",
      "Successfully fetched company profile for: Apple Inc\n"
     ]
    }
   ],
   "source": [
    "print(\"1.2: Finnhub Client Initialization\")\n",
    "print(\"-\"*30)\n",
    "# --- Secure API Key Management ---\n",
    "# It is a security best practice to store your API key as an environment variable\n",
    "# to avoid exposing it directly in the code.\n",
    "\n",
    "# Before running this cell, set the 'FINNHUB_API_KEY' in your environment.\n",
    "# For example, in your terminal:\n",
    "# export FINNHUB_API_KEY='your_api_key_here'\n",
    "# You will need to restart your notebook's kernel after setting the variable.\n",
    "\n",
    "api_key = \"d3r0knpr01qna05k8e40d3r0knpr01qna05k8e4g\"\n",
    "\n",
    "if not api_key:\n",
    "    print(\"API key not found in environment variables. Please set 'FINNHUB_API_KEY'.\")\n",
    "    # You can temporarily hardcode your key here for testing, but it is not recommended for production.\n",
    "    # api_key = \"YOUR_API_KEY_HERE\" \n",
    "    finnhub_client = None\n",
    "else:\n",
    "    finnhub_client = fi.Client(api_key=api_key)\n",
    "    print(\"Finnhub client initialized.\")\n",
    "    # --- Test API Client ---\n",
    "    # Optional: Test the client with a simple, free API call to ensure it's working.\n",
    "    try:\n",
    "        profile = finnhub_client.company_profile2(symbol='AAPL')\n",
    "        print(f\"Successfully fetched company profile for: {profile.get('name', 'AAPL')}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Client may be initialized, but a test API call failed: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e5a6fd1",
   "metadata": {},
   "source": [
    "## Phase 2: Data Gathering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e3acc75",
   "metadata": {},
   "source": [
    "### Strategy 2.1 : Historical `Insider Sentiment`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1f73d930",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.1.0: Global Configuration\n",
      "------------------------------\n",
      "Global configuration loaded:\n",
      "Tickers: ['AAPL', 'NVDA', 'GOOGL']\n",
      "Date Range: 2018-2024\n"
     ]
    }
   ],
   "source": [
    "print(\"2.1.0: Global Configuration\")\n",
    "print(\"-\"*30)\n",
    "\n",
    "# --- Configuration ---\n",
    "# Tickers for the companies we are analyzing.\n",
    "STOCKS = ['AAPL', 'NVDA', 'GOOGL']\n",
    "\n",
    "# --- Date Range for Long-Term Data (2018-2024) ---\n",
    "# This range applies to all data sources.\n",
    "START_YEAR = 2018\n",
    "END_YEAR = 2024\n",
    "\n",
    "print(\"Global configuration loaded:\")\n",
    "print(f\"Tickers: {STOCKS}\")\n",
    "print(f\"Date Range: {START_YEAR}-{END_YEAR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0756f904",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.1.1: Long-Term Data Extraction (Insider Sentiment)\n",
      "------------------------------\n",
      "Fetching long-term insider sentiment from 2018 to 2024...\n",
      "  > Processing AAPL...\n",
      "    - Found 10 records for 2018.\n",
      "    - Found 10 records for 2019.\n",
      "    - Found 9 records for 2020.\n",
      "    - Found 8 records for 2021.\n",
      "    - Found 9 records for 2022.\n",
      "    - Found 7 records for 2023.\n",
      "    - Found 8 records for 2024.\n",
      "  > Processing NVDA...\n",
      "    - Found 8 records for 2018.\n",
      "    - Found 10 records for 2019.\n",
      "    - Found 10 records for 2020.\n",
      "    - Found 10 records for 2021.\n",
      "    - Found 8 records for 2022.\n",
      "    - Found 10 records for 2023.\n",
      "    - Found 8 records for 2024.\n",
      "  > Processing GOOGL...\n",
      "    - Found 9 records for 2018.\n",
      "    - Found 7 records for 2019.\n",
      "    - Found 12 records for 2020.\n",
      "    - Found 12 records for 2021.\n",
      "    - Found 12 records for 2022.\n",
      "    - Found 12 records for 2023.\n",
      "    - Found 8 records for 2024.\n",
      "\n",
      "Long-term insider sentiment fetching complete.\n"
     ]
    }
   ],
   "source": [
    "print(\"2.1.1: Long-Term Data Extraction (Insider Sentiment)\")\n",
    "print(\"-\"*30)\n",
    "\n",
    "# --- Data Storage ---\n",
    "all_insider_data = []\n",
    "\n",
    "print(f\"Fetching long-term insider sentiment from {START_YEAR} to {END_YEAR}...\")\n",
    "\n",
    "# --- Fetch Data for Each Stock and Year ---\n",
    "for stock in STOCKS:\n",
    "    print(f\"  > Processing {stock}...\")\n",
    "    for year in range(START_YEAR, END_YEAR + 1):\n",
    "        start_date = f\"{year}-01-01\"\n",
    "        end_date = f\"{year}-12-31\"\n",
    "        try:\n",
    "            insider_sentiment = finnhub_client.stock_insider_sentiment(stock, _from=start_date, to=end_date)\n",
    "            insider_transactions = insider_sentiment.get('data', [])\n",
    "            for item in insider_transactions:\n",
    "                report_date = datetime(year=item['year'], month=item['month'], day=1).date()\n",
    "                all_insider_data.append({\n",
    "                    'ticker': stock,\n",
    "                    'date': report_date,\n",
    "                    'mspr': item['mspr'],\n",
    "                    'change': item['change']\n",
    "                })\n",
    "            # A small confirmation to show progress.\n",
    "            if insider_transactions:\n",
    "                print(f\"    - Found {len(insider_transactions)} records for {year}.\")\n",
    "        except Exception as e:\n",
    "            print(f\"    - Error fetching insider sentiment for {stock} in {year}: {e}\")\n",
    "\n",
    "print(\"\\nLong-term insider sentiment fetching complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "99c3a727",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.1.2: Create Company-Specific Insider DataFrames\n",
      "------------------------------\n",
      "Processing insider data for each target ticker...\n",
      "\n",
      "Successfully created and structured insider sentiment DataFrames for all tickers.\n",
      "DataFrames are stored in the 'insider_datasets' dictionary.\n"
     ]
    }
   ],
   "source": [
    "print(\"2.1.2: Create Company-Specific Insider DataFrames\")\n",
    "print(\"-\"*30)\n",
    "\n",
    "# This cell refactors the insider sentiment data into separate, clean\n",
    "# DataFrames for each company, formatted for time series analysis.\n",
    "\n",
    "# Create a temporary DataFrame from the raw collected data\n",
    "insider_df = pd.DataFrame(all_insider_data)\n",
    "\n",
    "# Dictionary to hold the final, structured DataFrames for each company\n",
    "# NOTE: We can access the selevant dataset by calling `insider_datasets[\"AAPL\"].head()` for example\n",
    "insider_datasets = {}\n",
    "\n",
    "if not insider_df.empty:\n",
    "    # Convert 'date' column to datetime objects for manipulation\n",
    "    insider_df['date'] = pd.to_datetime(insider_df['date'])\n",
    "\n",
    "    # Engineer the 'Period' column in the specified 'YYYY-Q' format\n",
    "    insider_df['Period'] = insider_df['date'].dt.year.astype(str) + '-Q' + insider_df['date'].dt.quarter.astype(str)\n",
    "    \n",
    "    print(\"Processing insider data for each target ticker...\")\n",
    "    # Iterate through the globally defined STOCKS list to create a DF for each\n",
    "    for ticker in STOCKS:\n",
    "        # Filter data for the current ticker\n",
    "        ticker_specific_df = insider_df[insider_df['ticker'] == ticker].copy()\n",
    "\n",
    "        if not ticker_specific_df.empty:\n",
    "            # Select, rename, and sort the columns to match the desired format\n",
    "            final_df = ticker_specific_df[['Period', 'mspr']].rename(columns={'mspr': 'MSPR'})\n",
    "            final_df = final_df.sort_values(by='Period').reset_index(drop=True)\n",
    "            \n",
    "            # Store the processed DataFrame in the dictionary\n",
    "            insider_datasets[ticker] = final_df \n",
    "        else:\n",
    "            print(f\"No insider sentiment data was found for {ticker}\")\n",
    "    \n",
    "    print(\"\\nSuccessfully created and structured insider sentiment DataFrames for all tickers.\")\n",
    "    print(\"DataFrames are stored in the 'insider_datasets' dictionary.\")\n",
    "\n",
    "else:\n",
    "    print(\"The initial 'all_insider_data' list is empty. No DataFrames were created.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9f06146e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.1.2: Display Company-Specific Insider DataFrames\n",
      "------------------------------\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Period</th>\n",
       "      <th>MSPR</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2018-Q1</td>\n",
       "      <td>-100.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2018-Q1</td>\n",
       "      <td>7.840257</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2018-Q2</td>\n",
       "      <td>-22.737514</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2018-Q2</td>\n",
       "      <td>-54.728600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2018-Q2</td>\n",
       "      <td>-33.333332</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Period        MSPR\n",
       "0  2018-Q1 -100.000000\n",
       "1  2018-Q1    7.840257\n",
       "2  2018-Q2  -22.737514\n",
       "3  2018-Q2  -54.728600\n",
       "4  2018-Q2  -33.333332"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Period</th>\n",
       "      <th>MSPR</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2018-Q1</td>\n",
       "      <td>-48.497414</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2018-Q1</td>\n",
       "      <td>-39.062780</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2018-Q2</td>\n",
       "      <td>-87.439430</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2018-Q2</td>\n",
       "      <td>-62.332840</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2018-Q3</td>\n",
       "      <td>-100.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Period        MSPR\n",
       "0  2018-Q1  -48.497414\n",
       "1  2018-Q1  -39.062780\n",
       "2  2018-Q2  -87.439430\n",
       "3  2018-Q2  -62.332840\n",
       "4  2018-Q3 -100.000000"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Period</th>\n",
       "      <th>MSPR</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2018-Q2</td>\n",
       "      <td>-33.796238</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2018-Q2</td>\n",
       "      <td>-52.667397</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2018-Q2</td>\n",
       "      <td>-48.464813</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2018-Q3</td>\n",
       "      <td>-59.293570</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2018-Q3</td>\n",
       "      <td>-61.716930</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Period       MSPR\n",
       "0  2018-Q2 -33.796238\n",
       "1  2018-Q2 -52.667397\n",
       "2  2018-Q2 -48.464813\n",
       "3  2018-Q3 -59.293570\n",
       "4  2018-Q3 -61.716930"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Relevant Information on AAPL:\n",
      "\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 61 entries, 0 to 60\n",
      "Data columns (total 2 columns):\n",
      " #   Column  Non-Null Count  Dtype  \n",
      "---  ------  --------------  -----  \n",
      " 0   Period  61 non-null     object \n",
      " 1   MSPR    61 non-null     float64\n",
      "dtypes: float64(1), object(1)\n",
      "memory usage: 1.1+ KB\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "None"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>MSPR</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>61.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>-26.480193</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>64.479050</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>-100.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>-85.370240</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>-33.200634</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>-7.226337</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>100.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             MSPR\n",
       "count   61.000000\n",
       "mean   -26.480193\n",
       "std     64.479050\n",
       "min   -100.000000\n",
       "25%    -85.370240\n",
       "50%    -33.200634\n",
       "75%     -7.226337\n",
       "max    100.000000"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Relevant Information on NVDA:\n",
      "\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 64 entries, 0 to 63\n",
      "Data columns (total 2 columns):\n",
      " #   Column  Non-Null Count  Dtype  \n",
      "---  ------  --------------  -----  \n",
      " 0   Period  64 non-null     object \n",
      " 1   MSPR    64 non-null     float64\n",
      "dtypes: float64(1), object(1)\n",
      "memory usage: 1.1+ KB\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "None"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>MSPR</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>64.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>-51.053201</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>49.574298</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>-100.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>-100.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>-55.415127</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>-10.659913</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>100.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             MSPR\n",
       "count   64.000000\n",
       "mean   -51.053201\n",
       "std     49.574298\n",
       "min   -100.000000\n",
       "25%   -100.000000\n",
       "50%    -55.415127\n",
       "75%    -10.659913\n",
       "max    100.000000"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Relevant Information on GOOGL:\n",
      "\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 72 entries, 0 to 71\n",
      "Data columns (total 2 columns):\n",
      " #   Column  Non-Null Count  Dtype  \n",
      "---  ------  --------------  -----  \n",
      " 0   Period  72 non-null     object \n",
      " 1   MSPR    72 non-null     float64\n",
      "dtypes: float64(1), object(1)\n",
      "memory usage: 1.3+ KB\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "None"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>MSPR</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>72.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>-29.655626</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>38.240153</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>-100.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>-49.633164</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>-37.794344</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>-15.689648</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>80.802160</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             MSPR\n",
       "count   72.000000\n",
       "mean   -29.655626\n",
       "std     38.240153\n",
       "min   -100.000000\n",
       "25%    -49.633164\n",
       "50%    -37.794344\n",
       "75%    -15.689648\n",
       "max     80.802160"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "print(\"2.1.2: Display Company-Specific Insider DataFrames\")\n",
    "print(\"-\"*30)\n",
    "\n",
    "# This is how we can access the insider_datasets ictionary now\n",
    "display(insider_datasets[\"AAPL\"].head())\n",
    "display(insider_datasets[\"NVDA\"].head())\n",
    "display(insider_datasets[\"GOOGL\"].head())\n",
    "\n",
    "# This bit is just to gather contextual info on data distributions, quantities, ect ect\n",
    "for TICKER in list(insider_datasets):\n",
    "    print(f\"Relevant Information on {TICKER}:\\n\")\n",
    "    display(insider_datasets[TICKER].info())\n",
    "    display(insider_datasets[TICKER].describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57c9aab5",
   "metadata": {},
   "source": [
    "We now have a `Dictionary` containing: \n",
    "* `AAPL` Dataset \n",
    "* `NVDA` Dataset\n",
    "* `GOOGL` Dataset\n",
    "\n",
    "Each of these datasets has this format:\n",
    "|   Period   | MSPR        |\n",
    "|------------|-------------|\n",
    "| 2018-Q1    | Value       | \n",
    "| 2018-Q1    | Value       | \n",
    "| 2018-Q1    | Value       | \n",
    "| 2018-Q2    | Value       | \n",
    "| ...        | ...         | \n",
    "| 2024-Q4    | Value       | "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8694c0ef",
   "metadata": {},
   "source": [
    "### Strategy 2.2: Historical News via Hugging Face Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5c4fe93a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.2.0: Configuration & Authentication for Hugging Face\n",
      "------------------------------\n",
      "Found Hugging Face token. Logging in...\n",
      "Login failed: name 'login' is not defined\n",
      "\n",
      "Configuration for historical news extraction is set.\n",
      "Tickers: ['AAPL', 'NVDA', 'GOOGL']\n",
      "Date Range: 2018-01-01 to 2024-12-31\n"
     ]
    }
   ],
   "source": [
    "print(\"2.2.0: Configuration & Authentication for Hugging Face\")\n",
    "print(\"-\"*30)\n",
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "\n",
    "# NOTE: --- Authentication (IMPORTANT) ---\n",
    "# The Hugging Face User Access Token is loaded from the .env file.\n",
    "# Make sure your .env file has the line: HF_TOKEN='your_token_here'\n",
    "\n",
    "hf_token = os.getenv(\"HF_TOKEN\")\n",
    "\n",
    "if hf_token:\n",
    "    print(\"Found Hugging Face token. Logging in...\")\n",
    "    try:\n",
    "        login(token=hf_token)\n",
    "        print(\"Login successful.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Login failed: {e}\")\n",
    "else:\n",
    "    print(\"Hugging Face token not found in environment variables.\")\n",
    "    print(\"Please ensure your .env file is correctly configured with 'HF_TOKEN'.\")\n",
    "\n",
    "\n",
    "# --- Configuration ---\n",
    "# These are defined globally, but we re-state them here for clarity.\n",
    "# Note: START_YEAR and END_YEAR must be defined in a previous cell.\n",
    "try:\n",
    "    TARGET_TICKERS = ['AAPL', 'NVDA', 'GOOGL']\n",
    "    START_DATE = f\"{START_YEAR}-01-01\"\n",
    "    END_DATE = f\"{END_YEAR}-12-31\"\n",
    "    \n",
    "    print(\"\\nConfiguration for historical news extraction is set.\")\n",
    "    print(f\"Tickers: {TARGET_TICKERS}\")\n",
    "    print(f\"Date Range: {START_DATE} to {END_DATE}\")\n",
    "except NameError:\n",
    "    print(\"\\nWarning: START_YEAR and END_YEAR are not defined.\")\n",
    "    print(\"Please run a configuration cell first.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68c39379",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"2.2.1: Extract Ticker-Specific News Articles via Streaming\")\n",
    "print(\"-\"*30)\n",
    "\n",
    "# --- Expanded Search Dictionary ---\n",
    "# Maps a canonical ticker to a set of lowercase search terms for broader matching.\n",
    "# Using sets provides a slight performance boost for lookups.\n",
    "SEARCH_TERMS = {\n",
    "    'AAPL': {'aapl', 'apple'},\n",
    "    'NVDA': {'nvda', 'nvidia'},\n",
    "    'GOOGL': {'googl', 'google', 'alphabet'}\n",
    "}\n",
    "# A flattened set of all terms for a very fast initial check.\n",
    "ALL_SEARCH_TERMS = set.union(*SEARCH_TERMS.values())\n",
    "\n",
    "# --- Data Storage ---\n",
    "multisource_articles = []\n",
    "\n",
    "print(\"Loading and filtering 'financial-news-multisource' dataset...\")\n",
    "print(\"(This is a large dataset and will take some time to process.)\")\n",
    "\n",
    "try:\n",
    "    # --- Increase the download timeout for stability on large datasets ---\n",
    "    import huggingface_hub.constants\n",
    "    huggingface_hub.constants.HF_HUB_DOWNLOAD_TIMEOUT = 120 \n",
    "\n",
    "    # --- Load all subsets of the dataset in streaming mode ---\n",
    "    multisource_dataset = load_dataset(\n",
    "        \"Brianferrell787/financial-news-multisource\",\n",
    "        data_files=\"data/*/*.parquet\",\n",
    "        split=\"train\",\n",
    "        streaming=True\n",
    "    )\n",
    "    \n",
    "    # --- Iterate through the stream with the optimized multi-step filter ---\n",
    "    for i, article in enumerate(iter(multisource_dataset)):\n",
    "        # Provide progress updates to show the process is not stalled\n",
    "        if (i + 1) % 10_000 == 0:\n",
    "            print(f\"  > Processed {i + 1:,} articles. Found {len(multisource_articles)} relevant so far...\")\n",
    "            print(f\"  > We are currently at date: {article['date'][:10]}\")\n",
    "\n",
    "        # --- OPTIMIZED FILTERING LOGIC ---\n",
    "        # Step 1: Filter by date (fastest, string comparison).\n",
    "        if not (START_DATE <= article['date'][:10] <= END_DATE):\n",
    "            continue\n",
    "        \n",
    "        # Step 2: Quick pre-filter. Check if any of our expanded search terms appear\n",
    "        # anywhere in the text or metadata before doing more expensive work.\n",
    "        text_lower = article['text'].lower()\n",
    "        extra_fields_lower = article['extra_fields'].lower()\n",
    "        if not any(term in text_lower or term in extra_fields_lower for term in ALL_SEARCH_TERMS):\n",
    "            continue\n",
    "\n",
    "        # --- Step 3: Precise ticker identification for articles that passed the pre-filters ---\n",
    "        mentioned_tickers = set() # Use a set to store found tickers to avoid duplicates\n",
    "\n",
    "        # Primary Method: Check the structured 'stocks' field for high precision.\n",
    "        try:\n",
    "            extra_data = json.loads(article['extra_fields'])\n",
    "            if 'stocks' in extra_data and isinstance(extra_data['stocks'], list):\n",
    "                # Find the intersection of our target tickers and the article's tickers\n",
    "                found = set(TARGET_TICKERS) & set(extra_data['stocks'])\n",
    "                mentioned_tickers.update(found)\n",
    "        except (json.JSONDecodeError, TypeError):\n",
    "            # If JSON is invalid, we can fall back to text search.\n",
    "            pass\n",
    "\n",
    "        # Fallback Method: If no tickers were found in metadata, check the text.\n",
    "        # This increases recall for articles that might not be perfectly tagged.\n",
    "        if not mentioned_tickers:\n",
    "            for ticker, terms in SEARCH_TERMS.items():\n",
    "                if any(term in text_lower for term in terms):\n",
    "                    mentioned_tickers.add(ticker)\n",
    "        \n",
    "        # If we found one or more relevant tickers, add entries to our list.\n",
    "        if mentioned_tickers:\n",
    "            for ticker in mentioned_tickers:\n",
    "                multisource_articles.append({\n",
    "                    'date': article['date'],\n",
    "                    'ticker': ticker,\n",
    "                    'text': article['text']\n",
    "                })\n",
    "    \n",
    "    print(f\"\\nExtraction complete. Total relevant article entries collected: {len(multisource_articles)}\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"\\nAn error occurred while processing the dataset: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25968da9",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"2.2.2: Create the News Articles DataFrame\")\n",
    "print(\"-\"*30)\n",
    "\n",
    "# --- Create DataFrame from the collected list ---\n",
    "news_articles_df = pd.DataFrame(multisource_articles)\n",
    "\n",
    "if not news_articles_df.empty:\n",
    "    # Convert 'date' column to datetime objects for future analysis\n",
    "    news_articles_df['date'] = pd.to_datetime(news_articles_df['date']).dt.date\n",
    "    \n",
    "    print(\"`news_articles_df` DataFrame created successfully.\")\n",
    "    \n",
    "    # Display summary and head\n",
    "    print(\"\\n--- DataFrame Info ---\")\n",
    "    news_articles_df.info()\n",
    "    \n",
    "    print(\"\\n--- DataFrame Head ---\")\n",
    "    display(news_articles_df.head())\n",
    "    \n",
    "else:\n",
    "    print(\"No articles from 'financial-news-multisource' matched the filtering criteria.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59c0f8e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"2.2.3: Save Filtered News Articles to a Compressed File\")\n",
    "print(\"-\"*30)\n",
    "\n",
    "# This step is crucial for checkpointing our progress.\n",
    "\n",
    "if 'news_articles_df' in locals() and not news_articles_df.empty:\n",
    "    # Define the output file path. Using the '.gz' extension with the 'gzip'\n",
    "    # compression type is a standard and efficient way to save large CSVs.\n",
    "    output_filename = \"filtered_news_articles.csv.gz\"\n",
    "\n",
    "    print(f\"Saving the 'news_articles_df' to a compressed CSV file: {output_filename}\")\n",
    "    print(f\"This may take a moment given the size of the DataFrame ({len(news_articles_df):,} rows)...\")\n",
    "\n",
    "    try:\n",
    "        # Save the DataFrame to a gzipped CSV.\n",
    "        # - compression='gzip' handles the zipping automatically.\n",
    "        # - index=False prevents pandas from writing the DataFrame index as a column.\n",
    "        news_articles_df.to_csv(output_filename, index=False, compression='gzip')\n",
    "        \n",
    "        print(f\"\\nDataFrame successfully saved to '{output_filename}'.\")\n",
    "        print(\"You can now load this file in future sessions to skip the long extraction process.\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"\\nAn error occurred while saving the DataFrame: {e}\")\n",
    "else:\n",
    "    print(\"The 'news_articles_df' DataFrame is not available or is empty. Skipping save operation.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b4f6a36",
   "metadata": {},
   "source": [
    "### Strategy 2.3: Article Headlines Via `Kaggle`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3bd99f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"2.3.0: Setup and Kaggle API Configuration\")\n",
    "print(\"-\"*30)\n",
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "\n",
    "# --- 1. Configure Kaggle API ---\n",
    "# Credentials are loaded from your .env file.\n",
    "# Ensure your .env file contains:\n",
    "# KAGGLE_USERNAME='your_username'\n",
    "# KAGGLE_KEY='your_api_key'\n",
    "\n",
    "KAGGLE_USERNAME = os.getenv('KAGGLE_USERNAME')\n",
    "KAGGLE_KEY = os.getenv('KAGGLE_KEY')\n",
    "\n",
    "# Set environment variables for the Kaggle CLI\n",
    "if KAGGLE_USERNAME and KAGGLE_KEY:\n",
    "    os.environ['KAGGLE_USERNAME'] = KAGGLE_USERNAME\n",
    "    os.environ['KAGGLE_KEY'] = KAGGLE_KEY\n",
    "    print(\"Kaggle API credentials configured from environment variables.\")\n",
    "else:\n",
    "    print(\"Warning: Kaggle credentials not found in environment variables.\")\n",
    "    print(\"Please ensure your .env file is correctly configured.\")\n",
    "\n",
    "# --- 2. Define Constants ---\n",
    "# Date range for filtering headlines.\n",
    "START_YEAR = 2018\n",
    "END_YEAR = 2024\n",
    "\n",
    "print(\"\\nSetup complete. Constants defined.\")\n",
    "print(f\"Data will be filtered for the period: {START_YEAR}-{END_YEAR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b159985d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"2.3.1: Download, Extract, and Filter S&P 500 Headlines Dataset\")\n",
    "print(\"-\"*30)\n",
    "\n",
    "import zipfile\n",
    "\n",
    "# --- Define constants for data directory and file paths ---\n",
    "DATA_DIR = \"Data\"\n",
    "DATASET_NAME = 'dyutidasmahaptra/s-and-p-500-with-financial-news-headlines-20082024'\n",
    "ZIP_FILE_NAME = 's-and-p-500-with-financial-news-headlines-20082024.zip'\n",
    "CSV_FILE_NAME = 'sp500_headlines_2008_2024.csv'\n",
    "\n",
    "# Construct full paths for the files within the Data directory\n",
    "zip_file_path = os.path.join(DATA_DIR, ZIP_FILE_NAME)\n",
    "csv_file_path = os.path.join(DATA_DIR, CSV_FILE_NAME)\n",
    "\n",
    "# Ensure the target data directory exists\n",
    "os.makedirs(DATA_DIR, exist_ok=True)\n",
    "print(f\"Ensured that the target directory '{DATA_DIR}' exists.\")\n",
    "\n",
    "# --- Download the dataset using the Kaggle API into the specified directory ---\n",
    "print(f\"Downloading dataset '{DATASET_NAME}' to '{DATA_DIR}'...\")\n",
    "# The -p flag directs the Kaggle CLI to download to the specified path.\n",
    "!kaggle datasets download -d {DATASET_NAME} -p {DATA_DIR} --quiet\n",
    "\n",
    "# --- Extract the CSV file from the downloaded zip ---\n",
    "print(f\"Extracting '{CSV_FILE_NAME}' into '{DATA_DIR}'...\")\n",
    "with zipfile.ZipFile(zip_file_path, 'r') as zip_ref:\n",
    "    zip_ref.extractall(DATA_DIR)\n",
    "print(\"Extraction complete.\")\n",
    "\n",
    "# --- Clean up the downloaded zip file ---\n",
    "print(f\"Removing temporary zip file: '{zip_file_path}'\")\n",
    "os.remove(zip_file_path)\n",
    "\n",
    "# --- Load and process the data with pandas ---\n",
    "print(f\"Loading data from '{csv_file_path}' into DataFrame and filtering...\")\n",
    "try:\n",
    "    # Load the CSV from the Data directory\n",
    "    df = pd.read_csv(csv_file_path)\n",
    "\n",
    "    # Convert the 'date' column to datetime objects for reliable filtering\n",
    "    df['Date'] = pd.to_datetime(df['Date'])\n",
    "\n",
    "    # Create the filter condition for the date range\n",
    "    start_date = f'{START_YEAR}-01-01'\n",
    "    end_date = f'{END_YEAR}-12-31'\n",
    "    date_filter = (df['Date'] >= start_date) & (df['Date'] <= end_date)\n",
    "\n",
    "    # Apply the filter and create the final DataFrame\n",
    "    market_headlines_df = df[date_filter].copy()\n",
    "\n",
    "    # Drop the 'close' column as it is not needed for sentiment analysis\n",
    "    market_headlines_df = market_headlines_df.drop(columns=['CP'])\n",
    "\n",
    "    print(\"Filtering successful. The data is ready in 'market_headlines_df'.\")\n",
    "\n",
    "except FileNotFoundError:\n",
    "    print(f\"ERROR: The file '{csv_file_path}' was not found after extraction.\")\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred during data processing: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "837f5d95",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"2.3.2: Display Final Market Headlines DataFrame\")\n",
    "print(\"-\"*30)\n",
    "\n",
    "# Sort by date just to be sure\n",
    "market_headlines_df = market_headlines_df.sort_values(by='Date').reset_index(drop=True)\n",
    "\n",
    "if 'market_headlines_df' in locals() and not market_headlines_df.empty:\n",
    "    print(\"DataFrame for general market sentiment created successfully.\")\n",
    "    \n",
    "    print(\"\\n--- DataFrame Info ---\")\n",
    "    market_headlines_df.info()\n",
    "    \n",
    "    print(\"\\n--- DataFrame Head (sorted) ---\")\n",
    "    display(market_headlines_df.head())\n",
    "    \n",
    "    print(\"\\n--- DataFrame Tail (sorted) ---\")\n",
    "    display(market_headlines_df.tail())\n",
    "else:\n",
    "    print(\"The 'market_headlines_df' DataFrame was not created or is empty. Please check the previous cell for errors.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de805d71",
   "metadata": {},
   "source": [
    "### Strategy 2.4: Company filings via `EDGAR` API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "083a5eb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"2.4.0: Setup, Installations, and EDGAR Identity\")\n",
    "print(\"-\"*30)\n",
    "\n",
    "# --- 1. Install necessary libraries ---\n",
    "# 'edgartools' is the library we'll use to interface with the SEC EDGAR database.\n",
    "import sys\n",
    "!{sys.executable} -m pip install edgartools --quiet\n",
    "\n",
    "import pandas as pd\n",
    "from edgar import Company, set_identity\n",
    "\n",
    "# --- 2. Set EDGAR User Identity (CRITICAL STEP) ---\n",
    "# The SEC requires any script or bot that accesses EDGAR to have a custom User-Agent\n",
    "# that identifies the user. This is a compliance requirement to avoid being blocked.\n",
    "# Replace the example with your own company/project name and email address.\n",
    "# Format: \"Sample Company Name your.email@example.com\"\n",
    "set_identity(\"University of Southampton ab3u21@soton.ac.uk\")\n",
    "print(\"EDGAR user identity set successfully.\")\n",
    "\n",
    "# --- 3. Define Constants ---\n",
    "# These constants will be used to filter the filings.\n",
    "# We use the globally defined START_YEAR and END_YEAR from cell 2.1.0\n",
    "TARGET_TICKERS = ['AAPL', 'NVDA', 'GOOGL']\n",
    "FORM_TYPES = [\"10-K\", \"10-Q\", \"8-K\"]\n",
    "DATE_RANGE = f\"{START_YEAR}-01-01:{END_YEAR}-12-31\"\n",
    "\n",
    "\n",
    "print(\"\\nSetup complete. Ready to extract SEC filings.\")\n",
    "print(f\"Tickers: {TARGET_TICKERS}\")\n",
    "print(f\"Form Types: {FORM_TYPES}\")\n",
    "print(f\"Date Range: {DATE_RANGE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "154eb914",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"2.4.1: Extract SEC Filings Data\")\n",
    "print(\"-\"*30)\n",
    "\n",
    "# --- Data Storage ---\n",
    "all_filings_data = []\n",
    "\n",
    "print(\"Starting extraction of SEC filings. This may take a significant amount of time...\")\n",
    "\n",
    "# --- Loop through each stock and extract its filings ---\n",
    "for ticker in TARGET_TICKERS:\n",
    "    print(f\"  > Processing filings for {ticker}...\")\n",
    "    try:\n",
    "        # Create a Company object for the current ticker\n",
    "        company = Company(ticker)\n",
    "        \n",
    "        # Get all filings and immediately filter by date range and form types\n",
    "        filings = company.get_filings().filter(date=DATE_RANGE, form=FORM_TYPES)\n",
    "        \n",
    "        # The 'filings' object is a generator; we iterate through it to get each filing\n",
    "        for filing in filings:\n",
    "            # The .text() method conveniently extracts and cleans the full filing text\n",
    "            filing_text = filing.text()\n",
    "            \n",
    "            # Append the structured data to our list\n",
    "            all_filings_data.append({\n",
    "                'filing_date': filing.filing_date,\n",
    "                'ticker': ticker,\n",
    "                'form_type': filing.form,\n",
    "                'text': filing_text\n",
    "            })\n",
    "            # Log progress for each file found\n",
    "            print(f\"    - Extracted {filing.form} from {filing.filing_date}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"    - ERROR: Could not process filings for {ticker}. Reason: {e}\")\n",
    "\n",
    "print(f\"\\nFilings extraction complete. Total documents extracted: {len(all_filings_data)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9d01eb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"2.4.2: Create and Display Filings DataFrame\")\n",
    "print(\"-\"*30)\n",
    "\n",
    "# --- Create DataFrame from the collected list ---\n",
    "filings_df = pd.DataFrame(all_filings_data)\n",
    "\n",
    "if not filings_df.empty:\n",
    "    # Convert 'filing_date' column to datetime objects\n",
    "    filings_df['filing_date'] = pd.to_datetime(filings_df['filing_date'])\n",
    "    \n",
    "    # Sort the DataFrame by date and ticker for good practice\n",
    "    filings_df = filings_df.sort_values(by=['filing_date', 'ticker']).reset_index(drop=True)\n",
    "    \n",
    "    print(\"`filings_df` DataFrame created successfully.\")\n",
    "    \n",
    "    # Display summary and head\n",
    "    print(\"\\n--- DataFrame Info ---\")\n",
    "    filings_df.info()\n",
    "    \n",
    "    print(\"\\n--- DataFrame Head ---\")\n",
    "    display(filings_df.head(40).sort_values([\"filing_date\"], ascending=True))\n",
    "    \n",
    "else:\n",
    "    print(\"The 'all_filings_data' list is empty. No DataFrame was created. Please check cell 2.4.1 for errors.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfcab2ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"2.4.3: Save Filtered Filings to a Compressed File\")\n",
    "print(\"-\"*30)\n",
    "\n",
    "# Check if the filings_df exists and is not empty before attempting to save.\n",
    "if 'filings_df' in locals() and not filings_df.empty:\n",
    "    \n",
    "    # Define the name for the compressed output file.\n",
    "    output_filename = \"filtered_filings.csv.gz\"\n",
    "\n",
    "    print(f\"Saving the 'filings_df' to a compressed CSV file: {output_filename}\")\n",
    "    print(f\"This may take a moment, as the DataFrame contains {len(filings_df):,} documents...\")\n",
    "\n",
    "    try:\n",
    "        # Save the DataFrame to a gzipped CSV.\n",
    "        # - compression='gzip' handles the compression.\n",
    "        # - index=False prevents pandas from writing the DataFrame index as a column.\n",
    "        filings_df.to_csv(output_filename, index=False, compression='gzip')\n",
    "        \n",
    "        print(f\"\\nDataFrame successfully saved to '{output_filename}'.\")\n",
    "        print(\"This file can be loaded in future sessions to bypass the EDGAR extraction process.\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"\\nAn error occurred while saving the filings DataFrame: {e}\")\n",
    "else:\n",
    "    print(\"The 'filings_df' DataFrame is not available or is empty. Skipping the save operation.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59183a89",
   "metadata": {},
   "source": [
    "## Phase 3: NLP Pipelines"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d827395",
   "metadata": {},
   "source": [
    "### 3.1: NLP Pipeline for `news_articles_df`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b13a3f82",
   "metadata": {},
   "source": [
    "#### We start this section by *Down_sampling*: \n",
    "We want to go from our `>700k` articles to `~11k` articles for each company over the 2018-2024 year period. That's `5` articles per day per company!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c01fd098",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"3.1.0: Setup and Imports for NLP Pre-processing\")\n",
    "print(\"-\"*30)\n",
    "\n",
    "# --- 1. Install necessary libraries ---\n",
    "import sys\n",
    "!{sys.executable} -m pip install nltk pandas --quiet\n",
    "\n",
    "import pandas as pd\n",
    "import re\n",
    "import nltk\n",
    "import os\n",
    "\n",
    "# --- 2. Download NLTK resources ---\n",
    "# We need 'punkt' and 'punkt_tab' for word tokenization. The most reliable\n",
    "# way to ensure they are available in a notebook is to call download() directly.\n",
    "# NLTK will not re-download the data if it's already present.\n",
    "print(\"Ensuring NLTK resources ('punkt', 'punkt_tab') are available...\")\n",
    "nltk.download('punkt', quiet=True)\n",
    "nltk.download('punkt_tab', quiet=True) # This command fixes the LookupError.\n",
    "print(\"NLTK resources are up to date.\")\n",
    "\n",
    "# --- 3. Define File path for fallback loading ---\n",
    "NEWS_ARTICLES_FILE = \"filtered_news_articles.csv.gz\"\n",
    "\n",
    "print(\"\\nSetup complete. Libraries and NLTK resources are ready.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "207acf63",
   "metadata": {},
   "source": [
    "#### Here we calculate a `Mention_Density` metric for each atricle that will help us understand how relevant an article is for our selected company."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2a36a3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"3.1.1: Calculate Mention Density Score for Each Article\")\n",
    "print(\"-\"*30)\n",
    "\n",
    "# --- 1. Define Comprehensive Company Keywords ---\n",
    "COMPANY_KEYWORDS = {\n",
    "    'AAPL': {\n",
    "        'aapl', 'apple', 'iphone', 'ipad', 'macbook', 'imac', 'watchos',\n",
    "        'ios', 'macos', 'airpods', 'tim cook', 'app store', 'vision pro'\n",
    "    },\n",
    "    'NVDA': {\n",
    "        'nvda', 'nvidia', 'geforce', 'rtx', 'quadro', 'tesla', # Note: 'tesla' is a GPU arch\n",
    "        'cuda', 'dgx', 'tegra', 'jensen huang', 'omniverse'\n",
    "    },\n",
    "    'GOOGL': {\n",
    "        'googl', 'google', 'alphabet', 'android', 'youtube', 'chrome',\n",
    "        'pixel', 'nest', 'waymo', 'gcp', 'sundar pichai', 'gemini'\n",
    "    }\n",
    "}\n",
    "print(\"Comprehensive keyword dictionaries defined.\")\n",
    "\n",
    "# --- 2. Load the Dataset (prioritizing live DataFrame) ---\n",
    "news_mention_density_df = None # Initialize to None\n",
    "\n",
    "# Prioritize using the 'news_articles_df' if it's already in memory.\n",
    "if 'news_articles_df' in locals() and isinstance(news_articles_df, pd.DataFrame) and not news_articles_df.empty:\n",
    "    print(\"Using the live 'news_articles_df' DataFrame from the current session.\")\n",
    "    news_mention_density_df = news_articles_df.copy() # Use a copy to avoid side effects\n",
    "    \n",
    "# NOTE: --- This block is commented out but can be used for future sessions ---\n",
    "# Fallback to loading from the file if the live DataFrame is not available.\n",
    "# elif os.path.exists(NEWS_ARTICLES_FILE):\n",
    "#     print(f\"Loading dataset from '{NEWS_ARTICLES_FILE}'...\")\n",
    "#     # In a new session, you would uncomment the line below:\n",
    "#     # news_mention_density_df = pd.read_csv(NEWS_ARTICLES_FILE, compression='gzip')\n",
    "# -----------------------------------------------------------------------\n",
    "\n",
    "if news_mention_density_df is None:\n",
    "    print(f\"ERROR: 'news_articles_df' not found in the current session.\")\n",
    "    print(f\"To run this cell independently, uncomment the file loading logic above and ensure '{NEWS_ARTICLES_FILE}' exists.\")\n",
    "else:\n",
    "    print(f\"DataFrame loaded with {len(news_mention_density_df):,} articles.\")\n",
    "    \n",
    "    # --- 3. Define the Density Calculation Function ---\n",
    "    def calculate_all_densities(text):\n",
    "        if not isinstance(text, str) or not text.strip():\n",
    "            return pd.Series({f'{ticker}_Density': 0.0 for ticker in COMPANY_KEYWORDS})\n",
    "        text_lower = text.lower()\n",
    "        total_words = len(nltk.word_tokenize(text_lower))\n",
    "        if total_words == 0:\n",
    "            return pd.Series({f'{ticker}_Density': 0.0 for ticker in COMPANY_KEYWORDS})\n",
    "        densities = {}\n",
    "        for ticker, keywords in COMPANY_KEYWORDS.items():\n",
    "            pattern = r'\\b(' + '|'.join(re.escape(k) for k in keywords) + r')\\b'\n",
    "            mention_count = len(re.findall(pattern, text_lower))\n",
    "            density = mention_count / total_words if total_words > 0 else 0\n",
    "            densities[f'{ticker}_Density'] = density\n",
    "        return pd.Series(densities)\n",
    "\n",
    "    # --- 4. Apply the function to the DataFrame ---\n",
    "    print(\"\\nCalculating mention densities for all articles. This may take a few minutes...\")\n",
    "    density_scores = news_mention_density_df['text'].apply(calculate_all_densities)\n",
    "    \n",
    "    news_mention_density_df = pd.concat([news_mention_density_df, density_scores], axis=1)\n",
    "    print(\"Mention density calculation complete.\")\n",
    "    \n",
    "    # --- 5. Display Results ---\n",
    "    print(\"\\n--- DataFrame with Mention Density Scores ---\")\n",
    "    display(news_mention_density_df[['date', 'ticker', 'AAPL_Density', 'NVDA_Density', 'GOOGL_Density']].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d30343f0",
   "metadata": {},
   "source": [
    "#### Now that we have `Mention_Density` for each company, we can select the Top `5` articles per day for each company that have a `Mention_Density` score of >1%.\n",
    "We will also include `article_volume` for each day, representing the total number of articles published for that company (with `Mention_Density` >1%.) in that day. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92e61a1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"3.1.2: Final Optimized Down-Sampling with Language Filter and Volume\")\n",
    "print(\"-\"*30)\n",
    "\n",
    "# --- Import language detection library ---\n",
    "import sys\n",
    "!{sys.executable} -m pip install langdetect --quiet\n",
    "from langdetect import detect, LangDetectException\n",
    "\n",
    "# --- 1. Define Filtering Parameters ---\n",
    "# MIN_DENSITY_THRESHOLD: The minimum relevance score an article must have to be considered.\n",
    "MIN_DENSITY_THRESHOLD = 0.01\n",
    "# TOP_N_ARTICLES: The maximum number of highest-scoring articles to select for any given day.\n",
    "TOP_N_ARTICLES = 5\n",
    "\n",
    "print(f\"Filtering Parameters:\")\n",
    "print(f\" - Minimum Mention Density Threshold: {MIN_DENSITY_THRESHOLD}\")\n",
    "print(f\" - Top N Articles per Day: {TOP_N_ARTICLES}\\n\")\n",
    "\n",
    "# --- 2. Initialize Storage ---\n",
    "# This dictionary will hold the final, filtered DataFrames, one for each company.\n",
    "company_top_articles = {}\n",
    "\n",
    "# Check if the main DataFrame from the previous step is available in memory.\n",
    "if 'news_mention_density_df' in locals() and not news_mention_density_df.empty:\n",
    "    \n",
    "    # --- Step A: De-duplicate Articles ---\n",
    "    # This ensures we only process each unique article text once per day.\n",
    "    print(f\"Original article entry count: {len(news_mention_density_df):,}\")\n",
    "    deduped_df = news_mention_density_df.drop_duplicates(subset=['date', 'text']).copy()\n",
    "    print(f\"De-duplicated article count: {len(deduped_df):,}\")\n",
    "    \n",
    "    # --- Step B: Lightweight Language Filtering with Progress Indicator ---\n",
    "    # This function checks only the first 100 characters of text for speed.\n",
    "    def is_english_fast(text):\n",
    "        try:\n",
    "            # We only need a small sample of the text to accurately detect the language.\n",
    "            sample = text[:100] if isinstance(text, str) else ''\n",
    "            # Return True only if the sample is valid and detected as English ('en').\n",
    "            return sample.strip() and detect(sample) == 'en'\n",
    "        except LangDetectException:\n",
    "            # If detection fails, we assume it's not the language we want.\n",
    "            return False\n",
    "\n",
    "    print(\"\\nFiltering for English-language articles (with progress updates)...\")\n",
    "    total_texts_to_check = len(deduped_df)\n",
    "    print_interval = 10000  # How often to print an update.\n",
    "    \n",
    "    english_mask = []\n",
    "    # We use an explicit loop here to provide progress feedback.\n",
    "    for i, text in enumerate(deduped_df['text']):\n",
    "        english_mask.append(is_english_fast(text))\n",
    "        \n",
    "        # This block prints a status update at the specified interval.\n",
    "        if (i + 1) % print_interval == 0 or (i + 1) == total_texts_to_check:\n",
    "            print(f\"  > Language check progress: {i + 1:,} of {total_texts_to_check:,} articles processed...\")\n",
    "\n",
    "    # Use the generated boolean mask to select only the English articles.\n",
    "    english_df = deduped_df[english_mask]\n",
    "    print(f\"\\nFiltered down to {len(english_df):,} English articles.\\n\")\n",
    "    \n",
    "    # --- Step 3. Process Each Company Independently ---\n",
    "    for ticker in COMPANY_KEYWORDS.keys():\n",
    "        density_col = f'{ticker}_Density'\n",
    "        print(f\"--- Processing {ticker} ---\")\n",
    "\n",
    "        # Step C: Filter for relevance based on the density threshold.\n",
    "        relevant_articles_df = english_df[english_df[density_col] >= MIN_DENSITY_THRESHOLD].copy()\n",
    "        \n",
    "        if relevant_articles_df.empty:\n",
    "            print(f\"  > No articles found for {ticker} above the density threshold. Skipping.\")\n",
    "            company_top_articles[ticker] = pd.DataFrame()\n",
    "            continue\n",
    "        \n",
    "        print(f\"  > Found {len(relevant_articles_df):,} relevant English articles for {ticker}.\")\n",
    "\n",
    "        # Step D: Calculate daily article volume from the relevant set.\n",
    "        daily_volume = relevant_articles_df.groupby('date').size().rename('article_volume')\n",
    "        \n",
    "        # Step E: Perform the \"Top N per Day\" selection.\n",
    "        top_n_df = relevant_articles_df.groupby('date').apply(\n",
    "            lambda x: x.nlargest(TOP_N_ARTICLES, density_col),\n",
    "            include_groups=False\n",
    "        ).reset_index(level=0)\n",
    "        \n",
    "        # Step F: Merge the daily volume feature onto the down-sampled DataFrame.\n",
    "        top_n_df = top_n_df.merge(daily_volume, on='date', how='left')\n",
    "        \n",
    "        # Step G: Clean up the ticker column for clarity.\n",
    "        top_n_df = top_n_df.drop(columns=['ticker'])\n",
    "        top_n_df['assigned_ticker'] = ticker\n",
    "        \n",
    "        print(f\"  > Down-sampled to {len(top_n_df):,} top articles for NLP analysis.\")\n",
    "\n",
    "        # Store the final, enriched DataFrame in the dictionary.\n",
    "        company_top_articles[ticker] = top_n_df\n",
    "    \n",
    "    print(\"\\nDown-sampling and all filtering complete.\")\n",
    "    \n",
    "    # --- 4. Display a Sample of the Results ---\n",
    "    print(\"\\n--- Sample of Final 'AAPL' Articles ---\")\n",
    "    if 'AAPL' in company_top_articles and not company_top_articles['AAPL'].empty:\n",
    "        display(company_top_articles['AAPL'][['date', 'assigned_ticker', 'AAPL_Density', 'article_volume']].head())\n",
    "    else:\n",
    "        print(\"No articles to display for AAPL.\")\n",
    "        \n",
    "else:\n",
    "    print(\"ERROR: 'news_mention_density_df' not found. Please run cell 3.1.1 first.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff310390",
   "metadata": {},
   "outputs": [],
   "source": [
    "for TICKER in list(company_top_articles):\n",
    "    print(TICKER)\n",
    "    display(company_top_articles[TICKER].head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a29dfd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"3.1.3: Inspect Down-Sampled DataFrames for Missing Data and Volume\")\n",
    "print(\"-\"*30)\n",
    "\n",
    "if 'company_top_articles' in locals() and isinstance(company_top_articles, dict):\n",
    "\n",
    "    full_date_range = pd.date_range(start=f'{START_YEAR}-01-01', end=f'{END_YEAR}-12-31', freq='D')\n",
    "    total_days_in_period = len(full_date_range)\n",
    "    \n",
    "    print(f\"Analyzing data coverage over a total period of {total_days_in_period} days ({START_YEAR}-{END_YEAR}).\\n\")\n",
    "\n",
    "    for ticker, df in company_top_articles.items():\n",
    "        print(f\"--- Inspection for {ticker} ---\")\n",
    "        \n",
    "        if df.empty:\n",
    "            print(\"  > No articles were selected for this ticker. DataFrame is empty.\\n\")\n",
    "            continue\n",
    "        \n",
    "        df['date'] = pd.to_datetime(df['date'])\n",
    "        \n",
    "        total_articles = len(df)\n",
    "        unique_days_with_articles = df['date'].nunique()\n",
    "        missing_days = total_days_in_period - unique_days_with_articles\n",
    "        coverage_percentage = (unique_days_with_articles / total_days_in_period) * 100\n",
    "        \n",
    "        print(f\"  Article Coverage:\")\n",
    "        print(f\"    > Total selected articles for NLP: {total_articles:,}\")\n",
    "        print(f\"    > Unique days with articles: {unique_days_with_articles:,}\")\n",
    "        print(f\"    > Days with NO articles (to be imputed): {missing_days:,}\")\n",
    "        print(f\"    > Daily coverage percentage: {coverage_percentage:.2f}%\")\n",
    "        \n",
    "        # --- NEW: Reporting on 'article_volume' ---\n",
    "        # To get daily stats, we first drop duplicates for each day since 'article_volume' is repeated.\n",
    "        daily_stats_df = df.drop_duplicates(subset=['date'])\n",
    "        \n",
    "        print(f\"\\n  Daily Article Volume (for days with coverage):\")\n",
    "        print(f\"    > Mean daily volume: {daily_stats_df['article_volume'].mean():.2f}\")\n",
    "        print(f\"    > Median daily volume: {daily_stats_df['article_volume'].median():.0f}\")\n",
    "        print(f\"    > Max daily volume: {daily_stats_df['article_volume'].max():.0f}\")\n",
    "        print(f\"    > Min daily volume: {daily_stats_df['article_volume'].min():.0f}\\n\")\n",
    "        \n",
    "else:\n",
    "    print(\"ERROR: 'company_top_articles' dictionary not found. Please run cell 3.1.2 first.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c8bad1b",
   "metadata": {},
   "source": [
    "#### These steps below are just exporting and verifying importing doesn't break anything."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c4b8ab6",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"3.1.4: Save Filtered Articles Dictionary to a File\")\n",
    "print(\"-\"*30)\n",
    "\n",
    "# Import the pickle library for object serialization.\n",
    "import pickle\n",
    "\n",
    "# Check if the dictionary from the previous cell exists.\n",
    "if 'company_top_articles' in locals() and isinstance(company_top_articles, dict):\n",
    "    \n",
    "    # Define the output filename for our serialized dictionary.\n",
    "    output_filename = \"company_top_articles.pkl\"\n",
    "\n",
    "    print(f\"Saving the 'company_top_articles' dictionary to '{output_filename}'...\")\n",
    "    print(\"This file will serve as a checkpoint to avoid re-running the filtering process.\")\n",
    "\n",
    "    try:\n",
    "        # Open the file in write-binary ('wb') mode.\n",
    "        with open(output_filename, 'wb') as f:\n",
    "            # Use pickle.dump() to serialize the entire dictionary object into the file.\n",
    "            pickle.dump(company_top_articles, f)\n",
    "        \n",
    "        print(f\"\\nDictionary successfully saved to '{output_filename}'.\")\n",
    "        \n",
    "        # Provide a snippet for how to load the data in a future session.\n",
    "        print(\"\\nTo load this data in a future session, you can use the following code:\")\n",
    "        print(\"------------------------------------------------------------------\")\n",
    "        print(\"import pickle\")\n",
    "        print(\"with open('company_top_articles.pkl', 'rb') as f:\")\n",
    "        print(\"    company_top_articles = pickle.load(f)\")\n",
    "        print(\"------------------------------------------------------------------\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"\\nAn error occurred while saving the dictionary: {e}\")\n",
    "else:\n",
    "    print(\"ERROR: 'company_top_articles' dictionary not found. Please run the previous cells first.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec4e6b73",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"3.1.5: Load and Verify the Filtered Articles Dictionary from File\")\n",
    "print(\"-\"*30)\n",
    "\n",
    "# Import the pickle library and pandas for displaying DataFrames.\n",
    "import pickle\n",
    "import pandas as pd\n",
    "\n",
    "# Define the filename where the dictionary was saved.\n",
    "FILENAME = \"Data/company_top_articles.pkl\"\n",
    "\n",
    "try:\n",
    "    # Open the file in read-binary ('rb') mode.\n",
    "    with open(FILENAME, 'rb') as f:\n",
    "        # Load the entire dictionary object from the pickle file.\n",
    "        loaded_company_articles = pickle.load(f)\n",
    "    \n",
    "    print(f\"Successfully loaded dictionary from '{FILENAME}'.\")\n",
    "    print(\"Verifying contents by displaying the head of each DataFrame...\\n\")\n",
    "    \n",
    "    # --- Verification Loop ---\n",
    "    # Iterate through the keys (tickers) and values (DataFrames) of the loaded dictionary.\n",
    "    for ticker, df in loaded_company_articles.items():\n",
    "        print(f\"--- Top 10 Articles for: {ticker} ---\")\n",
    "        \n",
    "        if not df.empty:\n",
    "            # Display the first 10 rows for visual inspection.\n",
    "            display(df.head(10))\n",
    "        else:\n",
    "            print(\"  > DataFrame is empty for this ticker.\")\n",
    "        print(\"\\n\" + \"-\"*50 + \"\\n\")\n",
    "\n",
    "except FileNotFoundError:\n",
    "    print(f\"ERROR: The file '{FILENAME}' was not found. Please run cell 3.1.4 to save the file first.\")\n",
    "except Exception as e:\n",
    "    print(f\"An unexpected error occurred while loading the file: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f25299f6",
   "metadata": {},
   "source": [
    "#### And we have completed the Down-Sampling. Next, we calculate composite_sentiment scores using `FinBERT`\n",
    "We now have some missing data because news articles (with `Mention_Density` >1%) are sometimes unavailable for certain days.\\\n",
    "Our lowest coverage is `82.79%` so we can safely reconstruct the missing sentiment data after we have computed these scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3d0dff3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.1.6: Install GPU-Enabled PyTorch (v2.6.0+) & Transformers\n",
      "------------------------------\n",
      "Uninstalling previous torch versions (if any)...\n",
      "Installing torch (>=2.6.0) for CUDA 12.4 (this may take a few minutes)...\n",
      "Installing Hugging Face Transformers and TQDM...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.2.1 -> 25.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "------------------------------\n",
      "! ! ! CRITICAL STEP ! ! !\n",
      "Installation complete. You MUST now restart the Jupyter kernel.\n",
      "In VS Code / Jupyter: Find the 'Restart' button for the kernel.\n",
      "After restarting, you can run the subsequent cells (3.1.7 onward).\n",
      "------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.2.1 -> 25.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "print(\"3.1.6: Install GPU-Enabled PyTorch (v2.6.0+) & Transformers\")\n",
    "print(\"-\"*30)\n",
    "\n",
    "import sys\n",
    "\n",
    "# --- Step 1: Uninstall any old versions ---\n",
    "# This is crucial to ensure a clean install.\n",
    "print(\"Uninstalling previous torch versions (if any)...\")\n",
    "# !{sys.executable} -m pip uninstall torch torchvision torchaudio -y --quiet\n",
    "\n",
    "# --- Step 2: Install CUDA-enabled PyTorch (v2.6.0+ for CUDA 12.4) ---\n",
    "# We MUST use the 'cu124' index, as 'cu121' does not have torch 2.6.0+.\n",
    "# This version is required by transformers to patch CVE-2025-32434.\n",
    "print(\"Installing torch (>=2.6.0) for CUDA 12.4 (this may take a few minutes)...\")\n",
    "!{sys.executable} -m pip install torch>=2.6.0 torchvision torchaudio --index-url https://download.pytorch.org/whl/cu124 --quiet\n",
    "\n",
    "# --- Step 3: Install Transformers & TQDM ---\n",
    "# We add 'tqdm' here to get the progress bars and remove the warning.\n",
    "print(\"Installing Hugging Face Transformers and TQDM...\")\n",
    "!{sys.executable} -m pip install transformers tqdm --quiet\n",
    "\n",
    "# --- Step 4: Critical Kernel Restart ---\n",
    "print(\"\\n\" + \"-\"*30)\n",
    "print(\"! ! ! CRITICAL STEP ! ! !\")\n",
    "print(\"Installation complete. You MUST now restart the Jupyter kernel.\")\n",
    "print(\"In VS Code / Jupyter: Find the 'Restart' button for the kernel.\")\n",
    "print(\"After restarting, you can run the subsequent cells (3.1.7 onward).\")\n",
    "print(\"-\"*30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a3fc3e42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.1.7: Verify Environment and Initialize Pipeline\n",
      "------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\totob\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version: 2.6.0+cu124\n",
      "\n",
      "Initializing the FinBERT sentiment analysis pipeline...\n",
      "GPU detected: NVIDIA GeForce RTX 3050 Ti Laptop GPU. The pipeline will run on the GPU.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "FinBERT pipeline and tokenizer initialized successfully.\n"
     ]
    }
   ],
   "source": [
    "print(\"3.1.7: Verify Environment and Initialize Pipeline\")\n",
    "print(\"-\"*30)\n",
    "\n",
    "import torch\n",
    "from transformers import pipeline, AutoTokenizer\n",
    "import sys\n",
    "\n",
    "# --- 1. Verify PyTorch Installation ---\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "if not torch.__version__.startswith(\"2.6.0\"):\n",
    "    print(\"Warning: PyTorch version is not 2.6.0. This might cause issues.\")\n",
    "\n",
    "# --- 2. Initialize the Sentiment Analysis Pipeline with GPU Auto-Detection ---\n",
    "print(\"\\nInitializing the FinBERT sentiment analysis pipeline...\")\n",
    "\n",
    "# Check if a CUDA-compatible GPU is available\n",
    "if torch.cuda.is_available():\n",
    "    device_id = 0\n",
    "    print(f\"GPU detected: {torch.cuda.get_device_name(device_id)}. The pipeline will run on the GPU.\")\n",
    "else:\n",
    "    device_id = -1\n",
    "    print(\"No GPU detected or PyTorch CUDA is not installed. The pipeline will run on the CPU.\")\n",
    "\n",
    "# Load the main pipeline\n",
    "# The device parameter automatically assigns the model to the GPU (0) or CPU (-1).\n",
    "sentiment_pipeline = pipeline(\"sentiment-analysis\", model=\"ProsusAI/finbert\", device=device_id)\n",
    "\n",
    "# Load the tokenizer separately.\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"ProsusAI/finbert\")\n",
    "\n",
    "print(\"\\nFinBERT pipeline and tokenizer initialized successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8727200f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"3.1.8: Re-import the dataframes after kernel restart\")\n",
    "print(\"-\"*30)\n",
    "\n",
    "import pickle\n",
    "with open('Data/company_top_articles.pkl', 'rb') as f:\n",
    "    company_top_articles = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3773da69",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"3.1.9: Apply NLP with Chunking and Aggregate Daily Sentiment\")\n",
    "print(\"-\"*30)\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm.auto import tqdm # For rich progress bars in notebooks\n",
    "\n",
    "# --- 1. Define Helper Functions ---\n",
    "# (These functions are unchanged)\n",
    "\n",
    "def get_sentiment_score(result):\n",
    "    \"\"\"Converts the pipeline's output dictionary to a single numerical score.\"\"\"\n",
    "    label = result['label']\n",
    "    score = result['score']\n",
    "    if label == 'positive':\n",
    "        return score\n",
    "    elif label == 'negative':\n",
    "        return -score\n",
    "    return 0.0\n",
    "\n",
    "def analyze_sentiment_with_chunking(text):\n",
    "    \"\"\"\n",
    "    Analyzes sentiment of a text, handling long inputs by chunking them.\n",
    "    Averages the sentiment scores of all chunks for a final document score.\n",
    "    \"\"\"\n",
    "    # Max tokens for the model, leaving a small buffer for special tokens.\n",
    "    max_length = 512\n",
    "    # Amount of token overlap between chunks to maintain context.\n",
    "    overlap = 25\n",
    "    \n",
    "    try:\n",
    "        # Tokenize the text to see if it needs chunking.\n",
    "        tokens = tokenizer.encode(str(text), return_tensors='pt', truncation=False)\n",
    "        \n",
    "        # If text is short enough, process it in one go.\n",
    "        if tokens.size(1) <= max_length:\n",
    "            result = sentiment_pipeline(str(text))[0]\n",
    "            return get_sentiment_score(result)\n",
    "\n",
    "        # If text is too long, split it into chunks.\n",
    "        chunk_texts = []\n",
    "        for i in range(0, tokens.size(1), max_length - overlap):\n",
    "            chunk_tokens = tokens[0, i:i + max_length]\n",
    "            chunk_texts.append(tokenizer.decode(chunk_tokens, skip_special_tokens=True))\n",
    "        \n",
    "        # Run the pipeline on the list of chunks.\n",
    "        chunk_sentiments = sentiment_pipeline(chunk_texts)\n",
    "        \n",
    "        # Calculate the average sentiment score from all the chunks.\n",
    "        scores = [get_sentiment_score(result) for result in chunk_sentiments]\n",
    "        return sum(scores) / len(scores) if scores else 0.0\n",
    "\n",
    "    except Exception:\n",
    "        # If any error occurs (e.g., empty text), return a neutral score.\n",
    "        return 0.0\n",
    "\n",
    "# --- 2. Initialize Storage for Final Results ---\n",
    "daily_sentiment_dfs = {}\n",
    "\n",
    "if 'company_top_articles' in locals():\n",
    "    print(f\"Starting NLP analysis on {len(company_top_articles)} companies...\")\n",
    "\n",
    "    # --- 3. Iterate Through Each Company's DataFrame (with TQDM) ---\n",
    "    for ticker, df in tqdm(company_top_articles.items(), desc=\"Processing all companies\"):\n",
    "        \n",
    "        if df.empty:\n",
    "            print(f\"  > DataFrame for {ticker} is empty. Skipping.\")\n",
    "            daily_sentiment_dfs[ticker] = pd.DataFrame()\n",
    "            continue\n",
    "\n",
    "        # --- 4. Apply Sentiment Analysis with a Progress Bar ---\n",
    "        # CORRECTED: Set the description for the inner progress bar by re-initializing tqdm.pandas()\n",
    "        tqdm.pandas(desc=f\"Analyzing {ticker} articles\")\n",
    "        \n",
    "        # Now, call .progress_apply() without any extra arguments.\n",
    "        df['sentiment_score'] = df['text'].progress_apply(analyze_sentiment_with_chunking)\n",
    "        \n",
    "        # --- 5. Aggregate to a Daily Time-Series DataFrame ---\n",
    "        \n",
    "        # Dynamically create the aggregation dictionary to include all density columns\n",
    "        agg_dict = {\n",
    "            'article_volume': ('article_volume', 'first'),\n",
    "            'average_news_sentiment': ('sentiment_score', 'mean')\n",
    "        }\n",
    "        \n",
    "        # Find all density columns in the DataFrame and add them to the aggregation.\n",
    "        density_cols = [col for col in df.columns if '_Density' in col]\n",
    "        for col in density_cols:\n",
    "            agg_dict[f'AVG_{col}'] = (col, 'mean')\n",
    "\n",
    "        daily_agg_df = df.groupby('date').agg(**agg_dict).reset_index()\n",
    "\n",
    "        daily_agg_df = daily_agg_df.rename(columns={'date': 'Period'})\n",
    "        \n",
    "        print(f\"\\n  > Aggregation for {ticker} complete. Created daily time-series with {len(daily_agg_df)} unique days.\")\n",
    "        daily_sentiment_dfs[ticker] = daily_agg_df\n",
    "\n",
    "    print(\"\\n--- All NLP Processing and Aggregation Complete ---\")\n",
    "    \n",
    "    # --- 6. Display a Sample of the Final Output ---\n",
    "    print(\"\\n--- Sample of Final Daily Sentiment DataFrame for 'AAPL' ---\")\n",
    "    if 'AAPL' in daily_sentiment_dfs and not daily_sentiment_dfs['AAPL'].empty:\n",
    "        display(daily_sentiment_dfs['AAPL'].head())\n",
    "\n",
    "else:\n",
    "    print(\"ERROR: 'company_top_articles' dictionary not found. Please run the previous cells first.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f349f844",
   "metadata": {},
   "source": [
    "#### Now the dataframe has composte sentiment scores for each day and for each company.\n",
    "We just want to fill in missing values via a simple `backward_fill` and `forward_fill` imputation strategy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b19232b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"3.1.10: Impute Missing Values in Daily Company Sentiment\")\n",
    "print(\"-\"*30)\n",
    "\n",
    "if 'daily_sentiment_dfs' in locals() and isinstance(daily_sentiment_dfs, dict):\n",
    "\n",
    "    # Create the full date range for the entire analysis period\n",
    "    full_date_range = pd.date_range(start=f'{START_YEAR}-01-01', end=f'{END_YEAR}-12-31', freq='D')\n",
    "    \n",
    "    imputed_sentiment_dfs = {}\n",
    "    \n",
    "    print(\"Starting imputation for company-specific sentiment data...\")\n",
    "\n",
    "    for ticker, df in daily_sentiment_dfs.items():\n",
    "        print(f\"  > Processing {ticker}...\")\n",
    "        \n",
    "        if df.empty:\n",
    "            print(f\"    - DataFrame for {ticker} is empty. Skipping.\")\n",
    "            imputed_sentiment_dfs[ticker] = pd.DataFrame()\n",
    "            continue\n",
    "        \n",
    "        # Ensure 'Period' is a datetime object and set it as the index\n",
    "        df['Period'] = pd.to_datetime(df['Period'])\n",
    "        df = df.set_index('Period')\n",
    "        \n",
    "        original_rows = len(df)\n",
    "        \n",
    "        # Reindex the DataFrame to include all days in the range, creating NaNs for missing days\n",
    "        imputed_df = df.reindex(full_date_range)\n",
    "        \n",
    "        # --- Imputation Strategy ---\n",
    "        # 1. For 'article_volume', missing days correctly mean 0 articles were published.\n",
    "        imputed_df['article_volume'] = imputed_df['article_volume'].fillna(0)\n",
    "        \n",
    "        # 2. For sentiment and density scores, the last known value persists.\n",
    "        sentiment_cols = [col for col in imputed_df.columns if col != 'article_volume']\n",
    "        imputed_df[sentiment_cols] = imputed_df[sentiment_cols].ffill().bfill()\n",
    "        \n",
    "        # Reset the index to bring 'Period' back as a column\n",
    "        imputed_df = imputed_df.reset_index().rename(columns={'index': 'Period'})\n",
    "        \n",
    "        imputed_sentiment_dfs[ticker] = imputed_df\n",
    "        \n",
    "        print(f\"    - Imputation complete. Rows changed from {original_rows} to {len(imputed_df)}.\")\n",
    "\n",
    "    # Overwrite the old dictionary with the new one containing the complete, imputed data\n",
    "    daily_sentiment_dfs = imputed_sentiment_dfs\n",
    "    \n",
    "    print(\"\\nImputation process complete for all tickers.\")\n",
    "    \n",
    "    # Display a sample to verify the imputation\n",
    "    print(\"\\n--- Sample of Imputed 'NVDA' DataFrame (showing start of 2018) ---\")\n",
    "    if 'NVDA' in daily_sentiment_dfs and not daily_sentiment_dfs['NVDA'].empty:\n",
    "        display(daily_sentiment_dfs['NVDA'].head())\n",
    "\n",
    "else:\n",
    "    print(\"ERROR: 'daily_sentiment_dfs' dictionary not found. Please run cell 3.1.9 first.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3e454d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"3.1.11: Save Daily Sentiment DataFrames to a File\")\n",
    "print(\"-\"*30)\n",
    "\n",
    "import pickle\n",
    "import os\n",
    "\n",
    "# Check if the dictionary from the previous cell exists.\n",
    "if 'daily_sentiment_dfs' in locals() and isinstance(daily_sentiment_dfs, dict):\n",
    "    \n",
    "    # Define the output directory and filename.\n",
    "    output_dir = \"Data\"\n",
    "    output_filename = \"daily_sentiment_dfs.pkl\"\n",
    "    output_path = os.path.join(output_dir, output_filename)\n",
    "    \n",
    "    # Ensure the output directory exists.\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    print(f\"Saving the 'daily_sentiment_dfs' dictionary to '{output_path}'...\")\n",
    "    print(\"This file will serve as a checkpoint to avoid re-running the NLP analysis.\")\n",
    "\n",
    "    try:\n",
    "        # Open the file in write-binary ('wb') mode.\n",
    "        with open(output_path, 'wb') as f:\n",
    "            # Use pickle.dump() to serialize the entire dictionary object into the file.\n",
    "            pickle.dump(daily_sentiment_dfs, f)\n",
    "        \n",
    "        print(f\"\\nDictionary successfully saved to '{output_path}'.\")\n",
    "        \n",
    "        # Provide a snippet for how to load the data in a future session.\n",
    "        print(\"\\nTo load this data in a future session, you can use the following code:\")\n",
    "        print(\"------------------------------------------------------------------\")\n",
    "        print(\"import pickle\")\n",
    "        print(f\"with open('{output_path}', 'rb') as f:\")\n",
    "        print(\"    daily_sentiment_dfs = pickle.load(f)\")\n",
    "        print(\"------------------------------------------------------------------\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"\\nAn error occurred while saving the dictionary: {e}\")\n",
    "else:\n",
    "    print(\"ERROR: 'daily_sentiment_dfs' dictionary not found. Please run cell 3.1.9 first.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6adb6e92",
   "metadata": {},
   "source": [
    "### 3.2: NLP Pipeline for `market_headlines_df`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3a14344",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"3.2.0: Setup and Load Market Headlines Data\")\n",
    "print(\"-\"*30)\n",
    "\n",
    "import sys\n",
    "# Ensure langdetect is installed for language filtering\n",
    "!{sys.executable} -m pip install langdetect --quiet\n",
    "\n",
    "import pandas as pd\n",
    "import os\n",
    "from langdetect import detect, LangDetectException\n",
    "\n",
    "# Define the file path for the market headlines CSV\n",
    "MARKET_HEADLINES_FILE = os.path.join(\"Data\", \"sp500_headlines_2008_2024.csv\")\n",
    "\n",
    "# --- Load the Dataset ---\n",
    "# Prioritize using the DataFrame if it's already in memory from cell 2.3.1.\n",
    "if 'market_headlines_df' in locals() and isinstance(market_headlines_df, pd.DataFrame):\n",
    "    print(\"Using 'market_headlines_df' from the current session.\")\n",
    "    # Create a copy to avoid modifying the original DataFrame\n",
    "    headlines_df = market_headlines_df.copy()\n",
    "else:\n",
    "    # Fallback to loading from the CSV file\n",
    "    print(f\"Loading market headlines from '{MARKET_HEADLINES_FILE}'...\")\n",
    "    try:\n",
    "        headlines_df = pd.read_csv(MARKET_HEADLINES_FILE)\n",
    "        # Ensure date column is in the correct format if loading fresh\n",
    "        headlines_df['Date'] = pd.to_datetime(headlines_df['Date'])\n",
    "        print(\"DataFrame loaded successfully.\")\n",
    "    except FileNotFoundError:\n",
    "        print(f\"ERROR: File not found at '{MARKET_HEADLINES_FILE}'. Please run cell 2.3.1 first.\")\n",
    "        headlines_df = pd.DataFrame() # Create empty DF to prevent downstream errors\n",
    "\n",
    "if not headlines_df.empty:\n",
    "    print(f\"\\nLoaded {len(headlines_df):,} total headlines for the period {START_YEAR}-{END_YEAR}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f49d2f41",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"3.2.1: Filter for English-Language Headlines\")\n",
    "print(\"-\"*30)\n",
    "\n",
    "if not 'headlines_df' in locals() or headlines_df.empty:\n",
    "    print(\"ERROR: 'headlines_df' not found or is empty. Skipping filtering.\")\n",
    "else:\n",
    "    # This function checks if a given text is English.\n",
    "    def is_english(text):\n",
    "        try:\n",
    "            # Check for valid string input for first 100 characters\n",
    "            return isinstance(text, str) and detect(text[:100]) == 'en'\n",
    "        except LangDetectException:\n",
    "            # If detection fails, assume it's not the language we want.\n",
    "            return False\n",
    "\n",
    "    print(\"Filtering for English headlines. This may take a moment...\")\n",
    "    # Apply the language filter to the 'Title' column\n",
    "    english_mask = headlines_df['Title'].apply(is_english)\n",
    "    english_market_headlines_df = headlines_df[english_mask]\n",
    "\n",
    "    original_count = len(headlines_df)\n",
    "    filtered_count = len(english_market_headlines_df)\n",
    "    print(f\"Filtering complete.\")\n",
    "    print(f\"  > Original headline count: {original_count:,}\")\n",
    "    print(f\"  > English headline count:  {filtered_count:,} ({filtered_count/original_count:.2%})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34b1199c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"3.2.2: Analyze Data for Missing Days\")\n",
    "print(\"-\"*30)\n",
    "\n",
    "if 'english_market_headlines_df' in locals() and not english_market_headlines_df.empty:\n",
    "    # Create a complete date range for our analysis period\n",
    "    full_date_range = pd.date_range(start=f'{START_YEAR}-01-01', end=f'{END_YEAR}-12-31', freq='D')\n",
    "    \n",
    "    # Get the unique days that have at least one headline\n",
    "    unique_days_with_headlines = english_market_headlines_df['Date'].nunique()\n",
    "    total_days_in_period = len(full_date_range)\n",
    "    missing_days = total_days_in_period - unique_days_with_headlines\n",
    "    \n",
    "    print(f\"Analyzing headline coverage from {START_YEAR} to {END_YEAR} ({total_days_in_period} days total).\")\n",
    "    print(f\"  > Unique days with at least one headline: {unique_days_with_headlines:,}\")\n",
    "    print(f\"  > Days with NO headlines (to be imputed): {missing_days:,}\")\n",
    "\n",
    "    if missing_days > 0:\n",
    "        print(\"\\nMissing days detected. Imputation will be required in a later step.\")\n",
    "    else:\n",
    "        print(\"\\nComplete daily coverage found. No imputation needed.\")\n",
    "else:\n",
    "    print(\"ERROR: 'english_market_headlines_df' not found. Cannot perform analysis.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c805497e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"3.2.3: Calculate and Aggregate Daily Market Sentiment\")\n",
    "print(\"-\"*30)\n",
    "\n",
    "if 'english_market_headlines_df' in locals() and not english_market_headlines_df.empty:\n",
    "    \n",
    "    # --- 1. Calculate Sentiment for Each Headline ---\n",
    "    # Register tqdm with pandas and set the description for the progress bar.\n",
    "    tqdm.pandas(desc=\"Analyzing market headlines\")\n",
    "    \n",
    "    # Since headlines are short, the chunking function will process them quickly.\n",
    "    # We re-use it for consistency with the previous NLP task.\n",
    "    english_market_headlines_df['sentiment_score'] = english_market_headlines_df['Title'].progress_apply(analyze_sentiment_with_chunking)\n",
    "    \n",
    "    # --- 2. Aggregate to a Daily Time-Series ---\n",
    "    print(\"\\nAggregating sentiment scores into a daily average...\")\n",
    "    daily_market_sentiment_df = english_market_headlines_df.groupby('Date').agg(\n",
    "        market_average_sentiment=('sentiment_score', 'mean')\n",
    "    ).reset_index()\n",
    "    \n",
    "    # Rename 'Date' to 'Period' for consistency\n",
    "    daily_market_sentiment_df = daily_market_sentiment_df.rename(columns={'Date': 'Period'})\n",
    "\n",
    "    print(\"Aggregation complete.\")\n",
    "    print(\"\\n--- Sample of Daily Market Sentiment ---\")\n",
    "    display(daily_market_sentiment_df.head())\n",
    "    \n",
    "else:\n",
    "    print(\"ERROR: 'english_market_headlines_df' is not available. Skipping sentiment analysis.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e40e8c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"3.2.4: Impute Missing Sentiment Values\")\n",
    "print(\"-\"*30)\n",
    "\n",
    "if 'daily_market_sentiment_df' in locals() and not daily_market_sentiment_df.empty:\n",
    "    \n",
    "    # Create a copy to work with, preventing state issues on re-runs\n",
    "    df_to_impute = daily_market_sentiment_df.copy()\n",
    "\n",
    "    # Ensure 'Period' column is in datetime format\n",
    "    df_to_impute['Period'] = pd.to_datetime(df_to_impute['Period'])\n",
    "\n",
    "    # Set 'Period' as the index to perform time-series operations\n",
    "    df_to_impute = df_to_impute.set_index('Period')\n",
    "    \n",
    "    # Create the full daily date range again\n",
    "    full_date_range = pd.date_range(start=f'{START_YEAR}-01-01', end=f'{END_YEAR}-12-31', freq='D')\n",
    "    \n",
    "    # Reindex the DataFrame to include all days in the range\n",
    "    imputed_market_sentiment_df = df_to_impute.reindex(full_date_range)\n",
    "    \n",
    "    # Use forward-fill and then back-fill to handle all missing values\n",
    "    imputed_market_sentiment_df['market_average_sentiment'] = imputed_market_sentiment_df['market_average_sentiment'].ffill().bfill()\n",
    "    \n",
    "    # Reset the index to bring 'Period' back to a column\n",
    "    imputed_market_sentiment_df = imputed_market_sentiment_df.reset_index().rename(columns={'index': 'Period'})\n",
    "\n",
    "    print(f\"Imputation complete. DataFrame now contains {len(imputed_market_sentiment_df)} days.\")\n",
    "    print(\"\\n--- Sample of Final Imputed Market Sentiment ---\")\n",
    "    display(imputed_market_sentiment_df.head())\n",
    "\n",
    "else:\n",
    "    print(\"ERROR: 'daily_market_sentiment_df' is not available. Skipping imputation.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39ebe4c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"3.2.5: Save Final Market Sentiment DataFrame\")\n",
    "print(\"-\"*30)\n",
    "\n",
    "import pickle\n",
    "import os\n",
    "\n",
    "if 'imputed_market_sentiment_df' in locals() and not imputed_market_sentiment_df.empty:\n",
    "    \n",
    "    output_dir = \"Data\"\n",
    "    output_filename = \"market_sentiment_df.pkl\"\n",
    "    output_path = os.path.join(output_dir, output_filename)\n",
    "    \n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    print(f\"Saving the final market sentiment DataFrame to '{output_path}'...\")\n",
    "\n",
    "    try:\n",
    "        with open(output_path, 'wb') as f:\n",
    "            pickle.dump(imputed_market_sentiment_df, f)\n",
    "        \n",
    "        print(f\"\\nDataFrame successfully saved to '{output_path}'.\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"\\nAn error occurred while saving the DataFrame: {e}\")\n",
    "else:\n",
    "    print(\"ERROR: 'imputed_market_sentiment_df' not found. Nothing to save.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe84931a",
   "metadata": {},
   "source": [
    "### 3.3: NLP Pipeline for `filings_df`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33c830b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"3.3.0: Setup and Environment Check for SEC Filings NLP\")\n",
    "print(\"-\"*30)\n",
    "\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# Check if 'filings_df' is already loaded in the environment\n",
    "if 'filings_df' in locals() and isinstance(filings_df, pd.DataFrame) and not filings_df.empty:\n",
    "    print(\"`filings_df` is loaded and ready for processing.\")\n",
    "    print(f\"Total filings to process: {len(filings_df):,}\")\n",
    "    # Create a working copy to avoid modifying the original DataFrame\n",
    "    working_filings_df = filings_df.copy()\n",
    "else:\n",
    "    print(\"`filings_df` not found or is empty. Attempting to load from file...\")\n",
    "    data_path = \"Data/filtered_filings.csv.gz\"\n",
    "    \n",
    "    try:\n",
    "        print(f\"Loading from '{data_path}'...\")\n",
    "        # Load the dataframe from the gzipped CSV\n",
    "        filings_df = pd.read_csv(data_path, compression='gzip')\n",
    "        \n",
    "        # Convert 'filing_date' back to datetime objects, as CSVs don't preserve the type\n",
    "        if 'filing_date' in filings_df.columns:\n",
    "            filings_df['filing_date'] = pd.to_datetime(filings_df['filing_date'])\n",
    "\n",
    "        print(f\"Successfully loaded `filings_df` with {len(filings_df):,} records.\")\n",
    "        \n",
    "        # Create a working copy for the NLP tasks\n",
    "        working_filings_df = filings_df.copy()\n",
    "        print(f\"Total filings to process: {len(working_filings_df):,}\")\n",
    "\n",
    "    except FileNotFoundError:\n",
    "        print(f\"ERROR: Backup file not found at '{data_path}'.\")\n",
    "        print(\"Please run Section 2.4 to generate it.\")\n",
    "        # Create an empty dataframe to prevent downstream errors\n",
    "        working_filings_df = pd.DataFrame()\n",
    "    except Exception as e:\n",
    "        print(f\"An unexpected error occurred while loading the data: {e}\")\n",
    "        # Create an empty dataframe to prevent downstream errors\n",
    "        working_filings_df = pd.DataFrame()\n",
    "\n",
    "if 'working_filings_df' not in locals():\n",
    "     working_filings_df = pd.DataFrame()\n",
    "\n",
    "if not working_filings_df.empty:\n",
    "    print(\"`working_filings_df` is ready.\")\n",
    "else:\n",
    "    print(\"Warning: `working_filings_df` is empty. Subsequent cells may not run correctly.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4171488e",
   "metadata": {},
   "outputs": [],
   "source": [
    "for item in working_filings_df['text']:\n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f0ad6dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"3.3.1: Normalize Filing Text using FinBERT Tokenizer\")\n",
    "print(\"-\"*30)\n",
    "\n",
    "import pandas as pd\n",
    "from transformers import AutoTokenizer\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# We use the same tokenizer as the sentiment model to ensure consistency.\n",
    "MODEL_NAME = \"ProsusAI/finbert\"\n",
    "try:\n",
    "    tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "    print(f\"Successfully loaded tokenizer for '{MODEL_NAME}'.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error loading tokenizer: {e}\")\n",
    "    tokenizer = None\n",
    "\n",
    "def normalize_text_with_tokenizer(text, tokenizer_instance):\n",
    "    \"\"\"\n",
    "    Cleans and normalizes text by tokenizing and re-joining it using a\n",
    "    pretrained transformer tokenizer. This is the most effective way to\n",
    "    prepare text for the corresponding model.\n",
    "    \"\"\"\n",
    "    if not isinstance(text, str) or not tokenizer_instance:\n",
    "        return \"\"\n",
    "    \n",
    "    # This process handles special characters, whitespace, and subword splitting\n",
    "    # according to the model's vocabulary.\n",
    "    tokens = tokenizer_instance.tokenize(text)\n",
    "    \n",
    "    # Convert tokens back to a single string.\n",
    "    return tokenizer_instance.convert_tokens_to_string(tokens)\n",
    "\n",
    "if 'working_filings_df' in locals() and not working_filings_df.empty and tokenizer:\n",
    "    print(\"Applying tokenizer-based normalization to the 'text' column...\")\n",
    "    tqdm.pandas(desc=\"Normalizing filing text\")\n",
    "    working_filings_df['cleaned_text'] = working_filings_df['text'].progress_apply(\n",
    "        normalize_text_with_tokenizer, \n",
    "        tokenizer_instance=tokenizer\n",
    "    )\n",
    "    print(\"Text normalization complete.\")\n",
    "elif tokenizer is None:\n",
    "    print(\"Skipping normalization as tokenizer failed to load.\")\n",
    "else:\n",
    "    print(\"Skipping text normalization as `working_filings_df` is empty.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "341d7a48",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(working_filings_df.head(200))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "361f5074",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"3.3.2: Inspect for Empty Filings Post-Cleaning\")\n",
    "print(\"-\"*30)\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "if not working_filings_df.empty:\n",
    "    # A filing is considered \"empty\" if its cleaned text has fewer than 100 characters.\n",
    "    # This lower threshold is less likely to incorrectly flag short filings like 8-Ks.\n",
    "    char_threshold = 100\n",
    "    working_filings_df['is_empty'] = working_filings_df['cleaned_text'].str.len() < char_threshold\n",
    "    \n",
    "    empty_count = working_filings_df['is_empty'].sum()\n",
    "    total_count = len(working_filings_df)\n",
    "    \n",
    "    print(f\"Found {empty_count} filings with insufficient text (less than {char_threshold} characters) after cleaning.\")\n",
    "    \n",
    "    if empty_count > 0:\n",
    "        print(\"These will be assigned a neutral sentiment score by skipping NLP processing.\")\n",
    "        # Replace the text of empty filings with NaN\n",
    "        working_filings_df.loc[working_filings_df['is_empty'], 'cleaned_text'] = np.nan\n",
    "else:\n",
    "    print(\"Skipping inspection as `working_filings_df` is empty.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb21c7ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(working_filings_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9415e98",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"3.3.3: Calculate Filing Sentiment and Reshape Data\")\n",
    "print(\"-\"*30)\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "import torch\n",
    "from transformers import pipeline, AutoTokenizer\n",
    "\n",
    "# --- 0. Suppress Warnings & Setup Environment ---\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning, module='transformers')\n",
    "device = 0 if torch.cuda.is_available() else -1\n",
    "print(f\"Using device: {'GPU' if device == 0 else 'CPU'}\")\n",
    "\n",
    "# --- 1. Helper Functions for Advanced Sentiment Analysis ---\n",
    "MODEL_NAME = \"ProsusAI/finbert\"\n",
    "try:\n",
    "    # Initialize the pipeline once\n",
    "    sentiment_pipeline = pipeline(\"sentiment-analysis\", model=MODEL_NAME, device=device)\n",
    "    tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "    MAX_CHUNK_LENGTH = 512\n",
    "    OVERLAP = 50\n",
    "except Exception as e:\n",
    "    print(f\"Error loading model {MODEL_NAME}: {e}\")\n",
    "    sentiment_pipeline = None\n",
    "\n",
    "def analyze_sentiment_with_full_distribution(text, pipeline_instance, tokenizer_instance):\n",
    "    \"\"\"\n",
    "    Analyzes sentiment using the model's full probability distribution\n",
    "    to create a continuous score from -1 to 1.\n",
    "    \"\"\"\n",
    "    if pd.isna(text) or not isinstance(text, str) or not text.strip() or not pipeline_instance:\n",
    "        return 0.0\n",
    "\n",
    "    tokens = tokenizer_instance.encode(text, add_special_tokens=False)\n",
    "    if not tokens:\n",
    "        return 0.0\n",
    "    \n",
    "    chunk_step = MAX_CHUNK_LENGTH - OVERLAP\n",
    "    token_chunks = [tokens[i:i + MAX_CHUNK_LENGTH] for i in range(0, len(tokens), chunk_step)]\n",
    "    text_chunks = [tokenizer_instance.decode(chunk) for chunk in token_chunks]\n",
    "    \n",
    "    if not text_chunks:\n",
    "        return 0.0\n",
    "\n",
    "    try:\n",
    "        # KEY CHANGE: Get scores for ALL labels (positive, negative, neutral)\n",
    "        sentiments_per_chunk = pipeline_instance(text_chunks, truncation=True, return_all_scores=True)\n",
    "    except Exception:\n",
    "        return 0.0\n",
    "    \n",
    "    chunk_scores = []\n",
    "    for sentiment_list in sentiments_per_chunk:\n",
    "        # sentiment_list is now like [{'label':'positive', 'score':...}, {'label':'negative', 'score':...}]\n",
    "        scores_dict = {item['label'].lower(): item['score'] for item in sentiment_list}\n",
    "        \n",
    "        # The score is the balance between positive and negative confidence\n",
    "        score = scores_dict.get('positive', 0.0) - scores_dict.get('negative', 0.0)\n",
    "        chunk_scores.append(score)\n",
    "        \n",
    "    return sum(chunk_scores) / len(chunk_scores) if chunk_scores else 0.0\n",
    "\n",
    "# --- 2. Main Processing Logic with Manual Progress Updates ---\n",
    "company_filings_sentiment_dfs = {}\n",
    "\n",
    "if 'working_filings_df' in locals() and not working_filings_df.empty and sentiment_pipeline:\n",
    "    total_companies = len(TARGET_TICKERS)\n",
    "    print(f\"Starting sentiment analysis for {total_companies} companies...\")\n",
    "\n",
    "    for i, ticker in enumerate(TARGET_TICKERS):\n",
    "        print(f\"\\n--- Processing company {i+1}/{total_companies}: {ticker} ---\")\n",
    "        ticker_df = working_filings_df[working_filings_df['ticker'] == ticker].copy()\n",
    "\n",
    "        if ticker_df.empty:\n",
    "            print(f\"   > No filings found for {ticker}. Skipping.\")\n",
    "            continue\n",
    "\n",
    "        sentiment_scores = []\n",
    "        total_filings = len(ticker_df)\n",
    "        print(f\"   > Analyzing {total_filings} filings for {ticker}...\")\n",
    "        \n",
    "        for idx, row in ticker_df.iterrows():\n",
    "            score = analyze_sentiment_with_full_distribution(row['cleaned_text'], sentiment_pipeline, tokenizer)\n",
    "            sentiment_scores.append(score)\n",
    "            \n",
    "            if (len(sentiment_scores)) % 20 == 0 and len(sentiment_scores) > 0:\n",
    "                print(f\"   ... processed {len(sentiment_scores)}/{total_filings} filings for {ticker}\")\n",
    "\n",
    "        print(f\"   > Finished analysis for {ticker}.\")\n",
    "        ticker_df['sentiment_score'] = sentiment_scores\n",
    "        ticker_df['sentiment_score'] = ticker_df['sentiment_score'].fillna(0.0)\n",
    "\n",
    "        pivoted_df = ticker_df.pivot_table(index='filing_date', columns='form_type', values='sentiment_score', aggfunc='mean')\n",
    "        \n",
    "        if not pivoted_df.empty:\n",
    "            full_date_range = pd.date_range(start='2018-01-01', end='2024-12-31', freq='D')\n",
    "            pivoted_df = pivoted_df.reindex(full_date_range)\n",
    "            pivoted_df.ffill(inplace=True)\n",
    "            pivoted_df.bfill(inplace=True)\n",
    "        \n",
    "        pivoted_df = pivoted_df.rename(columns={'10-K': '10-K_sentiment', '10-Q': '10-Q_sentiment', '8-K': '8-K_sentiment'})\n",
    "        company_filings_sentiment_dfs[ticker] = pivoted_df\n",
    "\n",
    "    print(\"\\n\\nAll tickers processed. Dictionary of filing sentiment DataFrames is ready.\")\n",
    "\n",
    "    if company_filings_sentiment_dfs:\n",
    "        display_ticker = next(iter(company_filings_sentiment_dfs))\n",
    "        print(f\"\\nSample of daily-indexed, forward-filled data for {display_ticker}:\\n\")\n",
    "        display(company_filings_sentiment_dfs[display_ticker].head(10))\n",
    "else:\n",
    "    print(\"`working_filings_df` is empty or sentiment pipeline failed to load. Skipping NLP and reshaping.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a402fe22",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(pd.DataFrame([company_filings_sentiment_dfs['AAPL']['8-K_sentiment']]).describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ff0a0b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"3.3.4: Impute Missing Daily Filing Sentiments\")\n",
    "print(\"-\"*30)\n",
    "\n",
    "if company_filings_sentiment_dfs:\n",
    "    \n",
    "    imputed_filings_sentiment_dfs = {}\n",
    "    full_date_range = pd.date_range(start=f'{START_YEAR}-01-01', end=f'{END_YEAR}-12-31', freq='D')\n",
    "\n",
    "    for ticker, df in company_filings_sentiment_dfs.items():\n",
    "        # Reindex to the full daily range\n",
    "        imputed_df = df.reindex(full_date_range)\n",
    "        \n",
    "        # Forward-fill and then back-fill to ensure no NaNs remain\n",
    "        imputed_df = imputed_df.ffill().bfill()\n",
    "        \n",
    "        imputed_df = imputed_df.reset_index().rename(columns={'index': 'Period'})\n",
    "        imputed_filings_sentiment_dfs[ticker] = imputed_df\n",
    "        print(f\"  > Imputed daily filing sentiment for {ticker}.\")\n",
    "        \n",
    "    # Overwrite the old dictionary with the imputed one\n",
    "    company_filings_sentiment_dfs = imputed_filings_sentiment_dfs\n",
    "    \n",
    "    print(\"\\nImputation complete for all tickers.\")\n",
    "    print(\"\\n--- Sample of Final Imputed Filing Sentiment for 'AAPL' ---\")\n",
    "    display(company_filings_sentiment_dfs['AAPL'].head())\n",
    "    \n",
    "else:\n",
    "    print(\"`company_filings_sentiment_dfs` is empty. Nothing to impute.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32297ba6",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"3.3.5: Save Filing Sentiment DataFrames\")\n",
    "print(\"-\"*30)\n",
    "\n",
    "import pickle\n",
    "\n",
    "if company_filings_sentiment_dfs:\n",
    "    \n",
    "    output_path = os.path.join(\"Data\", \"company_filings_sentiment.pkl\")\n",
    "    os.makedirs(\"Data\", exist_ok=True)\n",
    "    \n",
    "    print(f\"Saving the 'company_filings_sentiment_dfs' dictionary to '{output_path}'...\")\n",
    "    with open(output_path, 'wb') as f:\n",
    "        pickle.dump(company_filings_sentiment_dfs, f)\n",
    "    print(\"Save complete.\")\n",
    "\n",
    "else:\n",
    "    print(\"Dictionary is empty. Nothing to save.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed3a530a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"3.3.6: Checkpoint - How to Load Filing Sentiment Data\")\n",
    "print(\"-\"*30)\n",
    "\n",
    "import pickle\n",
    "import os\n",
    "\n",
    "path = os.path.join(\"Data\", \"company_filings_sentiment.pkl\")\n",
    "\n",
    "print(\"To load the saved filing sentiment data in a future session, run this code:\")\n",
    "print(\"-\" * 65)\n",
    "print(\"import pickle\")\n",
    "print(f\"with open('{path}', 'rb') as f:\")\n",
    "print(\"    loaded_filings_sentiment = pickle.load(f)\")\n",
    "print(\"\\n# Example: Accessing the AAPL DataFrame\")\n",
    "print(\"aapl_df = loaded_filings_sentiment.get('AAPL')\")\n",
    "print(\"-\" * 65)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d114d02f",
   "metadata": {},
   "source": [
    "#### NLP Pipeline is now complete. We have all the necessary data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc025e60",
   "metadata": {},
   "source": [
    "## Phase 4: Creating the `DataFrames`\n",
    "\n",
    "In this last part, we just merge our dataframes into one dataframe per company. All dataframes will have the same features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a4fba737",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.1: Load All Processed Data Sources\n",
      "------------------------------\n",
      "Loading 'Data\\daily_sentiment_dfs.pkl'...\n",
      "Loading 'Data\\market_sentiment_df.pkl'...\n",
      "Loading 'Data\\company_filings_sentiment.pkl'...\n",
      "Accessing 'insider_datasets' from the current session.\n",
      "\n",
      "Data loading process complete.\n"
     ]
    }
   ],
   "source": [
    "print(\"4.1: Load All Processed Data Sources\")\n",
    "print(\"-\"*30)\n",
    "\n",
    "import os\n",
    "import pickle\n",
    "\n",
    "# --- 1. Define Helper Function for Loading ---\n",
    "def load_pickle_data(filename):\n",
    "    \"\"\"Loads a pickle file from the 'Data' directory and returns its content.\"\"\"\n",
    "    path = os.path.join(\"Data\", filename)\n",
    "    print(f\"Loading '{path}'...\")\n",
    "    try:\n",
    "        with open(path, 'rb') as f:\n",
    "            return pickle.load(f)\n",
    "    except FileNotFoundError:\n",
    "        print(f\"  > ERROR: File not found. Please ensure the required file exists.\")\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        print(f\"  > ERROR: An unexpected error occurred: {e}\")\n",
    "        return None\n",
    "\n",
    "# --- 2. Load the Datasets ---\n",
    "# Company-specific news sentiment\n",
    "daily_sentiment_dfs = load_pickle_data(\"daily_sentiment_dfs.pkl\")\n",
    "# General market news sentiment\n",
    "market_sentiment_df = load_pickle_data(\"market_sentiment_df.pkl\")\n",
    "# Company-specific SEC filing sentiment\n",
    "company_filings_sentiment_dfs = load_pickle_data(\"company_filings_sentiment.pkl\")\n",
    "\n",
    "# --- 3. Access Live Insider Data ---\n",
    "# This dictionary was created in Section 2.1 and should be in memory.\n",
    "if 'insider_datasets' in locals():\n",
    "    print(\"Accessing 'insider_datasets' from the current session.\")\n",
    "else:\n",
    "    print(\"  > WARNING: 'insider_datasets' not found in memory. This data will be missing from the final merge.\")\n",
    "    insider_datasets = {} # Create an empty dict to prevent errors\n",
    "\n",
    "print(\"\\nData loading process complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "015c680d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.2: Verify Loaded DataFrames\n",
      "------------------------------\n",
      "\n",
      "--- Sample: Daily Sentiment for AAPL ---\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Period</th>\n",
       "      <th>article_volume</th>\n",
       "      <th>average_news_sentiment</th>\n",
       "      <th>AVG_AAPL_Density</th>\n",
       "      <th>AVG_NVDA_Density</th>\n",
       "      <th>AVG_GOOGL_Density</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2018-01-01</td>\n",
       "      <td>8.0</td>\n",
       "      <td>-0.017894</td>\n",
       "      <td>0.068807</td>\n",
       "      <td>0.0125</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2018-01-02</td>\n",
       "      <td>51.0</td>\n",
       "      <td>-0.127239</td>\n",
       "      <td>0.166147</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2018-01-03</td>\n",
       "      <td>62.0</td>\n",
       "      <td>-0.518287</td>\n",
       "      <td>0.124039</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Period  article_volume  average_news_sentiment  AVG_AAPL_Density  \\\n",
       "0 2018-01-01             8.0               -0.017894          0.068807   \n",
       "1 2018-01-02            51.0               -0.127239          0.166147   \n",
       "2 2018-01-03            62.0               -0.518287          0.124039   \n",
       "\n",
       "   AVG_NVDA_Density  AVG_GOOGL_Density  \n",
       "0            0.0125                0.0  \n",
       "1            0.0000                0.0  \n",
       "2            0.0000                0.0  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Sample: General Market Sentiment ---\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Period</th>\n",
       "      <th>market_average_sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2018-01-01</td>\n",
       "      <td>0.311223</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2018-01-02</td>\n",
       "      <td>0.311223</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2018-01-03</td>\n",
       "      <td>-0.018505</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Period  market_average_sentiment\n",
       "0 2018-01-01                  0.311223\n",
       "1 2018-01-02                  0.311223\n",
       "2 2018-01-03                 -0.018505"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Sample: Filing Sentiment for AAPL ---\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>form_type</th>\n",
       "      <th>Period</th>\n",
       "      <th>10-K_sentiment</th>\n",
       "      <th>10-Q_sentiment</th>\n",
       "      <th>8-K_sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2018-01-01</td>\n",
       "      <td>-0.146227</td>\n",
       "      <td>-0.217508</td>\n",
       "      <td>-0.023673</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2018-01-02</td>\n",
       "      <td>-0.146227</td>\n",
       "      <td>-0.217508</td>\n",
       "      <td>-0.023673</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2018-01-03</td>\n",
       "      <td>-0.146227</td>\n",
       "      <td>-0.217508</td>\n",
       "      <td>-0.023673</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "form_type     Period  10-K_sentiment  10-Q_sentiment  8-K_sentiment\n",
       "0         2018-01-01       -0.146227       -0.217508      -0.023673\n",
       "1         2018-01-02       -0.146227       -0.217508      -0.023673\n",
       "2         2018-01-03       -0.146227       -0.217508      -0.023673"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Sample: Insider Data for AAPL ---\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Period</th>\n",
       "      <th>MSPR</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2018-Q1</td>\n",
       "      <td>-100.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2018-Q1</td>\n",
       "      <td>7.840257</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2018-Q2</td>\n",
       "      <td>-22.737514</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Period        MSPR\n",
       "0  2018-Q1 -100.000000\n",
       "1  2018-Q1    7.840257\n",
       "2  2018-Q2  -22.737514"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(\"4.2: Verify Loaded DataFrames\")\n",
    "print(\"-\"*30)\n",
    "\n",
    "# Verify company news sentiment\n",
    "if daily_sentiment_dfs:\n",
    "    print(\"\\n--- Sample: Daily Sentiment for AAPL ---\")\n",
    "    display(daily_sentiment_dfs.get('AAPL', pd.DataFrame()).head(3))\n",
    "else:\n",
    "    print(\"\\n'daily_sentiment_dfs' is not loaded.\")\n",
    "\n",
    "# Verify market sentiment\n",
    "if market_sentiment_df is not None:\n",
    "    print(\"\\n--- Sample: General Market Sentiment ---\")\n",
    "    display(market_sentiment_df.head(3))\n",
    "else:\n",
    "    print(\"\\n'market_sentiment_df' is not loaded.\")\n",
    "    \n",
    "# Verify filing sentiment\n",
    "if company_filings_sentiment_dfs:\n",
    "    print(\"\\n--- Sample: Filing Sentiment for AAPL ---\")\n",
    "    display(company_filings_sentiment_dfs.get('AAPL', pd.DataFrame()).head(3))\n",
    "else:\n",
    "    print(\"\\n'company_filings_sentiment_dfs' is not loaded.\")\n",
    "\n",
    "# Verify insider data\n",
    "if insider_datasets:\n",
    "    print(\"\\n--- Sample: Insider Data for AAPL ---\")\n",
    "    display(insider_datasets.get('AAPL', pd.DataFrame()).head(3))\n",
    "else:\n",
    "    print(\"\\n'insider_datasets' is not loaded.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1be88f92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.3: Merge All Data Sources into Final Company DataFrames\n",
      "------------------------------\n",
      "Starting merge process for each target ticker...\n",
      "  > Processing AAPL...\n",
      "    - Merge complete for AAPL. Final shape: (2557, 11)\n",
      "  > Processing NVDA...\n",
      "    - Merge complete for NVDA. Final shape: (2557, 11)\n",
      "  > Processing GOOGL...\n",
      "    - Merge complete for GOOGL. Final shape: (2557, 11)\n",
      "\n",
      "--- All companies processed. Final DataFrames are ready. ---\n",
      "\n",
      "--- Sample of Final Consolidated DataFrame for 'NVDA' ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\totob\\AppData\\Local\\Temp\\ipykernel_31624\\323115389.py:31: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  insider_df['Period'] = pd.to_datetime(insider_df['Period'])\n",
      "C:\\Users\\totob\\AppData\\Local\\Temp\\ipykernel_31624\\323115389.py:31: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  insider_df['Period'] = pd.to_datetime(insider_df['Period'])\n",
      "C:\\Users\\totob\\AppData\\Local\\Temp\\ipykernel_31624\\323115389.py:31: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  insider_df['Period'] = pd.to_datetime(insider_df['Period'])\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Period</th>\n",
       "      <th>article_volume</th>\n",
       "      <th>average_news_sentiment</th>\n",
       "      <th>AVG_AAPL_Density</th>\n",
       "      <th>AVG_NVDA_Density</th>\n",
       "      <th>AVG_GOOGL_Density</th>\n",
       "      <th>market_average_sentiment</th>\n",
       "      <th>10-K_sentiment</th>\n",
       "      <th>10-Q_sentiment</th>\n",
       "      <th>8-K_sentiment</th>\n",
       "      <th>mspr</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2018-01-01</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.062500</td>\n",
       "      <td>0.062500</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.311223</td>\n",
       "      <td>-0.073956</td>\n",
       "      <td>0.020997</td>\n",
       "      <td>-0.032128</td>\n",
       "      <td>-43.780097</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2018-01-02</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000997</td>\n",
       "      <td>0.041338</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.311223</td>\n",
       "      <td>-0.073956</td>\n",
       "      <td>0.020997</td>\n",
       "      <td>-0.032128</td>\n",
       "      <td>-43.780097</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2018-01-03</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0.234918</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.071479</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.018505</td>\n",
       "      <td>-0.073956</td>\n",
       "      <td>0.020997</td>\n",
       "      <td>-0.032128</td>\n",
       "      <td>-43.780097</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2018-01-04</td>\n",
       "      <td>13.0</td>\n",
       "      <td>-0.065777</td>\n",
       "      <td>0.005405</td>\n",
       "      <td>0.041763</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.073956</td>\n",
       "      <td>0.020997</td>\n",
       "      <td>-0.032128</td>\n",
       "      <td>-43.780097</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2018-01-05</td>\n",
       "      <td>9.0</td>\n",
       "      <td>0.375672</td>\n",
       "      <td>0.012500</td>\n",
       "      <td>0.071191</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.071511</td>\n",
       "      <td>-0.073956</td>\n",
       "      <td>0.020997</td>\n",
       "      <td>-0.032128</td>\n",
       "      <td>-43.780097</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Period  article_volume  average_news_sentiment  AVG_AAPL_Density  \\\n",
       "0 2018-01-01             1.0                0.000000          0.062500   \n",
       "1 2018-01-02             7.0                0.000000          0.000997   \n",
       "2 2018-01-03            10.0                0.234918          0.000000   \n",
       "3 2018-01-04            13.0               -0.065777          0.005405   \n",
       "4 2018-01-05             9.0                0.375672          0.012500   \n",
       "\n",
       "   AVG_NVDA_Density  AVG_GOOGL_Density  market_average_sentiment  \\\n",
       "0          0.062500                0.0                  0.311223   \n",
       "1          0.041338                0.0                  0.311223   \n",
       "2          0.071479                0.0                 -0.018505   \n",
       "3          0.041763                0.0                  0.000000   \n",
       "4          0.071191                0.0                  0.071511   \n",
       "\n",
       "   10-K_sentiment  10-Q_sentiment  8-K_sentiment       mspr  \n",
       "0       -0.073956        0.020997      -0.032128 -43.780097  \n",
       "1       -0.073956        0.020997      -0.032128 -43.780097  \n",
       "2       -0.073956        0.020997      -0.032128 -43.780097  \n",
       "3       -0.073956        0.020997      -0.032128 -43.780097  \n",
       "4       -0.073956        0.020997      -0.032128 -43.780097  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(\"4.3: Merge All Data Sources into Final Company DataFrames\")\n",
    "print(\"-\"*30)\n",
    "\n",
    "from functools import reduce\n",
    "\n",
    "final_company_dfs = {}\n",
    "\n",
    "# Check if all data sources are available before starting\n",
    "if not all([daily_sentiment_dfs, market_sentiment_df is not None, company_filings_sentiment_dfs, insider_datasets]):\n",
    "    print(\"ERROR: One or more required data sources are missing. Cannot proceed with the merge.\")\n",
    "else:\n",
    "    print(\"Starting merge process for each target ticker...\")\n",
    "    for ticker in TARGET_TICKERS:\n",
    "        print(f\"  > Processing {ticker}...\")\n",
    "        \n",
    "        # --- 1. Prepare all DataFrames for the merge ---\n",
    "        \n",
    "        # Company-specific news sentiment (Base DataFrame)\n",
    "        df1 = daily_sentiment_dfs[ticker].copy()\n",
    "        \n",
    "        # General market sentiment\n",
    "        df2 = market_sentiment_df.copy()\n",
    "\n",
    "        # Company-specific filing sentiment\n",
    "        df3 = company_filings_sentiment_dfs[ticker].copy()\n",
    "        \n",
    "        # Insider sentiment (needs pre-processing)\n",
    "        insider_df = insider_datasets[ticker].copy()\n",
    "        # Rename date column and convert to datetime\n",
    "        insider_df = insider_df.rename(columns={'filingDate': 'Period'})\n",
    "        insider_df['Period'] = pd.to_datetime(insider_df['Period'])\n",
    "        # Aggregate by day in case of multiple filings on the same day\n",
    "        df4 = insider_df.groupby('Period').agg(mspr=('MSPR', 'mean')).reset_index()\n",
    "\n",
    "        # --- 2. Perform the merge ---\n",
    "        data_frames_to_merge = [df1, df2, df3, df4]\n",
    "        \n",
    "        # Use reduce to cleanly merge all dataframes in the list on 'Period'\n",
    "        # 'how=left' ensures we start with the complete daily index from our base df\n",
    "        merged_df = reduce(lambda left, right: pd.merge(left, right, on='Period', how='left'), data_frames_to_merge)\n",
    "        \n",
    "        # --- 3. Impute the sparse 'mspr' column ---\n",
    "        # The only column with NaNs should be 'mspr' from the sparse insider data\n",
    "        merged_df['mspr'] = merged_df['mspr'].ffill().bfill()\n",
    "\n",
    "        final_company_dfs[ticker] = merged_df\n",
    "        print(f\"    - Merge complete for {ticker}. Final shape: {merged_df.shape}\")\n",
    "\n",
    "    print(\"\\n--- All companies processed. Final DataFrames are ready. ---\")\n",
    "    print(\"\\n--- Sample of Final Consolidated DataFrame for 'NVDA' ---\")\n",
    "    display(final_company_dfs['NVDA'].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "979cea6c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Period</th>\n",
       "      <th>article_volume</th>\n",
       "      <th>average_news_sentiment</th>\n",
       "      <th>AVG_AAPL_Density</th>\n",
       "      <th>AVG_NVDA_Density</th>\n",
       "      <th>AVG_GOOGL_Density</th>\n",
       "      <th>market_average_sentiment</th>\n",
       "      <th>10-K_sentiment</th>\n",
       "      <th>10-Q_sentiment</th>\n",
       "      <th>8-K_sentiment</th>\n",
       "      <th>mspr</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2018-01-01</td>\n",
       "      <td>8.0</td>\n",
       "      <td>-0.017894</td>\n",
       "      <td>0.068807</td>\n",
       "      <td>0.0125</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.311223</td>\n",
       "      <td>-0.146227</td>\n",
       "      <td>-0.217508</td>\n",
       "      <td>-0.023673</td>\n",
       "      <td>-46.079872</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2018-01-02</td>\n",
       "      <td>51.0</td>\n",
       "      <td>-0.127239</td>\n",
       "      <td>0.166147</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.311223</td>\n",
       "      <td>-0.146227</td>\n",
       "      <td>-0.217508</td>\n",
       "      <td>-0.023673</td>\n",
       "      <td>-46.079872</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2018-01-03</td>\n",
       "      <td>62.0</td>\n",
       "      <td>-0.518287</td>\n",
       "      <td>0.124039</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.018505</td>\n",
       "      <td>-0.146227</td>\n",
       "      <td>-0.217508</td>\n",
       "      <td>-0.023673</td>\n",
       "      <td>-46.079872</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2018-01-04</td>\n",
       "      <td>53.0</td>\n",
       "      <td>0.222064</td>\n",
       "      <td>0.148336</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.146227</td>\n",
       "      <td>-0.217508</td>\n",
       "      <td>-0.023673</td>\n",
       "      <td>-46.079872</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2018-01-05</td>\n",
       "      <td>104.0</td>\n",
       "      <td>-0.443398</td>\n",
       "      <td>0.173987</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.071511</td>\n",
       "      <td>-0.146227</td>\n",
       "      <td>-0.217508</td>\n",
       "      <td>-0.023673</td>\n",
       "      <td>-46.079872</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Period  article_volume  average_news_sentiment  AVG_AAPL_Density  \\\n",
       "0 2018-01-01             8.0               -0.017894          0.068807   \n",
       "1 2018-01-02            51.0               -0.127239          0.166147   \n",
       "2 2018-01-03            62.0               -0.518287          0.124039   \n",
       "3 2018-01-04            53.0                0.222064          0.148336   \n",
       "4 2018-01-05           104.0               -0.443398          0.173987   \n",
       "\n",
       "   AVG_NVDA_Density  AVG_GOOGL_Density  market_average_sentiment  \\\n",
       "0            0.0125                0.0                  0.311223   \n",
       "1            0.0000                0.0                  0.311223   \n",
       "2            0.0000                0.0                 -0.018505   \n",
       "3            0.0000                0.0                  0.000000   \n",
       "4            0.0000                0.0                  0.071511   \n",
       "\n",
       "   10-K_sentiment  10-Q_sentiment  8-K_sentiment       mspr  \n",
       "0       -0.146227       -0.217508      -0.023673 -46.079872  \n",
       "1       -0.146227       -0.217508      -0.023673 -46.079872  \n",
       "2       -0.146227       -0.217508      -0.023673 -46.079872  \n",
       "3       -0.146227       -0.217508      -0.023673 -46.079872  \n",
       "4       -0.146227       -0.217508      -0.023673 -46.079872  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(final_company_dfs['AAPL'].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2a883dfa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Period</th>\n",
       "      <th>article_volume</th>\n",
       "      <th>average_news_sentiment</th>\n",
       "      <th>AVG_AAPL_Density</th>\n",
       "      <th>AVG_NVDA_Density</th>\n",
       "      <th>AVG_GOOGL_Density</th>\n",
       "      <th>market_average_sentiment</th>\n",
       "      <th>10-K_sentiment</th>\n",
       "      <th>10-Q_sentiment</th>\n",
       "      <th>8-K_sentiment</th>\n",
       "      <th>mspr</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2018-01-01</td>\n",
       "      <td>13.0</td>\n",
       "      <td>-0.014047</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.071884</td>\n",
       "      <td>0.311223</td>\n",
       "      <td>-0.094467</td>\n",
       "      <td>-0.055268</td>\n",
       "      <td>-0.035295</td>\n",
       "      <td>-44.976149</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2018-01-02</td>\n",
       "      <td>25.0</td>\n",
       "      <td>0.031997</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.054931</td>\n",
       "      <td>0.311223</td>\n",
       "      <td>-0.094467</td>\n",
       "      <td>-0.055268</td>\n",
       "      <td>-0.035295</td>\n",
       "      <td>-44.976149</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2018-01-03</td>\n",
       "      <td>58.0</td>\n",
       "      <td>-0.333055</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.119026</td>\n",
       "      <td>-0.018505</td>\n",
       "      <td>-0.094467</td>\n",
       "      <td>-0.055268</td>\n",
       "      <td>-0.035295</td>\n",
       "      <td>-44.976149</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2018-01-04</td>\n",
       "      <td>51.0</td>\n",
       "      <td>-0.224332</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.124848</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.094467</td>\n",
       "      <td>-0.055268</td>\n",
       "      <td>-0.035295</td>\n",
       "      <td>-44.976149</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2018-01-05</td>\n",
       "      <td>64.0</td>\n",
       "      <td>0.180071</td>\n",
       "      <td>0.014286</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.106121</td>\n",
       "      <td>0.071511</td>\n",
       "      <td>-0.094467</td>\n",
       "      <td>-0.055268</td>\n",
       "      <td>-0.035295</td>\n",
       "      <td>-44.976149</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Period  article_volume  average_news_sentiment  AVG_AAPL_Density  \\\n",
       "0 2018-01-01            13.0               -0.014047          0.000000   \n",
       "1 2018-01-02            25.0                0.031997          0.000000   \n",
       "2 2018-01-03            58.0               -0.333055          0.000000   \n",
       "3 2018-01-04            51.0               -0.224332          0.000000   \n",
       "4 2018-01-05            64.0                0.180071          0.014286   \n",
       "\n",
       "   AVG_NVDA_Density  AVG_GOOGL_Density  market_average_sentiment  \\\n",
       "0               0.0           0.071884                  0.311223   \n",
       "1               0.0           0.054931                  0.311223   \n",
       "2               0.0           0.119026                 -0.018505   \n",
       "3               0.0           0.124848                  0.000000   \n",
       "4               0.0           0.106121                  0.071511   \n",
       "\n",
       "   10-K_sentiment  10-Q_sentiment  8-K_sentiment       mspr  \n",
       "0       -0.094467       -0.055268      -0.035295 -44.976149  \n",
       "1       -0.094467       -0.055268      -0.035295 -44.976149  \n",
       "2       -0.094467       -0.055268      -0.035295 -44.976149  \n",
       "3       -0.094467       -0.055268      -0.035295 -44.976149  \n",
       "4       -0.094467       -0.055268      -0.035295 -44.976149  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(final_company_dfs['GOOGL'].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b77f5e3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.4: Export Final Company DataFrames\n",
      "------------------------------\n",
      "Ensured output directory 'Datasets' exists.\n",
      "\n",
      "Saving the complete dictionary to 'Datasets\\final_company_dfs.pkl'...\n",
      "  > Dictionary saved successfully.\n",
      "\n",
      "Saving each company's DataFrame as a separate CSV file...\n",
      "  > Successfully saved 'Datasets\\AAPL_dataset.csv'\n",
      "  > Successfully saved 'Datasets\\NVDA_dataset.csv'\n",
      "  > Successfully saved 'Datasets\\GOOGL_dataset.csv'\n",
      "\n",
      "--- All datasets have been exported. ---\n"
     ]
    }
   ],
   "source": [
    "print(\"4.4: Export Final Company DataFrames\")\n",
    "print(\"-\"*30)\n",
    "\n",
    "import os\n",
    "import pickle\n",
    "\n",
    "# Check if the final dictionary exists before attempting to save.\n",
    "if 'final_company_dfs' in locals() and isinstance(final_company_dfs, dict):\n",
    "    \n",
    "    # Define the new directory for the final datasets.\n",
    "    output_dir = \"Datasets\"\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    print(f\"Ensured output directory '{output_dir}' exists.\")\n",
    "\n",
    "    # --- 1. Save the entire dictionary as a single .pkl file ---\n",
    "    pkl_file_path = os.path.join(output_dir, 'final_company_dfs.pkl')\n",
    "    print(f\"\\nSaving the complete dictionary to '{pkl_file_path}'...\")\n",
    "    try:\n",
    "        with open(pkl_file_path, 'wb') as f:\n",
    "            pickle.dump(final_company_dfs, f)\n",
    "        print(\"  > Dictionary saved successfully.\")\n",
    "    except Exception as e:\n",
    "        print(f\"  > An error occurred while saving the dictionary: {e}\")\n",
    "\n",
    "    # --- 2. Save each DataFrame as a separate .csv file ---\n",
    "    print(\"\\nSaving each company's DataFrame as a separate CSV file...\")\n",
    "    for ticker, df in final_company_dfs.items():\n",
    "        csv_file_path = os.path.join(output_dir, f\"{ticker}_dataset.csv\")\n",
    "        try:\n",
    "            # Use index=False to avoid writing the DataFrame index as a column.\n",
    "            df.to_csv(csv_file_path, index=False)\n",
    "            print(f\"  > Successfully saved '{csv_file_path}'\")\n",
    "        except Exception as e:\n",
    "            print(f\"  > An error occurred while saving the CSV for {ticker}: {e}\")\n",
    "            \n",
    "    print(\"\\n--- All datasets have been exported. ---\")\n",
    "\n",
    "else:\n",
    "    print(\"ERROR: 'final_company_dfs' dictionary not found. Nothing to export.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
