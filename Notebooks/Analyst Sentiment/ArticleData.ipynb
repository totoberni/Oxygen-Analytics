{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a1008709",
   "metadata": {},
   "source": [
    "# Advanced Financial NLP Pipeline: AAPL, NVDA, GOOGL\n",
    "\n",
    "This notebook implements the multi-feature sentiment analysis pipeline as defined in `plan.md`. We will analyze `AAPL`, `NVDA`, and `GOOGL` for the most recent 12-month period available through the Finnhub API.\n",
    "\n",
    "### Notebook Workflow\n",
    "\n",
    "1.  **Phase 1: Environment Setup**\n",
    "    - Import libraries (`finnhub`, `pandas`, `transformers`, etc.).\n",
    "    - Initialize the Finnhub API client.\n",
    "\n",
    "2.  **Phase 2: Data Acquisition**\n",
    "    - Fetch company news (headlines, summaries) from the `company-news` endpoint.\n",
    "    - Fetch insider transaction sentiment (MSPR) from the `stock/insider-sentiment` endpoint.\n",
    "\n",
    "3.  **Phase 3: NLP Sentiment Analysis**\n",
    "    - Load a pre-trained `FinBERT` model for sentiment analysis.\n",
    "    - Calculate and apply sentiment scores to news headlines and summaries.\n",
    "\n",
    "4.  **Phase 4: Data Aggregation & Consolidation**\n",
    "    - Merge the news and insider sentiment data into a single DataFrame.\n",
    "    - Resample the combined data into a final quarterly format.\n",
    "    - Generate the aggregated DataFrame containing mean sentiment scores, news volume, and insider sentiment metrics."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c9349dd",
   "metadata": {},
   "source": [
    "## Phase 1: Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a90467d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0: Library Installation\n",
      "------------------------------\n",
      "Required libraries installed successfully.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.2.1 -> 25.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "print(\"1.0: Library Installation\")\n",
    "print(\"-\"*30)\n",
    "# To ensure packages install into the correct kernel environment, we explicitly use\n",
    "# the 'sys.executable' to call pip. This avoids issues where '!pip' might\n",
    "# point to a different Python installation.\n",
    "import sys\n",
    "\n",
    "# NOTE: To run this on GColab, you can also use the %pip magic command instead of !{sys.executable} -m pip\n",
    "\n",
    "# Consolidated installation of all required libraries\n",
    "!{sys.executable} -m pip install finnhub-python pandas seaborn matplotlib numpy datasets kaggle python-dotenv --quiet\n",
    "\n",
    "print(\"Required libraries installed successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7e71de03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.1: Library Imports\n",
      "------------------------------\n",
      "Core libraries imported successfully.\n"
     ]
    }
   ],
   "source": [
    "print(\"1.1: Library Imports\")\n",
    "print(\"-\"*30)\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import finnhub as fi\n",
    "import matplotlib.pyplot as plt\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "import json\n",
    "from datetime import datetime\n",
    "\n",
    "print(\"Core libraries imported successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ae308c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"1.2: Finnhub Client Initialization\")\n",
    "print(\"-\"*30)\n",
    "# --- Secure API Key Management ---\n",
    "# It is a security best practice to store your API key as an environment variable\n",
    "# to avoid exposing it directly in the code.\n",
    "\n",
    "# Before running this cell, set the 'FINNHUB_API_KEY' in your environment.\n",
    "# For example, in your terminal:\n",
    "# export FINNHUB_API_KEY='your_api_key_here'\n",
    "# You will need to restart your notebook's kernel after setting the variable.\n",
    "\n",
    "api_key = \"d3r0knpr01qna05k8e40d3r0knpr01qna05k8e4g\"\n",
    "\n",
    "if not api_key:\n",
    "    print(\"API key not found in environment variables. Please set 'FINNHUB_API_KEY'.\")\n",
    "    # You can temporarily hardcode your key here for testing, but it is not recommended for production.\n",
    "    # api_key = \"YOUR_API_KEY_HERE\" \n",
    "    finnhub_client = None\n",
    "else:\n",
    "    finnhub_client = fi.Client(api_key=api_key)\n",
    "    print(\"Finnhub client initialized.\")\n",
    "    # --- Test API Client ---\n",
    "    # Optional: Test the client with a simple, free API call to ensure it's working.\n",
    "    try:\n",
    "        profile = finnhub_client.company_profile2(symbol='AAPL')\n",
    "        print(f\"Successfully fetched company profile for: {profile.get('name', 'AAPL')}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Client may be initialized, but a test API call failed: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e5a6fd1",
   "metadata": {},
   "source": [
    "## Phase 2: Data Gathering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e3acc75",
   "metadata": {},
   "source": [
    "### Strategy 2.1 : Historical `Insider Sentiment`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f73d930",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"2.1.0: Global Configuration\")\n",
    "print(\"-\"*30)\n",
    "\n",
    "# --- Configuration ---\n",
    "# Tickers for the companies we are analyzing.\n",
    "STOCKS = ['AAPL', 'NVDA', 'GOOGL']\n",
    "\n",
    "# --- Date Range for Long-Term Data (2018-2024) ---\n",
    "# This range applies to all data sources.\n",
    "START_YEAR = 2018\n",
    "END_YEAR = 2024\n",
    "\n",
    "print(\"Global configuration loaded:\")\n",
    "print(f\"Tickers: {STOCKS}\")\n",
    "print(f\"Date Range: {START_YEAR}-{END_YEAR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0756f904",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"2.1.1: Long-Term Data Extraction (Insider Sentiment)\")\n",
    "print(\"-\"*30)\n",
    "\n",
    "# --- Data Storage ---\n",
    "all_insider_data = []\n",
    "\n",
    "print(f\"Fetching long-term insider sentiment from {START_YEAR} to {END_YEAR}...\")\n",
    "\n",
    "# --- Fetch Data for Each Stock and Year ---\n",
    "for stock in STOCKS:\n",
    "    print(f\"  > Processing {stock}...\")\n",
    "    for year in range(START_YEAR, END_YEAR + 1):\n",
    "        start_date = f\"{year}-01-01\"\n",
    "        end_date = f\"{year}-12-31\"\n",
    "        try:\n",
    "            insider_sentiment = finnhub_client.stock_insider_sentiment(stock, _from=start_date, to=end_date)\n",
    "            insider_transactions = insider_sentiment.get('data', [])\n",
    "            for item in insider_transactions:\n",
    "                report_date = datetime(year=item['year'], month=item['month'], day=1).date()\n",
    "                all_insider_data.append({\n",
    "                    'ticker': stock,\n",
    "                    'date': report_date,\n",
    "                    'mspr': item['mspr'],\n",
    "                    'change': item['change']\n",
    "                })\n",
    "            # A small confirmation to show progress.\n",
    "            if insider_transactions:\n",
    "                print(f\"    - Found {len(insider_transactions)} records for {year}.\")\n",
    "        except Exception as e:\n",
    "            print(f\"    - Error fetching insider sentiment for {stock} in {year}: {e}\")\n",
    "\n",
    "print(\"\\nLong-term insider sentiment fetching complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99c3a727",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"2.1.2: Create Company-Specific Insider DataFrames\")\n",
    "print(\"-\"*30)\n",
    "\n",
    "# This cell refactors the insider sentiment data into separate, clean\n",
    "# DataFrames for each company, formatted for time series analysis.\n",
    "\n",
    "# Create a temporary DataFrame from the raw collected data\n",
    "insider_df = pd.DataFrame(all_insider_data)\n",
    "\n",
    "# Dictionary to hold the final, structured DataFrames for each company\n",
    "# NOTE: We can access the selevant dataset by calling `insider_datasets[\"AAPL\"].head()` for example\n",
    "insider_datasets = {}\n",
    "\n",
    "if not insider_df.empty:\n",
    "    # Convert 'date' column to datetime objects for manipulation\n",
    "    insider_df['date'] = pd.to_datetime(insider_df['date'])\n",
    "\n",
    "    # Engineer the 'Period' column in the specified 'YYYY-Q' format\n",
    "    insider_df['Period'] = insider_df['date'].dt.year.astype(str) + '-Q' + insider_df['date'].dt.quarter.astype(str)\n",
    "    \n",
    "    print(\"Processing insider data for each target ticker...\")\n",
    "    # Iterate through the globally defined STOCKS list to create a DF for each\n",
    "    for ticker in STOCKS:\n",
    "        # Filter data for the current ticker\n",
    "        ticker_specific_df = insider_df[insider_df['ticker'] == ticker].copy()\n",
    "\n",
    "        if not ticker_specific_df.empty:\n",
    "            # Select, rename, and sort the columns to match the desired format\n",
    "            final_df = ticker_specific_df[['Period', 'mspr']].rename(columns={'mspr': 'MSPR'})\n",
    "            final_df = final_df.sort_values(by='Period').reset_index(drop=True)\n",
    "            \n",
    "            # Store the processed DataFrame in the dictionary\n",
    "            insider_datasets[ticker] = final_df \n",
    "        else:\n",
    "            print(f\"No insider sentiment data was found for {ticker}\")\n",
    "    \n",
    "    print(\"\\nSuccessfully created and structured insider sentiment DataFrames for all tickers.\")\n",
    "    print(\"DataFrames are stored in the 'insider_datasets' dictionary.\")\n",
    "\n",
    "else:\n",
    "    print(\"The initial 'all_insider_data' list is empty. No DataFrames were created.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f06146e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(\"2.1.2: Display Company-Specific Insider DataFrames\")\n",
    "print(\"-\"*30)\n",
    "\n",
    "# This is how we can access the insider_datasets ictionary now\n",
    "display(insider_datasets[\"AAPL\"].head())\n",
    "display(insider_datasets[\"NVDA\"].head())\n",
    "display(insider_datasets[\"GOOGL\"].head())\n",
    "\n",
    "# This bit is just to gather contextual info on data distributions, quantities, ect ect\n",
    "for TICKER in list(insider_datasets):\n",
    "    print(f\"Relevant Information on {TICKER}:\\n\")\n",
    "    display(insider_datasets[TICKER].info())\n",
    "    display(insider_datasets[TICKER].describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57c9aab5",
   "metadata": {},
   "source": [
    "We now have a `Dictionary` containing: \n",
    "* `AAPL` Dataset \n",
    "* `NVDA` Dataset\n",
    "* `GOOGL` Dataset\n",
    "\n",
    "Each of these datasets has this format:\n",
    "|   Period   | MSPR        |\n",
    "|------------|-------------|\n",
    "| 2018-Q1    | Value       | \n",
    "| 2018-Q1    | Value       | \n",
    "| 2018-Q1    | Value       | \n",
    "| 2018-Q2    | Value       | \n",
    "| ...        | ...         | \n",
    "| 2024-Q4    | Value       | "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8694c0ef",
   "metadata": {},
   "source": [
    "### Strategy 2.2: Historical News via Hugging Face Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c4fe93a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"2.2.0: Configuration & Authentication for Hugging Face\")\n",
    "print(\"-\"*30)\n",
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "\n",
    "# NOTE: --- Authentication (IMPORTANT) ---\n",
    "# The Hugging Face User Access Token is loaded from the .env file.\n",
    "# Make sure your .env file has the line: HF_TOKEN='your_token_here'\n",
    "\n",
    "hf_token = os.getenv(\"HF_TOKEN\")\n",
    "\n",
    "if hf_token:\n",
    "    print(\"Found Hugging Face token. Logging in...\")\n",
    "    try:\n",
    "        login(token=hf_token)\n",
    "        print(\"Login successful.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Login failed: {e}\")\n",
    "else:\n",
    "    print(\"Hugging Face token not found in environment variables.\")\n",
    "    print(\"Please ensure your .env file is correctly configured with 'HF_TOKEN'.\")\n",
    "\n",
    "\n",
    "# --- Configuration ---\n",
    "# These are defined globally, but we re-state them here for clarity.\n",
    "# Note: START_YEAR and END_YEAR must be defined in a previous cell.\n",
    "try:\n",
    "    TARGET_TICKERS = ['AAPL', 'NVDA', 'GOOGL']\n",
    "    START_DATE = f\"{START_YEAR}-01-01\"\n",
    "    END_DATE = f\"{END_YEAR}-12-31\"\n",
    "    \n",
    "    print(\"\\nConfiguration for historical news extraction is set.\")\n",
    "    print(f\"Tickers: {TARGET_TICKERS}\")\n",
    "    print(f\"Date Range: {START_DATE} to {END_DATE}\")\n",
    "except NameError:\n",
    "    print(\"\\nWarning: START_YEAR and END_YEAR are not defined.\")\n",
    "    print(\"Please run a configuration cell first.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68c39379",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"2.2.1: Extract Ticker-Specific News Articles via Streaming\")\n",
    "print(\"-\"*30)\n",
    "\n",
    "# --- Expanded Search Dictionary ---\n",
    "# Maps a canonical ticker to a set of lowercase search terms for broader matching.\n",
    "# Using sets provides a slight performance boost for lookups.\n",
    "SEARCH_TERMS = {\n",
    "    'AAPL': {'aapl', 'apple'},\n",
    "    'NVDA': {'nvda', 'nvidia'},\n",
    "    'GOOGL': {'googl', 'google', 'alphabet'}\n",
    "}\n",
    "# A flattened set of all terms for a very fast initial check.\n",
    "ALL_SEARCH_TERMS = set.union(*SEARCH_TERMS.values())\n",
    "\n",
    "# --- Data Storage ---\n",
    "multisource_articles = []\n",
    "\n",
    "print(\"Loading and filtering 'financial-news-multisource' dataset...\")\n",
    "print(\"(This is a large dataset and will take some time to process.)\")\n",
    "\n",
    "try:\n",
    "    # --- Increase the download timeout for stability on large datasets ---\n",
    "    import huggingface_hub.constants\n",
    "    huggingface_hub.constants.HF_HUB_DOWNLOAD_TIMEOUT = 120 \n",
    "\n",
    "    # --- Load all subsets of the dataset in streaming mode ---\n",
    "    multisource_dataset = load_dataset(\n",
    "        \"Brianferrell787/financial-news-multisource\",\n",
    "        data_files=\"data/*/*.parquet\",\n",
    "        split=\"train\",\n",
    "        streaming=True\n",
    "    )\n",
    "    \n",
    "    # --- Iterate through the stream with the optimized multi-step filter ---\n",
    "    for i, article in enumerate(iter(multisource_dataset)):\n",
    "        # Provide progress updates to show the process is not stalled\n",
    "        if (i + 1) % 10_000 == 0:\n",
    "            print(f\"  > Processed {i + 1:,} articles. Found {len(multisource_articles)} relevant so far...\")\n",
    "            print(f\"  > We are currently at date: {article['date'][:10]}\")\n",
    "\n",
    "        # --- OPTIMIZED FILTERING LOGIC ---\n",
    "        # Step 1: Filter by date (fastest, string comparison).\n",
    "        if not (START_DATE <= article['date'][:10] <= END_DATE):\n",
    "            continue\n",
    "        \n",
    "        # Step 2: Quick pre-filter. Check if any of our expanded search terms appear\n",
    "        # anywhere in the text or metadata before doing more expensive work.\n",
    "        text_lower = article['text'].lower()\n",
    "        extra_fields_lower = article['extra_fields'].lower()\n",
    "        if not any(term in text_lower or term in extra_fields_lower for term in ALL_SEARCH_TERMS):\n",
    "            continue\n",
    "\n",
    "        # --- Step 3: Precise ticker identification for articles that passed the pre-filters ---\n",
    "        mentioned_tickers = set() # Use a set to store found tickers to avoid duplicates\n",
    "\n",
    "        # Primary Method: Check the structured 'stocks' field for high precision.\n",
    "        try:\n",
    "            extra_data = json.loads(article['extra_fields'])\n",
    "            if 'stocks' in extra_data and isinstance(extra_data['stocks'], list):\n",
    "                # Find the intersection of our target tickers and the article's tickers\n",
    "                found = set(TARGET_TICKERS) & set(extra_data['stocks'])\n",
    "                mentioned_tickers.update(found)\n",
    "        except (json.JSONDecodeError, TypeError):\n",
    "            # If JSON is invalid, we can fall back to text search.\n",
    "            pass\n",
    "\n",
    "        # Fallback Method: If no tickers were found in metadata, check the text.\n",
    "        # This increases recall for articles that might not be perfectly tagged.\n",
    "        if not mentioned_tickers:\n",
    "            for ticker, terms in SEARCH_TERMS.items():\n",
    "                if any(term in text_lower for term in terms):\n",
    "                    mentioned_tickers.add(ticker)\n",
    "        \n",
    "        # If we found one or more relevant tickers, add entries to our list.\n",
    "        if mentioned_tickers:\n",
    "            for ticker in mentioned_tickers:\n",
    "                multisource_articles.append({\n",
    "                    'date': article['date'],\n",
    "                    'ticker': ticker,\n",
    "                    'text': article['text']\n",
    "                })\n",
    "    \n",
    "    print(f\"\\nExtraction complete. Total relevant article entries collected: {len(multisource_articles)}\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"\\nAn error occurred while processing the dataset: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25968da9",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"2.2.2: Create the News Articles DataFrame\")\n",
    "print(\"-\"*30)\n",
    "\n",
    "# --- Create DataFrame from the collected list ---\n",
    "news_articles_df = pd.DataFrame(multisource_articles)\n",
    "\n",
    "if not news_articles_df.empty:\n",
    "    # Convert 'date' column to datetime objects for future analysis\n",
    "    news_articles_df['date'] = pd.to_datetime(news_articles_df['date']).dt.date\n",
    "    \n",
    "    print(\"`news_articles_df` DataFrame created successfully.\")\n",
    "    \n",
    "    # Display summary and head\n",
    "    print(\"\\n--- DataFrame Info ---\")\n",
    "    news_articles_df.info()\n",
    "    \n",
    "    print(\"\\n--- DataFrame Head ---\")\n",
    "    display(news_articles_df.head())\n",
    "    \n",
    "else:\n",
    "    print(\"No articles from 'financial-news-multisource' matched the filtering criteria.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59c0f8e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"2.2.3: Save Filtered News Articles to a Compressed File\")\n",
    "print(\"-\"*30)\n",
    "\n",
    "# This step is crucial for checkpointing our progress.\n",
    "\n",
    "if 'news_articles_df' in locals() and not news_articles_df.empty:\n",
    "    # Define the output file path. Using the '.gz' extension with the 'gzip'\n",
    "    # compression type is a standard and efficient way to save large CSVs.\n",
    "    output_filename = \"filtered_news_articles.csv.gz\"\n",
    "\n",
    "    print(f\"Saving the 'news_articles_df' to a compressed CSV file: {output_filename}\")\n",
    "    print(f\"This may take a moment given the size of the DataFrame ({len(news_articles_df):,} rows)...\")\n",
    "\n",
    "    try:\n",
    "        # Save the DataFrame to a gzipped CSV.\n",
    "        # - compression='gzip' handles the zipping automatically.\n",
    "        # - index=False prevents pandas from writing the DataFrame index as a column.\n",
    "        news_articles_df.to_csv(output_filename, index=False, compression='gzip')\n",
    "        \n",
    "        print(f\"\\nDataFrame successfully saved to '{output_filename}'.\")\n",
    "        print(\"You can now load this file in future sessions to skip the long extraction process.\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"\\nAn error occurred while saving the DataFrame: {e}\")\n",
    "else:\n",
    "    print(\"The 'news_articles_df' DataFrame is not available or is empty. Skipping save operation.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b4f6a36",
   "metadata": {},
   "source": [
    "### Strategy 2.3: Article Headlines Via `Kaggle`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c3bd99f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.3.0: Setup and Kaggle API Configuration\n",
      "------------------------------\n",
      "Kaggle API credentials configured from environment variables.\n",
      "\n",
      "Setup complete. Constants defined.\n",
      "Data will be filtered for the period: 2018-2024\n"
     ]
    }
   ],
   "source": [
    "print(\"2.3.0: Setup and Kaggle API Configuration\")\n",
    "print(\"-\"*30)\n",
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "\n",
    "# --- 1. Configure Kaggle API ---\n",
    "# Credentials are loaded from your .env file.\n",
    "# Ensure your .env file contains:\n",
    "# KAGGLE_USERNAME='your_username'\n",
    "# KAGGLE_KEY='your_api_key'\n",
    "\n",
    "KAGGLE_USERNAME = os.getenv('KAGGLE_USERNAME')\n",
    "KAGGLE_KEY = os.getenv('KAGGLE_KEY')\n",
    "\n",
    "# Set environment variables for the Kaggle CLI\n",
    "if KAGGLE_USERNAME and KAGGLE_KEY:\n",
    "    os.environ['KAGGLE_USERNAME'] = KAGGLE_USERNAME\n",
    "    os.environ['KAGGLE_KEY'] = KAGGLE_KEY\n",
    "    print(\"Kaggle API credentials configured from environment variables.\")\n",
    "else:\n",
    "    print(\"Warning: Kaggle credentials not found in environment variables.\")\n",
    "    print(\"Please ensure your .env file is correctly configured.\")\n",
    "\n",
    "# --- 2. Define Constants ---\n",
    "# Date range for filtering headlines.\n",
    "START_YEAR = 2018\n",
    "END_YEAR = 2024\n",
    "\n",
    "print(\"\\nSetup complete. Constants defined.\")\n",
    "print(f\"Data will be filtered for the period: {START_YEAR}-{END_YEAR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b159985d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.3.1: Download, Extract, and Filter S&P 500 Headlines Dataset\n",
      "------------------------------\n",
      "Ensured that the target directory 'Data' exists.\n",
      "Downloading dataset 'dyutidasmahaptra/s-and-p-500-with-financial-news-headlines-20082024' to 'Data'...\n",
      "Dataset URL: https://www.kaggle.com/datasets/dyutidasmahaptra/s-and-p-500-with-financial-news-headlines-20082024\n",
      "License(s): CC-BY-SA-4.0\n",
      "Extracting 'sp500_headlines_2008_2024.csv' into 'Data'...\n",
      "Extraction complete.\n",
      "Removing temporary zip file: 'Data\\s-and-p-500-with-financial-news-headlines-20082024.zip'\n",
      "Loading data from 'Data\\sp500_headlines_2008_2024.csv' into DataFrame and filtering...\n",
      "Filtering successful. The data is ready in 'market_headlines_df'.\n"
     ]
    }
   ],
   "source": [
    "print(\"2.3.1: Download, Extract, and Filter S&P 500 Headlines Dataset\")\n",
    "print(\"-\"*30)\n",
    "\n",
    "import zipfile\n",
    "\n",
    "# --- Define constants for data directory and file paths ---\n",
    "DATA_DIR = \"Data\"\n",
    "DATASET_NAME = 'dyutidasmahaptra/s-and-p-500-with-financial-news-headlines-20082024'\n",
    "ZIP_FILE_NAME = 's-and-p-500-with-financial-news-headlines-20082024.zip'\n",
    "CSV_FILE_NAME = 'sp500_headlines_2008_2024.csv'\n",
    "\n",
    "# Construct full paths for the files within the Data directory\n",
    "zip_file_path = os.path.join(DATA_DIR, ZIP_FILE_NAME)\n",
    "csv_file_path = os.path.join(DATA_DIR, CSV_FILE_NAME)\n",
    "\n",
    "# Ensure the target data directory exists\n",
    "os.makedirs(DATA_DIR, exist_ok=True)\n",
    "print(f\"Ensured that the target directory '{DATA_DIR}' exists.\")\n",
    "\n",
    "# --- Download the dataset using the Kaggle API into the specified directory ---\n",
    "print(f\"Downloading dataset '{DATASET_NAME}' to '{DATA_DIR}'...\")\n",
    "# The -p flag directs the Kaggle CLI to download to the specified path.\n",
    "!kaggle datasets download -d {DATASET_NAME} -p {DATA_DIR} --quiet\n",
    "\n",
    "# --- Extract the CSV file from the downloaded zip ---\n",
    "print(f\"Extracting '{CSV_FILE_NAME}' into '{DATA_DIR}'...\")\n",
    "with zipfile.ZipFile(zip_file_path, 'r') as zip_ref:\n",
    "    zip_ref.extractall(DATA_DIR)\n",
    "print(\"Extraction complete.\")\n",
    "\n",
    "# --- Clean up the downloaded zip file ---\n",
    "print(f\"Removing temporary zip file: '{zip_file_path}'\")\n",
    "os.remove(zip_file_path)\n",
    "\n",
    "# --- Load and process the data with pandas ---\n",
    "print(f\"Loading data from '{csv_file_path}' into DataFrame and filtering...\")\n",
    "try:\n",
    "    # Load the CSV from the Data directory\n",
    "    df = pd.read_csv(csv_file_path)\n",
    "\n",
    "    # Convert the 'date' column to datetime objects for reliable filtering\n",
    "    df['Date'] = pd.to_datetime(df['Date'])\n",
    "\n",
    "    # Create the filter condition for the date range\n",
    "    start_date = f'{START_YEAR}-01-01'\n",
    "    end_date = f'{END_YEAR}-12-31'\n",
    "    date_filter = (df['Date'] >= start_date) & (df['Date'] <= end_date)\n",
    "\n",
    "    # Apply the filter and create the final DataFrame\n",
    "    market_headlines_df = df[date_filter].copy()\n",
    "\n",
    "    # Drop the 'close' column as it is not needed for sentiment analysis\n",
    "    market_headlines_df = market_headlines_df.drop(columns=['CP'])\n",
    "\n",
    "    print(\"Filtering successful. The data is ready in 'market_headlines_df'.\")\n",
    "\n",
    "except FileNotFoundError:\n",
    "    print(f\"ERROR: The file '{csv_file_path}' was not found after extraction.\")\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred during data processing: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "837f5d95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.3.2: Display Final Market Headlines DataFrame\n",
      "------------------------------\n",
      "DataFrame for general market sentiment created successfully.\n",
      "\n",
      "--- DataFrame Info ---\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 13422 entries, 0 to 13421\n",
      "Data columns (total 2 columns):\n",
      " #   Column  Non-Null Count  Dtype         \n",
      "---  ------  --------------  -----         \n",
      " 0   Title   13422 non-null  object        \n",
      " 1   Date    13422 non-null  datetime64[ns]\n",
      "dtypes: datetime64[ns](1), object(1)\n",
      "memory usage: 209.8+ KB\n",
      "\n",
      "--- DataFrame Head (sorted) ---\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Title</th>\n",
       "      <th>Date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2018 Stock Market Cycles Outlook: New Highs......</td>\n",
       "      <td>2018-01-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>The Surprising Outperformance Of A Value-Tilte...</td>\n",
       "      <td>2018-01-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>This Day In Market History: NYSE Volume Hits 2...</td>\n",
       "      <td>2018-01-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>These 4 S&amp;P 500 Stocks Doubled in 2017</td>\n",
       "      <td>2018-01-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Warren Buffett wins $1M bet against hedge fund...</td>\n",
       "      <td>2018-01-02</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               Title       Date\n",
       "0  2018 Stock Market Cycles Outlook: New Highs...... 2018-01-02\n",
       "1  The Surprising Outperformance Of A Value-Tilte... 2018-01-02\n",
       "2  This Day In Market History: NYSE Volume Hits 2... 2018-01-02\n",
       "3             These 4 S&P 500 Stocks Doubled in 2017 2018-01-02\n",
       "4  Warren Buffett wins $1M bet against hedge fund... 2018-01-02"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- DataFrame Tail (sorted) ---\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Title</th>\n",
       "      <th>Date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>13417</th>\n",
       "      <td>Zions (ZION) Loses Spot in S&amp;P 500 as Concerns...</td>\n",
       "      <td>2024-03-04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13418</th>\n",
       "      <td>S&amp;P 500: Super Micro, Deckers Jump On News The...</td>\n",
       "      <td>2024-03-04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13419</th>\n",
       "      <td>Bank of America boosts S&amp;P 500 target to 5,400...</td>\n",
       "      <td>2024-03-04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13420</th>\n",
       "      <td>S&amp;P 500 Price Forecast – S&amp;P 500 Continues to ...</td>\n",
       "      <td>2024-03-04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13421</th>\n",
       "      <td>S&amp;P 500 Gains and Losses Today: Tesla Shares T...</td>\n",
       "      <td>2024-03-04</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   Title       Date\n",
       "13417  Zions (ZION) Loses Spot in S&P 500 as Concerns... 2024-03-04\n",
       "13418  S&P 500: Super Micro, Deckers Jump On News The... 2024-03-04\n",
       "13419  Bank of America boosts S&P 500 target to 5,400... 2024-03-04\n",
       "13420  S&P 500 Price Forecast – S&P 500 Continues to ... 2024-03-04\n",
       "13421  S&P 500 Gains and Losses Today: Tesla Shares T... 2024-03-04"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(\"2.3.2: Display Final Market Headlines DataFrame\")\n",
    "print(\"-\"*30)\n",
    "\n",
    "# Sort by date just to be sure\n",
    "market_headlines_df = market_headlines_df.sort_values(by='Date').reset_index(drop=True)\n",
    "\n",
    "if 'market_headlines_df' in locals() and not market_headlines_df.empty:\n",
    "    print(\"DataFrame for general market sentiment created successfully.\")\n",
    "    \n",
    "    print(\"\\n--- DataFrame Info ---\")\n",
    "    market_headlines_df.info()\n",
    "    \n",
    "    print(\"\\n--- DataFrame Head (sorted) ---\")\n",
    "    display(market_headlines_df.head())\n",
    "    \n",
    "    print(\"\\n--- DataFrame Tail (sorted) ---\")\n",
    "    display(market_headlines_df.tail())\n",
    "else:\n",
    "    print(\"The 'market_headlines_df' DataFrame was not created or is empty. Please check the previous cell for errors.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de805d71",
   "metadata": {},
   "source": [
    "### Strategy 2.4: Company filings via `EDGAR` API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "083a5eb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"2.4.0: Setup, Installations, and EDGAR Identity\")\n",
    "print(\"-\"*30)\n",
    "\n",
    "# --- 1. Install necessary libraries ---\n",
    "# 'edgartools' is the library we'll use to interface with the SEC EDGAR database.\n",
    "import sys\n",
    "!{sys.executable} -m pip install edgartools --quiet\n",
    "\n",
    "import pandas as pd\n",
    "from edgar import Company, set_identity\n",
    "\n",
    "# --- 2. Set EDGAR User Identity (CRITICAL STEP) ---\n",
    "# The SEC requires any script or bot that accesses EDGAR to have a custom User-Agent\n",
    "# that identifies the user. This is a compliance requirement to avoid being blocked.\n",
    "# Replace the example with your own company/project name and email address.\n",
    "# Format: \"Sample Company Name your.email@example.com\"\n",
    "set_identity(\"University of Southampton ab3u21@soton.ac.uk\")\n",
    "print(\"EDGAR user identity set successfully.\")\n",
    "\n",
    "# --- 3. Define Constants ---\n",
    "# These constants will be used to filter the filings.\n",
    "# We use the globally defined START_YEAR and END_YEAR from cell 2.1.0\n",
    "TARGET_TICKERS = ['AAPL', 'NVDA', 'GOOGL']\n",
    "FORM_TYPES = [\"10-K\", \"10-Q\", \"8-K\"]\n",
    "DATE_RANGE = f\"{START_YEAR}-01-01:{END_YEAR}-12-31\"\n",
    "\n",
    "\n",
    "print(\"\\nSetup complete. Ready to extract SEC filings.\")\n",
    "print(f\"Tickers: {TARGET_TICKERS}\")\n",
    "print(f\"Form Types: {FORM_TYPES}\")\n",
    "print(f\"Date Range: {DATE_RANGE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "154eb914",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"2.4.1: Extract SEC Filings Data\")\n",
    "print(\"-\"*30)\n",
    "\n",
    "# --- Data Storage ---\n",
    "all_filings_data = []\n",
    "\n",
    "print(\"Starting extraction of SEC filings. This may take a significant amount of time...\")\n",
    "\n",
    "# --- Loop through each stock and extract its filings ---\n",
    "for ticker in TARGET_TICKERS:\n",
    "    print(f\"  > Processing filings for {ticker}...\")\n",
    "    try:\n",
    "        # Create a Company object for the current ticker\n",
    "        company = Company(ticker)\n",
    "        \n",
    "        # Get all filings and immediately filter by date range and form types\n",
    "        filings = company.get_filings().filter(date=DATE_RANGE, form=FORM_TYPES)\n",
    "        \n",
    "        # The 'filings' object is a generator; we iterate through it to get each filing\n",
    "        for filing in filings:\n",
    "            # The .text() method conveniently extracts and cleans the full filing text\n",
    "            filing_text = filing.text()\n",
    "            \n",
    "            # Append the structured data to our list\n",
    "            all_filings_data.append({\n",
    "                'filing_date': filing.filing_date,\n",
    "                'ticker': ticker,\n",
    "                'form_type': filing.form,\n",
    "                'text': filing_text\n",
    "            })\n",
    "            # Log progress for each file found\n",
    "            print(f\"    - Extracted {filing.form} from {filing.filing_date}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"    - ERROR: Could not process filings for {ticker}. Reason: {e}\")\n",
    "\n",
    "print(f\"\\nFilings extraction complete. Total documents extracted: {len(all_filings_data)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9d01eb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"2.4.2: Create and Display Filings DataFrame\")\n",
    "print(\"-\"*30)\n",
    "\n",
    "# --- Create DataFrame from the collected list ---\n",
    "filings_df = pd.DataFrame(all_filings_data)\n",
    "\n",
    "if not filings_df.empty:\n",
    "    # Convert 'filing_date' column to datetime objects\n",
    "    filings_df['filing_date'] = pd.to_datetime(filings_df['filing_date'])\n",
    "    \n",
    "    # Sort the DataFrame by date and ticker for good practice\n",
    "    filings_df = filings_df.sort_values(by=['filing_date', 'ticker']).reset_index(drop=True)\n",
    "    \n",
    "    print(\"`filings_df` DataFrame created successfully.\")\n",
    "    \n",
    "    # Display summary and head\n",
    "    print(\"\\n--- DataFrame Info ---\")\n",
    "    filings_df.info()\n",
    "    \n",
    "    print(\"\\n--- DataFrame Head ---\")\n",
    "    display(filings_df.head(40).sort_values([\"filing_date\"], ascending=True))\n",
    "    \n",
    "else:\n",
    "    print(\"The 'all_filings_data' list is empty. No DataFrame was created. Please check cell 2.4.1 for errors.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfcab2ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"2.4.3: Save Filtered Filings to a Compressed File\")\n",
    "print(\"-\"*30)\n",
    "\n",
    "# Check if the filings_df exists and is not empty before attempting to save.\n",
    "if 'filings_df' in locals() and not filings_df.empty:\n",
    "    \n",
    "    # Define the name for the compressed output file.\n",
    "    output_filename = \"filtered_filings.csv.gz\"\n",
    "\n",
    "    print(f\"Saving the 'filings_df' to a compressed CSV file: {output_filename}\")\n",
    "    print(f\"This may take a moment, as the DataFrame contains {len(filings_df):,} documents...\")\n",
    "\n",
    "    try:\n",
    "        # Save the DataFrame to a gzipped CSV.\n",
    "        # - compression='gzip' handles the compression.\n",
    "        # - index=False prevents pandas from writing the DataFrame index as a column.\n",
    "        filings_df.to_csv(output_filename, index=False, compression='gzip')\n",
    "        \n",
    "        print(f\"\\nDataFrame successfully saved to '{output_filename}'.\")\n",
    "        print(\"This file can be loaded in future sessions to bypass the EDGAR extraction process.\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"\\nAn error occurred while saving the filings DataFrame: {e}\")\n",
    "else:\n",
    "    print(\"The 'filings_df' DataFrame is not available or is empty. Skipping the save operation.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59183a89",
   "metadata": {},
   "source": [
    "## Phase 3: NLP Pipelines"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d827395",
   "metadata": {},
   "source": [
    "### 3.1: NLP Pipeline for `news_articles_df`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b13a3f82",
   "metadata": {},
   "source": [
    "#### We start this section by *Down_sampling*: \n",
    "We want to go from our `>700k` articles to `~11k` articles for each company over the 2018-2024 year period. That's `5` articles per day per company!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c01fd098",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"3.1.0: Setup and Imports for NLP Pre-processing\")\n",
    "print(\"-\"*30)\n",
    "\n",
    "# --- 1. Install necessary libraries ---\n",
    "import sys\n",
    "!{sys.executable} -m pip install nltk pandas --quiet\n",
    "\n",
    "import pandas as pd\n",
    "import re\n",
    "import nltk\n",
    "import os\n",
    "\n",
    "# --- 2. Download NLTK resources ---\n",
    "# We need 'punkt' and 'punkt_tab' for word tokenization. The most reliable\n",
    "# way to ensure they are available in a notebook is to call download() directly.\n",
    "# NLTK will not re-download the data if it's already present.\n",
    "print(\"Ensuring NLTK resources ('punkt', 'punkt_tab') are available...\")\n",
    "nltk.download('punkt', quiet=True)\n",
    "nltk.download('punkt_tab', quiet=True) # This command fixes the LookupError.\n",
    "print(\"NLTK resources are up to date.\")\n",
    "\n",
    "# --- 3. Define File path for fallback loading ---\n",
    "NEWS_ARTICLES_FILE = \"filtered_news_articles.csv.gz\"\n",
    "\n",
    "print(\"\\nSetup complete. Libraries and NLTK resources are ready.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "207acf63",
   "metadata": {},
   "source": [
    "#### Here we calculate a `Mention_Density` metric for each atricle that will help us understand how relevant an article is for our selected company."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2a36a3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"3.1.1: Calculate Mention Density Score for Each Article\")\n",
    "print(\"-\"*30)\n",
    "\n",
    "# --- 1. Define Comprehensive Company Keywords ---\n",
    "COMPANY_KEYWORDS = {\n",
    "    'AAPL': {\n",
    "        'aapl', 'apple', 'iphone', 'ipad', 'macbook', 'imac', 'watchos',\n",
    "        'ios', 'macos', 'airpods', 'tim cook', 'app store', 'vision pro'\n",
    "    },\n",
    "    'NVDA': {\n",
    "        'nvda', 'nvidia', 'geforce', 'rtx', 'quadro', 'tesla', # Note: 'tesla' is a GPU arch\n",
    "        'cuda', 'dgx', 'tegra', 'jensen huang', 'omniverse'\n",
    "    },\n",
    "    'GOOGL': {\n",
    "        'googl', 'google', 'alphabet', 'android', 'youtube', 'chrome',\n",
    "        'pixel', 'nest', 'waymo', 'gcp', 'sundar pichai', 'gemini'\n",
    "    }\n",
    "}\n",
    "print(\"Comprehensive keyword dictionaries defined.\")\n",
    "\n",
    "# --- 2. Load the Dataset (prioritizing live DataFrame) ---\n",
    "news_mention_density_df = None # Initialize to None\n",
    "\n",
    "# Prioritize using the 'news_articles_df' if it's already in memory.\n",
    "if 'news_articles_df' in locals() and isinstance(news_articles_df, pd.DataFrame) and not news_articles_df.empty:\n",
    "    print(\"Using the live 'news_articles_df' DataFrame from the current session.\")\n",
    "    news_mention_density_df = news_articles_df.copy() # Use a copy to avoid side effects\n",
    "    \n",
    "# NOTE: --- This block is commented out but can be used for future sessions ---\n",
    "# Fallback to loading from the file if the live DataFrame is not available.\n",
    "# elif os.path.exists(NEWS_ARTICLES_FILE):\n",
    "#     print(f\"Loading dataset from '{NEWS_ARTICLES_FILE}'...\")\n",
    "#     # In a new session, you would uncomment the line below:\n",
    "#     # news_mention_density_df = pd.read_csv(NEWS_ARTICLES_FILE, compression='gzip')\n",
    "# -----------------------------------------------------------------------\n",
    "\n",
    "if news_mention_density_df is None:\n",
    "    print(f\"ERROR: 'news_articles_df' not found in the current session.\")\n",
    "    print(f\"To run this cell independently, uncomment the file loading logic above and ensure '{NEWS_ARTICLES_FILE}' exists.\")\n",
    "else:\n",
    "    print(f\"DataFrame loaded with {len(news_mention_density_df):,} articles.\")\n",
    "    \n",
    "    # --- 3. Define the Density Calculation Function ---\n",
    "    def calculate_all_densities(text):\n",
    "        if not isinstance(text, str) or not text.strip():\n",
    "            return pd.Series({f'{ticker}_Density': 0.0 for ticker in COMPANY_KEYWORDS})\n",
    "        text_lower = text.lower()\n",
    "        total_words = len(nltk.word_tokenize(text_lower))\n",
    "        if total_words == 0:\n",
    "            return pd.Series({f'{ticker}_Density': 0.0 for ticker in COMPANY_KEYWORDS})\n",
    "        densities = {}\n",
    "        for ticker, keywords in COMPANY_KEYWORDS.items():\n",
    "            pattern = r'\\b(' + '|'.join(re.escape(k) for k in keywords) + r')\\b'\n",
    "            mention_count = len(re.findall(pattern, text_lower))\n",
    "            density = mention_count / total_words if total_words > 0 else 0\n",
    "            densities[f'{ticker}_Density'] = density\n",
    "        return pd.Series(densities)\n",
    "\n",
    "    # --- 4. Apply the function to the DataFrame ---\n",
    "    print(\"\\nCalculating mention densities for all articles. This may take a few minutes...\")\n",
    "    density_scores = news_mention_density_df['text'].apply(calculate_all_densities)\n",
    "    \n",
    "    news_mention_density_df = pd.concat([news_mention_density_df, density_scores], axis=1)\n",
    "    print(\"Mention density calculation complete.\")\n",
    "    \n",
    "    # --- 5. Display Results ---\n",
    "    print(\"\\n--- DataFrame with Mention Density Scores ---\")\n",
    "    display(news_mention_density_df[['date', 'ticker', 'AAPL_Density', 'NVDA_Density', 'GOOGL_Density']].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d30343f0",
   "metadata": {},
   "source": [
    "#### Now that we have `Mention_Density` for each company, we can select the Top `5` articles per day for each company that have a `Mention_Density` score of >1%.\n",
    "We will also include `article_volume` for each day, representing the total number of articles published for that company (with `Mention_Density` >1%.) in that day. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92e61a1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"3.1.2: Final Optimized Down-Sampling with Language Filter and Volume\")\n",
    "print(\"-\"*30)\n",
    "\n",
    "# --- Import language detection library ---\n",
    "import sys\n",
    "!{sys.executable} -m pip install langdetect --quiet\n",
    "from langdetect import detect, LangDetectException\n",
    "\n",
    "# --- 1. Define Filtering Parameters ---\n",
    "# MIN_DENSITY_THRESHOLD: The minimum relevance score an article must have to be considered.\n",
    "MIN_DENSITY_THRESHOLD = 0.01\n",
    "# TOP_N_ARTICLES: The maximum number of highest-scoring articles to select for any given day.\n",
    "TOP_N_ARTICLES = 5\n",
    "\n",
    "print(f\"Filtering Parameters:\")\n",
    "print(f\" - Minimum Mention Density Threshold: {MIN_DENSITY_THRESHOLD}\")\n",
    "print(f\" - Top N Articles per Day: {TOP_N_ARTICLES}\\n\")\n",
    "\n",
    "# --- 2. Initialize Storage ---\n",
    "# This dictionary will hold the final, filtered DataFrames, one for each company.\n",
    "company_top_articles = {}\n",
    "\n",
    "# Check if the main DataFrame from the previous step is available in memory.\n",
    "if 'news_mention_density_df' in locals() and not news_mention_density_df.empty:\n",
    "    \n",
    "    # --- Step A: De-duplicate Articles ---\n",
    "    # This ensures we only process each unique article text once per day.\n",
    "    print(f\"Original article entry count: {len(news_mention_density_df):,}\")\n",
    "    deduped_df = news_mention_density_df.drop_duplicates(subset=['date', 'text']).copy()\n",
    "    print(f\"De-duplicated article count: {len(deduped_df):,}\")\n",
    "    \n",
    "    # --- Step B: Lightweight Language Filtering with Progress Indicator ---\n",
    "    # This function checks only the first 100 characters of text for speed.\n",
    "    def is_english_fast(text):\n",
    "        try:\n",
    "            # We only need a small sample of the text to accurately detect the language.\n",
    "            sample = text[:100] if isinstance(text, str) else ''\n",
    "            # Return True only if the sample is valid and detected as English ('en').\n",
    "            return sample.strip() and detect(sample) == 'en'\n",
    "        except LangDetectException:\n",
    "            # If detection fails, we assume it's not the language we want.\n",
    "            return False\n",
    "\n",
    "    print(\"\\nFiltering for English-language articles (with progress updates)...\")\n",
    "    total_texts_to_check = len(deduped_df)\n",
    "    print_interval = 10000  # How often to print an update.\n",
    "    \n",
    "    english_mask = []\n",
    "    # We use an explicit loop here to provide progress feedback.\n",
    "    for i, text in enumerate(deduped_df['text']):\n",
    "        english_mask.append(is_english_fast(text))\n",
    "        \n",
    "        # This block prints a status update at the specified interval.\n",
    "        if (i + 1) % print_interval == 0 or (i + 1) == total_texts_to_check:\n",
    "            print(f\"  > Language check progress: {i + 1:,} of {total_texts_to_check:,} articles processed...\")\n",
    "\n",
    "    # Use the generated boolean mask to select only the English articles.\n",
    "    english_df = deduped_df[english_mask]\n",
    "    print(f\"\\nFiltered down to {len(english_df):,} English articles.\\n\")\n",
    "    \n",
    "    # --- Step 3. Process Each Company Independently ---\n",
    "    for ticker in COMPANY_KEYWORDS.keys():\n",
    "        density_col = f'{ticker}_Density'\n",
    "        print(f\"--- Processing {ticker} ---\")\n",
    "\n",
    "        # Step C: Filter for relevance based on the density threshold.\n",
    "        relevant_articles_df = english_df[english_df[density_col] >= MIN_DENSITY_THRESHOLD].copy()\n",
    "        \n",
    "        if relevant_articles_df.empty:\n",
    "            print(f\"  > No articles found for {ticker} above the density threshold. Skipping.\")\n",
    "            company_top_articles[ticker] = pd.DataFrame()\n",
    "            continue\n",
    "        \n",
    "        print(f\"  > Found {len(relevant_articles_df):,} relevant English articles for {ticker}.\")\n",
    "\n",
    "        # Step D: Calculate daily article volume from the relevant set.\n",
    "        daily_volume = relevant_articles_df.groupby('date').size().rename('article_volume')\n",
    "        \n",
    "        # Step E: Perform the \"Top N per Day\" selection.\n",
    "        top_n_df = relevant_articles_df.groupby('date').apply(\n",
    "            lambda x: x.nlargest(TOP_N_ARTICLES, density_col),\n",
    "            include_groups=False\n",
    "        ).reset_index(level=0)\n",
    "        \n",
    "        # Step F: Merge the daily volume feature onto the down-sampled DataFrame.\n",
    "        top_n_df = top_n_df.merge(daily_volume, on='date', how='left')\n",
    "        \n",
    "        # Step G: Clean up the ticker column for clarity.\n",
    "        top_n_df = top_n_df.drop(columns=['ticker'])\n",
    "        top_n_df['assigned_ticker'] = ticker\n",
    "        \n",
    "        print(f\"  > Down-sampled to {len(top_n_df):,} top articles for NLP analysis.\")\n",
    "\n",
    "        # Store the final, enriched DataFrame in the dictionary.\n",
    "        company_top_articles[ticker] = top_n_df\n",
    "    \n",
    "    print(\"\\nDown-sampling and all filtering complete.\")\n",
    "    \n",
    "    # --- 4. Display a Sample of the Results ---\n",
    "    print(\"\\n--- Sample of Final 'AAPL' Articles ---\")\n",
    "    if 'AAPL' in company_top_articles and not company_top_articles['AAPL'].empty:\n",
    "        display(company_top_articles['AAPL'][['date', 'assigned_ticker', 'AAPL_Density', 'article_volume']].head())\n",
    "    else:\n",
    "        print(\"No articles to display for AAPL.\")\n",
    "        \n",
    "else:\n",
    "    print(\"ERROR: 'news_mention_density_df' not found. Please run cell 3.1.1 first.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff310390",
   "metadata": {},
   "outputs": [],
   "source": [
    "for TICKER in list(company_top_articles):\n",
    "    print(TICKER)\n",
    "    display(company_top_articles[TICKER].head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2a29dfd4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.1.3: Inspect Down-Sampled DataFrames for Missing Data and Volume\n",
      "------------------------------\n",
      "Analyzing data coverage over a total period of 2557 days (2018-2024).\n",
      "\n",
      "--- Inspection for AAPL ---\n",
      "  Article Coverage:\n",
      "    > Total selected articles for NLP: 10,634\n",
      "    > Unique days with articles: 2,336\n",
      "    > Days with NO articles (to be imputed): 221\n",
      "    > Daily coverage percentage: 91.36%\n",
      "\n",
      "  Daily Article Volume (for days with coverage):\n",
      "    > Mean daily volume: 25.17\n",
      "    > Median daily volume: 14\n",
      "    > Max daily volume: 387\n",
      "    > Min daily volume: 1\n",
      "\n",
      "--- Inspection for NVDA ---\n",
      "  Article Coverage:\n",
      "    > Total selected articles for NLP: 6,961\n",
      "    > Unique days with articles: 2,117\n",
      "    > Days with NO articles (to be imputed): 440\n",
      "    > Daily coverage percentage: 82.79%\n",
      "\n",
      "  Daily Article Volume (for days with coverage):\n",
      "    > Mean daily volume: 5.61\n",
      "    > Median daily volume: 3\n",
      "    > Max daily volume: 103\n",
      "    > Min daily volume: 1\n",
      "\n",
      "--- Inspection for GOOGL ---\n",
      "  Article Coverage:\n",
      "    > Total selected articles for NLP: 10,110\n",
      "    > Unique days with articles: 2,273\n",
      "    > Days with NO articles (to be imputed): 284\n",
      "    > Daily coverage percentage: 88.89%\n",
      "\n",
      "  Daily Article Volume (for days with coverage):\n",
      "    > Mean daily volume: 21.26\n",
      "    > Median daily volume: 11\n",
      "    > Max daily volume: 382\n",
      "    > Min daily volume: 1\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"3.1.3: Inspect Down-Sampled DataFrames for Missing Data and Volume\")\n",
    "print(\"-\"*30)\n",
    "\n",
    "if 'company_top_articles' in locals() and isinstance(company_top_articles, dict):\n",
    "\n",
    "    full_date_range = pd.date_range(start=f'{START_YEAR}-01-01', end=f'{END_YEAR}-12-31', freq='D')\n",
    "    total_days_in_period = len(full_date_range)\n",
    "    \n",
    "    print(f\"Analyzing data coverage over a total period of {total_days_in_period} days ({START_YEAR}-{END_YEAR}).\\n\")\n",
    "\n",
    "    for ticker, df in company_top_articles.items():\n",
    "        print(f\"--- Inspection for {ticker} ---\")\n",
    "        \n",
    "        if df.empty:\n",
    "            print(\"  > No articles were selected for this ticker. DataFrame is empty.\\n\")\n",
    "            continue\n",
    "        \n",
    "        df['date'] = pd.to_datetime(df['date'])\n",
    "        \n",
    "        total_articles = len(df)\n",
    "        unique_days_with_articles = df['date'].nunique()\n",
    "        missing_days = total_days_in_period - unique_days_with_articles\n",
    "        coverage_percentage = (unique_days_with_articles / total_days_in_period) * 100\n",
    "        \n",
    "        print(f\"  Article Coverage:\")\n",
    "        print(f\"    > Total selected articles for NLP: {total_articles:,}\")\n",
    "        print(f\"    > Unique days with articles: {unique_days_with_articles:,}\")\n",
    "        print(f\"    > Days with NO articles (to be imputed): {missing_days:,}\")\n",
    "        print(f\"    > Daily coverage percentage: {coverage_percentage:.2f}%\")\n",
    "        \n",
    "        # --- NEW: Reporting on 'article_volume' ---\n",
    "        # To get daily stats, we first drop duplicates for each day since 'article_volume' is repeated.\n",
    "        daily_stats_df = df.drop_duplicates(subset=['date'])\n",
    "        \n",
    "        print(f\"\\n  Daily Article Volume (for days with coverage):\")\n",
    "        print(f\"    > Mean daily volume: {daily_stats_df['article_volume'].mean():.2f}\")\n",
    "        print(f\"    > Median daily volume: {daily_stats_df['article_volume'].median():.0f}\")\n",
    "        print(f\"    > Max daily volume: {daily_stats_df['article_volume'].max():.0f}\")\n",
    "        print(f\"    > Min daily volume: {daily_stats_df['article_volume'].min():.0f}\\n\")\n",
    "        \n",
    "else:\n",
    "    print(\"ERROR: 'company_top_articles' dictionary not found. Please run cell 3.1.2 first.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c8bad1b",
   "metadata": {},
   "source": [
    "#### These steps below are just exporting and verifying importing doesn't break anything."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c4b8ab6",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"3.1.4: Save Filtered Articles Dictionary to a File\")\n",
    "print(\"-\"*30)\n",
    "\n",
    "# Import the pickle library for object serialization.\n",
    "import pickle\n",
    "\n",
    "# Check if the dictionary from the previous cell exists.\n",
    "if 'company_top_articles' in locals() and isinstance(company_top_articles, dict):\n",
    "    \n",
    "    # Define the output filename for our serialized dictionary.\n",
    "    output_filename = \"company_top_articles.pkl\"\n",
    "\n",
    "    print(f\"Saving the 'company_top_articles' dictionary to '{output_filename}'...\")\n",
    "    print(\"This file will serve as a checkpoint to avoid re-running the filtering process.\")\n",
    "\n",
    "    try:\n",
    "        # Open the file in write-binary ('wb') mode.\n",
    "        with open(output_filename, 'wb') as f:\n",
    "            # Use pickle.dump() to serialize the entire dictionary object into the file.\n",
    "            pickle.dump(company_top_articles, f)\n",
    "        \n",
    "        print(f\"\\nDictionary successfully saved to '{output_filename}'.\")\n",
    "        \n",
    "        # Provide a snippet for how to load the data in a future session.\n",
    "        print(\"\\nTo load this data in a future session, you can use the following code:\")\n",
    "        print(\"------------------------------------------------------------------\")\n",
    "        print(\"import pickle\")\n",
    "        print(\"with open('company_top_articles.pkl', 'rb') as f:\")\n",
    "        print(\"    company_top_articles = pickle.load(f)\")\n",
    "        print(\"------------------------------------------------------------------\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"\\nAn error occurred while saving the dictionary: {e}\")\n",
    "else:\n",
    "    print(\"ERROR: 'company_top_articles' dictionary not found. Please run the previous cells first.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec4e6b73",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"3.1.5: Load and Verify the Filtered Articles Dictionary from File\")\n",
    "print(\"-\"*30)\n",
    "\n",
    "# Import the pickle library and pandas for displaying DataFrames.\n",
    "import pickle\n",
    "import pandas as pd\n",
    "\n",
    "# Define the filename where the dictionary was saved.\n",
    "FILENAME = \"Data/company_top_articles.pkl\"\n",
    "\n",
    "try:\n",
    "    # Open the file in read-binary ('rb') mode.\n",
    "    with open(FILENAME, 'rb') as f:\n",
    "        # Load the entire dictionary object from the pickle file.\n",
    "        loaded_company_articles = pickle.load(f)\n",
    "    \n",
    "    print(f\"Successfully loaded dictionary from '{FILENAME}'.\")\n",
    "    print(\"Verifying contents by displaying the head of each DataFrame...\\n\")\n",
    "    \n",
    "    # --- Verification Loop ---\n",
    "    # Iterate through the keys (tickers) and values (DataFrames) of the loaded dictionary.\n",
    "    for ticker, df in loaded_company_articles.items():\n",
    "        print(f\"--- Top 10 Articles for: {ticker} ---\")\n",
    "        \n",
    "        if not df.empty:\n",
    "            # Display the first 10 rows for visual inspection.\n",
    "            display(df.head(10))\n",
    "        else:\n",
    "            print(\"  > DataFrame is empty for this ticker.\")\n",
    "        print(\"\\n\" + \"-\"*50 + \"\\n\")\n",
    "\n",
    "except FileNotFoundError:\n",
    "    print(f\"ERROR: The file '{FILENAME}' was not found. Please run cell 3.1.4 to save the file first.\")\n",
    "except Exception as e:\n",
    "    print(f\"An unexpected error occurred while loading the file: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f25299f6",
   "metadata": {},
   "source": [
    "#### And we have completed the Down-Sampling. Next, we calculate composite_sentiment scores using `FinBERT`\n",
    "We now have some missing data because news articles (with `Mention_Density` >1%) are sometimes unavailable for certain days.\\\n",
    "Our lowest coverage is `82.79%` so we can safely reconstruct the missing sentiment data after we have computed these scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3d0dff3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.1.6: Install GPU-Enabled PyTorch (v2.6.0+) & Transformers\n",
      "------------------------------\n",
      "Uninstalling previous torch versions (if any)...\n",
      "Installing torch (>=2.6.0) for CUDA 12.4 (this may take a few minutes)...\n",
      "Installing Hugging Face Transformers and TQDM...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.2.1 -> 25.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "------------------------------\n",
      "! ! ! CRITICAL STEP ! ! !\n",
      "Installation complete. You MUST now restart the Jupyter kernel.\n",
      "In VS Code / Jupyter: Find the 'Restart' button for the kernel.\n",
      "After restarting, you can run the subsequent cells (3.1.7 onward).\n",
      "------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.2.1 -> 25.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "print(\"3.1.6: Install GPU-Enabled PyTorch (v2.6.0+) & Transformers\")\n",
    "print(\"-\"*30)\n",
    "\n",
    "import sys\n",
    "\n",
    "# --- Step 1: Uninstall any old versions ---\n",
    "# This is crucial to ensure a clean install.\n",
    "print(\"Uninstalling previous torch versions (if any)...\")\n",
    "# !{sys.executable} -m pip uninstall torch torchvision torchaudio -y --quiet\n",
    "\n",
    "# --- Step 2: Install CUDA-enabled PyTorch (v2.6.0+ for CUDA 12.4) ---\n",
    "# We MUST use the 'cu124' index, as 'cu121' does not have torch 2.6.0+.\n",
    "# This version is required by transformers to patch CVE-2025-32434.\n",
    "print(\"Installing torch (>=2.6.0) for CUDA 12.4 (this may take a few minutes)...\")\n",
    "!{sys.executable} -m pip install torch>=2.6.0 torchvision torchaudio --index-url https://download.pytorch.org/whl/cu124 --quiet\n",
    "\n",
    "# --- Step 3: Install Transformers & TQDM ---\n",
    "# We add 'tqdm' here to get the progress bars and remove the warning.\n",
    "print(\"Installing Hugging Face Transformers and TQDM...\")\n",
    "!{sys.executable} -m pip install transformers tqdm --quiet\n",
    "\n",
    "# --- Step 4: Critical Kernel Restart ---\n",
    "print(\"\\n\" + \"-\"*30)\n",
    "print(\"! ! ! CRITICAL STEP ! ! !\")\n",
    "print(\"Installation complete. You MUST now restart the Jupyter kernel.\")\n",
    "print(\"In VS Code / Jupyter: Find the 'Restart' button for the kernel.\")\n",
    "print(\"After restarting, you can run the subsequent cells (3.1.7 onward).\")\n",
    "print(\"-\"*30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a3fc3e42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.1.7: Verify Environment and Initialize Pipeline\n",
      "------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\totob\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version: 2.6.0+cu124\n",
      "\n",
      "Initializing the FinBERT sentiment analysis pipeline...\n",
      "GPU detected: NVIDIA GeForce RTX 3050 Ti Laptop GPU. The pipeline will run on the GPU.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "FinBERT pipeline and tokenizer initialized successfully.\n"
     ]
    }
   ],
   "source": [
    "print(\"3.1.7: Verify Environment and Initialize Pipeline\")\n",
    "print(\"-\"*30)\n",
    "\n",
    "import torch\n",
    "from transformers import pipeline, AutoTokenizer\n",
    "import sys\n",
    "\n",
    "# --- 1. Verify PyTorch Installation ---\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "if not torch.__version__.startswith(\"2.6.0\"):\n",
    "    print(\"Warning: PyTorch version is not 2.6.0. This might cause issues.\")\n",
    "\n",
    "# --- 2. Initialize the Sentiment Analysis Pipeline with GPU Auto-Detection ---\n",
    "print(\"\\nInitializing the FinBERT sentiment analysis pipeline...\")\n",
    "\n",
    "# Check if a CUDA-compatible GPU is available\n",
    "if torch.cuda.is_available():\n",
    "    device_id = 0\n",
    "    print(f\"GPU detected: {torch.cuda.get_device_name(device_id)}. The pipeline will run on the GPU.\")\n",
    "else:\n",
    "    device_id = -1\n",
    "    print(\"No GPU detected or PyTorch CUDA is not installed. The pipeline will run on the CPU.\")\n",
    "\n",
    "# Load the main pipeline\n",
    "# The device parameter automatically assigns the model to the GPU (0) or CPU (-1).\n",
    "sentiment_pipeline = pipeline(\"sentiment-analysis\", model=\"ProsusAI/finbert\", device=device_id)\n",
    "\n",
    "# Load the tokenizer separately.\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"ProsusAI/finbert\")\n",
    "\n",
    "print(\"\\nFinBERT pipeline and tokenizer initialized successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8727200f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.1.8: Re-import the dataframes after kernel restart\n",
      "------------------------------\n"
     ]
    }
   ],
   "source": [
    "print(\"3.1.8: Re-import the dataframes after kernel restart\")\n",
    "print(\"-\"*30)\n",
    "\n",
    "import pickle\n",
    "with open('Data/company_top_articles.pkl', 'rb') as f:\n",
    "    company_top_articles = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3773da69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.1.9: Apply NLP with Chunking and Aggregate Daily Sentiment\n",
      "------------------------------\n",
      "Starting NLP analysis on 3 companies...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Analyzing AAPL articles: 100%|██████████| 10634/10634 [02:24<00:00, 73.54it/s]\n",
      "Processing all companies:  33%|███▎      | 1/3 [02:24<04:49, 144.62s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  > Aggregation for AAPL complete. Created daily time-series with 2336 unique days.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Analyzing NVDA articles: 100%|██████████| 6961/6961 [01:48<00:00, 64.03it/s]\n",
      "Processing all companies:  67%|██████▋   | 2/3 [04:13<02:03, 123.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  > Aggregation for NVDA complete. Created daily time-series with 2117 unique days.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Analyzing GOOGL articles: 100%|██████████| 10110/10110 [02:32<00:00, 66.37it/s]\n",
      "Processing all companies: 100%|██████████| 3/3 [06:45<00:00, 135.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  > Aggregation for GOOGL complete. Created daily time-series with 2273 unique days.\n",
      "\n",
      "--- All NLP Processing and Aggregation Complete ---\n",
      "\n",
      "--- Sample of Final Daily Sentiment DataFrame for 'AAPL' ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Period</th>\n",
       "      <th>article_volume</th>\n",
       "      <th>average_news_sentiment</th>\n",
       "      <th>AVG_AAPL_Density</th>\n",
       "      <th>AVG_NVDA_Density</th>\n",
       "      <th>AVG_GOOGL_Density</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2018-01-01</td>\n",
       "      <td>8</td>\n",
       "      <td>-0.017894</td>\n",
       "      <td>0.068807</td>\n",
       "      <td>0.0125</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2018-01-02</td>\n",
       "      <td>51</td>\n",
       "      <td>-0.127239</td>\n",
       "      <td>0.166147</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2018-01-03</td>\n",
       "      <td>62</td>\n",
       "      <td>-0.518287</td>\n",
       "      <td>0.124039</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2018-01-04</td>\n",
       "      <td>53</td>\n",
       "      <td>0.222064</td>\n",
       "      <td>0.148336</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2018-01-05</td>\n",
       "      <td>104</td>\n",
       "      <td>-0.443398</td>\n",
       "      <td>0.173987</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Period  article_volume  average_news_sentiment  AVG_AAPL_Density  \\\n",
       "0 2018-01-01               8               -0.017894          0.068807   \n",
       "1 2018-01-02              51               -0.127239          0.166147   \n",
       "2 2018-01-03              62               -0.518287          0.124039   \n",
       "3 2018-01-04              53                0.222064          0.148336   \n",
       "4 2018-01-05             104               -0.443398          0.173987   \n",
       "\n",
       "   AVG_NVDA_Density  AVG_GOOGL_Density  \n",
       "0            0.0125                0.0  \n",
       "1            0.0000                0.0  \n",
       "2            0.0000                0.0  \n",
       "3            0.0000                0.0  \n",
       "4            0.0000                0.0  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(\"3.1.9: Apply NLP with Chunking and Aggregate Daily Sentiment\")\n",
    "print(\"-\"*30)\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm.auto import tqdm # For rich progress bars in notebooks\n",
    "\n",
    "# --- 1. Define Helper Functions ---\n",
    "# (These functions are unchanged)\n",
    "\n",
    "def get_sentiment_score(result):\n",
    "    \"\"\"Converts the pipeline's output dictionary to a single numerical score.\"\"\"\n",
    "    label = result['label']\n",
    "    score = result['score']\n",
    "    if label == 'positive':\n",
    "        return score\n",
    "    elif label == 'negative':\n",
    "        return -score\n",
    "    return 0.0\n",
    "\n",
    "def analyze_sentiment_with_chunking(text):\n",
    "    \"\"\"\n",
    "    Analyzes sentiment of a text, handling long inputs by chunking them.\n",
    "    Averages the sentiment scores of all chunks for a final document score.\n",
    "    \"\"\"\n",
    "    # Max tokens for the model, leaving a small buffer for special tokens.\n",
    "    max_length = 512\n",
    "    # Amount of token overlap between chunks to maintain context.\n",
    "    overlap = 25\n",
    "    \n",
    "    try:\n",
    "        # Tokenize the text to see if it needs chunking.\n",
    "        tokens = tokenizer.encode(str(text), return_tensors='pt', truncation=False)\n",
    "        \n",
    "        # If text is short enough, process it in one go.\n",
    "        if tokens.size(1) <= max_length:\n",
    "            result = sentiment_pipeline(str(text))[0]\n",
    "            return get_sentiment_score(result)\n",
    "\n",
    "        # If text is too long, split it into chunks.\n",
    "        chunk_texts = []\n",
    "        for i in range(0, tokens.size(1), max_length - overlap):\n",
    "            chunk_tokens = tokens[0, i:i + max_length]\n",
    "            chunk_texts.append(tokenizer.decode(chunk_tokens, skip_special_tokens=True))\n",
    "        \n",
    "        # Run the pipeline on the list of chunks.\n",
    "        chunk_sentiments = sentiment_pipeline(chunk_texts)\n",
    "        \n",
    "        # Calculate the average sentiment score from all the chunks.\n",
    "        scores = [get_sentiment_score(result) for result in chunk_sentiments]\n",
    "        return sum(scores) / len(scores) if scores else 0.0\n",
    "\n",
    "    except Exception:\n",
    "        # If any error occurs (e.g., empty text), return a neutral score.\n",
    "        return 0.0\n",
    "\n",
    "# --- 2. Initialize Storage for Final Results ---\n",
    "daily_sentiment_dfs = {}\n",
    "\n",
    "if 'company_top_articles' in locals():\n",
    "    print(f\"Starting NLP analysis on {len(company_top_articles)} companies...\")\n",
    "\n",
    "    # --- 3. Iterate Through Each Company's DataFrame (with TQDM) ---\n",
    "    for ticker, df in tqdm(company_top_articles.items(), desc=\"Processing all companies\"):\n",
    "        \n",
    "        if df.empty:\n",
    "            print(f\"  > DataFrame for {ticker} is empty. Skipping.\")\n",
    "            daily_sentiment_dfs[ticker] = pd.DataFrame()\n",
    "            continue\n",
    "\n",
    "        # --- 4. Apply Sentiment Analysis with a Progress Bar ---\n",
    "        # CORRECTED: Set the description for the inner progress bar by re-initializing tqdm.pandas()\n",
    "        tqdm.pandas(desc=f\"Analyzing {ticker} articles\")\n",
    "        \n",
    "        # Now, call .progress_apply() without any extra arguments.\n",
    "        df['sentiment_score'] = df['text'].progress_apply(analyze_sentiment_with_chunking)\n",
    "        \n",
    "        # --- 5. Aggregate to a Daily Time-Series DataFrame ---\n",
    "        \n",
    "        # Dynamically create the aggregation dictionary to include all density columns\n",
    "        agg_dict = {\n",
    "            'article_volume': ('article_volume', 'first'),\n",
    "            'average_news_sentiment': ('sentiment_score', 'mean')\n",
    "        }\n",
    "        \n",
    "        # Find all density columns in the DataFrame and add them to the aggregation.\n",
    "        density_cols = [col for col in df.columns if '_Density' in col]\n",
    "        for col in density_cols:\n",
    "            agg_dict[f'AVG_{col}'] = (col, 'mean')\n",
    "\n",
    "        daily_agg_df = df.groupby('date').agg(**agg_dict).reset_index()\n",
    "\n",
    "        daily_agg_df = daily_agg_df.rename(columns={'date': 'Period'})\n",
    "        \n",
    "        print(f\"\\n  > Aggregation for {ticker} complete. Created daily time-series with {len(daily_agg_df)} unique days.\")\n",
    "        daily_sentiment_dfs[ticker] = daily_agg_df\n",
    "\n",
    "    print(\"\\n--- All NLP Processing and Aggregation Complete ---\")\n",
    "    \n",
    "    # --- 6. Display a Sample of the Final Output ---\n",
    "    print(\"\\n--- Sample of Final Daily Sentiment DataFrame for 'AAPL' ---\")\n",
    "    if 'AAPL' in daily_sentiment_dfs and not daily_sentiment_dfs['AAPL'].empty:\n",
    "        display(daily_sentiment_dfs['AAPL'].head())\n",
    "\n",
    "else:\n",
    "    print(\"ERROR: 'company_top_articles' dictionary not found. Please run the previous cells first.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f349f844",
   "metadata": {},
   "source": [
    "#### Now the dataframe has composte sentiment scores for each day and for each company.\n",
    "We just want to fill in missing values via a simple `backward_fill` and `forward_fill` imputation strategy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6b19232b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.1.10: Impute Missing Values in Daily Company Sentiment\n",
      "------------------------------\n",
      "Starting imputation for company-specific sentiment data...\n",
      "  > Processing AAPL...\n",
      "    - Imputation complete. Rows changed from 2336 to 2557.\n",
      "  > Processing NVDA...\n",
      "    - Imputation complete. Rows changed from 2117 to 2557.\n",
      "  > Processing GOOGL...\n",
      "    - Imputation complete. Rows changed from 2273 to 2557.\n",
      "\n",
      "Imputation process complete for all tickers.\n",
      "\n",
      "--- Sample of Imputed 'NVDA' DataFrame (showing start of 2018) ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\totob\\AppData\\Local\\Temp\\ipykernel_27536\\3321305823.py:32: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  imputed_df['article_volume'].fillna(0, inplace=True)\n",
      "C:\\Users\\totob\\AppData\\Local\\Temp\\ipykernel_27536\\3321305823.py:32: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  imputed_df['article_volume'].fillna(0, inplace=True)\n",
      "C:\\Users\\totob\\AppData\\Local\\Temp\\ipykernel_27536\\3321305823.py:32: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  imputed_df['article_volume'].fillna(0, inplace=True)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Period</th>\n",
       "      <th>article_volume</th>\n",
       "      <th>average_news_sentiment</th>\n",
       "      <th>AVG_AAPL_Density</th>\n",
       "      <th>AVG_NVDA_Density</th>\n",
       "      <th>AVG_GOOGL_Density</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2018-01-01</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.062500</td>\n",
       "      <td>0.062500</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2018-01-02</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000997</td>\n",
       "      <td>0.041338</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2018-01-03</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0.234918</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.071479</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2018-01-04</td>\n",
       "      <td>13.0</td>\n",
       "      <td>-0.065777</td>\n",
       "      <td>0.005405</td>\n",
       "      <td>0.041763</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2018-01-05</td>\n",
       "      <td>9.0</td>\n",
       "      <td>0.375672</td>\n",
       "      <td>0.012500</td>\n",
       "      <td>0.071191</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Period  article_volume  average_news_sentiment  AVG_AAPL_Density  \\\n",
       "0 2018-01-01             1.0                0.000000          0.062500   \n",
       "1 2018-01-02             7.0                0.000000          0.000997   \n",
       "2 2018-01-03            10.0                0.234918          0.000000   \n",
       "3 2018-01-04            13.0               -0.065777          0.005405   \n",
       "4 2018-01-05             9.0                0.375672          0.012500   \n",
       "\n",
       "   AVG_NVDA_Density  AVG_GOOGL_Density  \n",
       "0          0.062500                0.0  \n",
       "1          0.041338                0.0  \n",
       "2          0.071479                0.0  \n",
       "3          0.041763                0.0  \n",
       "4          0.071191                0.0  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(\"3.1.10: Impute Missing Values in Daily Company Sentiment\")\n",
    "print(\"-\"*30)\n",
    "\n",
    "if 'daily_sentiment_dfs' in locals() and isinstance(daily_sentiment_dfs, dict):\n",
    "\n",
    "    # Create the full date range for the entire analysis period\n",
    "    full_date_range = pd.date_range(start=f'{START_YEAR}-01-01', end=f'{END_YEAR}-12-31', freq='D')\n",
    "    \n",
    "    imputed_sentiment_dfs = {}\n",
    "    \n",
    "    print(\"Starting imputation for company-specific sentiment data...\")\n",
    "\n",
    "    for ticker, df in daily_sentiment_dfs.items():\n",
    "        print(f\"  > Processing {ticker}...\")\n",
    "        \n",
    "        if df.empty:\n",
    "            print(f\"    - DataFrame for {ticker} is empty. Skipping.\")\n",
    "            imputed_sentiment_dfs[ticker] = pd.DataFrame()\n",
    "            continue\n",
    "        \n",
    "        # Ensure 'Period' is a datetime object and set it as the index\n",
    "        df['Period'] = pd.to_datetime(df['Period'])\n",
    "        df = df.set_index('Period')\n",
    "        \n",
    "        original_rows = len(df)\n",
    "        \n",
    "        # Reindex the DataFrame to include all days in the range, creating NaNs for missing days\n",
    "        imputed_df = df.reindex(full_date_range)\n",
    "        \n",
    "        # --- Imputation Strategy ---\n",
    "        # 1. For 'article_volume', missing days correctly mean 0 articles were published.\n",
    "        imputed_df['article_volume'].fillna(0, inplace=True)\n",
    "        \n",
    "        # 2. For sentiment and density scores, the last known value persists.\n",
    "        # We forward-fill first, then back-fill any remaining NaNs at the very start of the series.\n",
    "        sentiment_cols = [col for col in imputed_df.columns if col != 'article_volume']\n",
    "        imputed_df[sentiment_cols] = imputed_df[sentiment_cols].ffill().bfill()\n",
    "        \n",
    "        # Reset the index to bring 'Period' back as a column\n",
    "        imputed_df = imputed_df.reset_index().rename(columns={'index': 'Period'})\n",
    "        \n",
    "        imputed_sentiment_dfs[ticker] = imputed_df\n",
    "        \n",
    "        print(f\"    - Imputation complete. Rows changed from {original_rows} to {len(imputed_df)}.\")\n",
    "\n",
    "    # Overwrite the old dictionary with the new one containing the complete, imputed data\n",
    "    daily_sentiment_dfs = imputed_sentiment_dfs\n",
    "    \n",
    "    print(\"\\nImputation process complete for all tickers.\")\n",
    "    \n",
    "    # Display a sample to verify the imputation\n",
    "    print(\"\\n--- Sample of Imputed 'NVDA' DataFrame (showing start of 2018) ---\")\n",
    "    if 'NVDA' in daily_sentiment_dfs and not daily_sentiment_dfs['NVDA'].empty:\n",
    "        display(daily_sentiment_dfs['NVDA'].head())\n",
    "\n",
    "else:\n",
    "    print(\"ERROR: 'daily_sentiment_dfs' dictionary not found. Please run cell 3.1.9 first.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3e454d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.1.10: Save Daily Sentiment DataFrames to a File\n",
      "------------------------------\n",
      "Saving the 'daily_sentiment_dfs' dictionary to 'Data\\daily_sentiment_dfs.pkl'...\n",
      "This file will serve as a checkpoint to avoid re-running the NLP analysis.\n",
      "\n",
      "Dictionary successfully saved to 'Data\\daily_sentiment_dfs.pkl'.\n",
      "\n",
      "To load this data in a future session, you can use the following code:\n",
      "------------------------------------------------------------------\n",
      "import pickle\n",
      "with open('Data\\daily_sentiment_dfs.pkl', 'rb') as f:\n",
      "    daily_sentiment_dfs = pickle.load(f)\n",
      "------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "print(\"3.1.11: Save Daily Sentiment DataFrames to a File\")\n",
    "print(\"-\"*30)\n",
    "\n",
    "import pickle\n",
    "import os\n",
    "\n",
    "# Check if the dictionary from the previous cell exists.\n",
    "if 'daily_sentiment_dfs' in locals() and isinstance(daily_sentiment_dfs, dict):\n",
    "    \n",
    "    # Define the output directory and filename.\n",
    "    output_dir = \"Data\"\n",
    "    output_filename = \"daily_sentiment_dfs.pkl\"\n",
    "    output_path = os.path.join(output_dir, output_filename)\n",
    "    \n",
    "    # Ensure the output directory exists.\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    print(f\"Saving the 'daily_sentiment_dfs' dictionary to '{output_path}'...\")\n",
    "    print(\"This file will serve as a checkpoint to avoid re-running the NLP analysis.\")\n",
    "\n",
    "    try:\n",
    "        # Open the file in write-binary ('wb') mode.\n",
    "        with open(output_path, 'wb') as f:\n",
    "            # Use pickle.dump() to serialize the entire dictionary object into the file.\n",
    "            pickle.dump(daily_sentiment_dfs, f)\n",
    "        \n",
    "        print(f\"\\nDictionary successfully saved to '{output_path}'.\")\n",
    "        \n",
    "        # Provide a snippet for how to load the data in a future session.\n",
    "        print(\"\\nTo load this data in a future session, you can use the following code:\")\n",
    "        print(\"------------------------------------------------------------------\")\n",
    "        print(\"import pickle\")\n",
    "        print(f\"with open('{output_path}', 'rb') as f:\")\n",
    "        print(\"    daily_sentiment_dfs = pickle.load(f)\")\n",
    "        print(\"------------------------------------------------------------------\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"\\nAn error occurred while saving the dictionary: {e}\")\n",
    "else:\n",
    "    print(\"ERROR: 'daily_sentiment_dfs' dictionary not found. Please run cell 3.1.9 first.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6adb6e92",
   "metadata": {},
   "source": [
    "### 3.2: NLP Pipeline for `market_headlines_df`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f3a14344",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.2.0: Setup and Load Market Headlines Data\n",
      "------------------------------\n",
      "Using 'market_headlines_df' from the current session.\n",
      "\n",
      "Loaded 13,422 total headlines for the period 2018-2024.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.2.1 -> 25.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "print(\"3.2.0: Setup and Load Market Headlines Data\")\n",
    "print(\"-\"*30)\n",
    "\n",
    "import sys\n",
    "# Ensure langdetect is installed for language filtering\n",
    "!{sys.executable} -m pip install langdetect --quiet\n",
    "\n",
    "import pandas as pd\n",
    "import os\n",
    "from langdetect import detect, LangDetectException\n",
    "\n",
    "# Define the file path for the market headlines CSV\n",
    "MARKET_HEADLINES_FILE = os.path.join(\"Data\", \"sp500_headlines_2008_2024.csv\")\n",
    "\n",
    "# --- Load the Dataset ---\n",
    "# Prioritize using the DataFrame if it's already in memory from cell 2.3.1.\n",
    "if 'market_headlines_df' in locals() and isinstance(market_headlines_df, pd.DataFrame):\n",
    "    print(\"Using 'market_headlines_df' from the current session.\")\n",
    "    # Create a copy to avoid modifying the original DataFrame\n",
    "    headlines_df = market_headlines_df.copy()\n",
    "else:\n",
    "    # Fallback to loading from the CSV file\n",
    "    print(f\"Loading market headlines from '{MARKET_HEADLINES_FILE}'...\")\n",
    "    try:\n",
    "        headlines_df = pd.read_csv(MARKET_HEADLINES_FILE)\n",
    "        # Ensure date column is in the correct format if loading fresh\n",
    "        headlines_df['Date'] = pd.to_datetime(headlines_df['Date'])\n",
    "        print(\"DataFrame loaded successfully.\")\n",
    "    except FileNotFoundError:\n",
    "        print(f\"ERROR: File not found at '{MARKET_HEADLINES_FILE}'. Please run cell 2.3.1 first.\")\n",
    "        headlines_df = pd.DataFrame() # Create empty DF to prevent downstream errors\n",
    "\n",
    "if not headlines_df.empty:\n",
    "    print(f\"\\nLoaded {len(headlines_df):,} total headlines for the period {START_YEAR}-{END_YEAR}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f49d2f41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.2.1: Filter for English-Language Headlines\n",
      "------------------------------\n",
      "Filtering for English headlines. This may take a moment...\n",
      "Filtering complete.\n",
      "  > Original headline count: 13,422\n",
      "  > English headline count:  12,990 (96.78%)\n"
     ]
    }
   ],
   "source": [
    "print(\"3.2.1: Filter for English-Language Headlines\")\n",
    "print(\"-\"*30)\n",
    "\n",
    "if not 'headlines_df' in locals() or headlines_df.empty:\n",
    "    print(\"ERROR: 'headlines_df' not found or is empty. Skipping filtering.\")\n",
    "else:\n",
    "    # This function checks if a given text is English.\n",
    "    def is_english(text):\n",
    "        try:\n",
    "            # Check for valid string input for first 100 characters\n",
    "            return isinstance(text, str) and detect(text[:100]) == 'en'\n",
    "        except LangDetectException:\n",
    "            # If detection fails, assume it's not the language we want.\n",
    "            return False\n",
    "\n",
    "    print(\"Filtering for English headlines. This may take a moment...\")\n",
    "    # Apply the language filter to the 'Title' column\n",
    "    english_mask = headlines_df['Title'].apply(is_english)\n",
    "    english_market_headlines_df = headlines_df[english_mask]\n",
    "\n",
    "    original_count = len(headlines_df)\n",
    "    filtered_count = len(english_market_headlines_df)\n",
    "    print(f\"Filtering complete.\")\n",
    "    print(f\"  > Original headline count: {original_count:,}\")\n",
    "    print(f\"  > English headline count:  {filtered_count:,} ({filtered_count/original_count:.2%})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "34b1199c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.2.2: Analyze Data for Missing Days\n",
      "------------------------------\n",
      "Analyzing headline coverage from 2018 to 2024 (2557 days total).\n",
      "  > Unique days with at least one headline: 1,523\n",
      "  > Days with NO headlines (to be imputed): 1,034\n",
      "\n",
      "Missing days detected. Imputation will be required in a later step.\n"
     ]
    }
   ],
   "source": [
    "print(\"3.2.2: Analyze Data for Missing Days\")\n",
    "print(\"-\"*30)\n",
    "\n",
    "if 'english_market_headlines_df' in locals() and not english_market_headlines_df.empty:\n",
    "    # Create a complete date range for our analysis period\n",
    "    full_date_range = pd.date_range(start=f'{START_YEAR}-01-01', end=f'{END_YEAR}-12-31', freq='D')\n",
    "    \n",
    "    # Get the unique days that have at least one headline\n",
    "    unique_days_with_headlines = english_market_headlines_df['Date'].nunique()\n",
    "    total_days_in_period = len(full_date_range)\n",
    "    missing_days = total_days_in_period - unique_days_with_headlines\n",
    "    \n",
    "    print(f\"Analyzing headline coverage from {START_YEAR} to {END_YEAR} ({total_days_in_period} days total).\")\n",
    "    print(f\"  > Unique days with at least one headline: {unique_days_with_headlines:,}\")\n",
    "    print(f\"  > Days with NO headlines (to be imputed): {missing_days:,}\")\n",
    "\n",
    "    if missing_days > 0:\n",
    "        print(\"\\nMissing days detected. Imputation will be required in a later step.\")\n",
    "    else:\n",
    "        print(\"\\nComplete daily coverage found. No imputation needed.\")\n",
    "else:\n",
    "    print(\"ERROR: 'english_market_headlines_df' not found. Cannot perform analysis.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c805497e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.2.3: Calculate and Aggregate Daily Market Sentiment\n",
      "------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Analyzing market headlines: 100%|██████████| 12990/12990 [02:24<00:00, 90.03it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Aggregating sentiment scores into a daily average...\n",
      "Aggregation complete.\n",
      "\n",
      "--- Sample of Daily Market Sentiment ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "C:\\Users\\totob\\AppData\\Local\\Temp\\ipykernel_27536\\3989239549.py:12: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  english_market_headlines_df['sentiment_score'] = english_market_headlines_df['Title'].progress_apply(analyze_sentiment_with_chunking)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Period</th>\n",
       "      <th>market_average_sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2018-01-02</td>\n",
       "      <td>0.311223</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2018-01-03</td>\n",
       "      <td>-0.018505</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2018-01-04</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2018-01-05</td>\n",
       "      <td>0.071511</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2018-01-08</td>\n",
       "      <td>0.152372</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Period  market_average_sentiment\n",
       "0 2018-01-02                  0.311223\n",
       "1 2018-01-03                 -0.018505\n",
       "2 2018-01-04                  0.000000\n",
       "3 2018-01-05                  0.071511\n",
       "4 2018-01-08                  0.152372"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(\"3.2.3: Calculate and Aggregate Daily Market Sentiment\")\n",
    "print(\"-\"*30)\n",
    "\n",
    "if 'english_market_headlines_df' in locals() and not english_market_headlines_df.empty:\n",
    "    \n",
    "    # --- 1. Calculate Sentiment for Each Headline ---\n",
    "    # Register tqdm with pandas and set the description for the progress bar.\n",
    "    tqdm.pandas(desc=\"Analyzing market headlines\")\n",
    "    \n",
    "    # Since headlines are short, the chunking function will process them quickly.\n",
    "    # We re-use it for consistency with the previous NLP task.\n",
    "    english_market_headlines_df['sentiment_score'] = english_market_headlines_df['Title'].progress_apply(analyze_sentiment_with_chunking)\n",
    "    \n",
    "    # --- 2. Aggregate to a Daily Time-Series ---\n",
    "    print(\"\\nAggregating sentiment scores into a daily average...\")\n",
    "    daily_market_sentiment_df = english_market_headlines_df.groupby('Date').agg(\n",
    "        market_average_sentiment=('sentiment_score', 'mean')\n",
    "    ).reset_index()\n",
    "    \n",
    "    # Rename 'Date' to 'Period' for consistency\n",
    "    daily_market_sentiment_df = daily_market_sentiment_df.rename(columns={'Date': 'Period'})\n",
    "\n",
    "    print(\"Aggregation complete.\")\n",
    "    print(\"\\n--- Sample of Daily Market Sentiment ---\")\n",
    "    display(daily_market_sentiment_df.head())\n",
    "    \n",
    "else:\n",
    "    print(\"ERROR: 'english_market_headlines_df' is not available. Skipping sentiment analysis.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8e40e8c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.2.4: Impute Missing Sentiment Values\n",
      "------------------------------\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "\"None of ['Period'] are in the columns\"",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[32m~\\AppData\\Local\\Temp\\ipykernel_27536\\3386222792.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m      3\u001b[39m \n\u001b[32m      4\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m'daily_market_sentiment_df'\u001b[39m \u001b[38;5;28;01min\u001b[39;00m locals() \u001b[38;5;28;01mand\u001b[39;00m \u001b[38;5;28;01mnot\u001b[39;00m daily_market_sentiment_df.empty:\n\u001b[32m      5\u001b[39m \n\u001b[32m      6\u001b[39m     \u001b[38;5;66;03m# Set 'Period' as the index to perform time-series operations\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m7\u001b[39m     daily_market_sentiment_df = daily_market_sentiment_df.set_index(\u001b[33m'Period'\u001b[39m)\n\u001b[32m      8\u001b[39m \n\u001b[32m      9\u001b[39m     \u001b[38;5;66;03m# Create the full daily date range again\u001b[39;00m\n\u001b[32m     10\u001b[39m     full_date_range = pd.date_range(start=f'{START_YEAR}-01-01', end=f'{END_YEAR}-12-31', freq=\u001b[33m'D'\u001b[39m)\n",
      "\u001b[32mc:\\Users\\totob\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\core\\frame.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(self, keys, drop, append, inplace, verify_integrity)\u001b[39m\n\u001b[32m   6140\u001b[39m                     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28;01mnot\u001b[39;00m found:\n\u001b[32m   6141\u001b[39m                         missing.append(col)\n\u001b[32m   6142\u001b[39m \n\u001b[32m   6143\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m missing:\n\u001b[32m-> \u001b[39m\u001b[32m6144\u001b[39m             \u001b[38;5;28;01mraise\u001b[39;00m KeyError(f\"None of {missing} are in the columns\")\n\u001b[32m   6145\u001b[39m \n\u001b[32m   6146\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m inplace:\n\u001b[32m   6147\u001b[39m             frame = self\n",
      "\u001b[31mKeyError\u001b[39m: \"None of ['Period'] are in the columns\""
     ]
    }
   ],
   "source": [
    "print(\"3.2.4: Impute Missing Sentiment Values\")\n",
    "print(\"-\"*30)\n",
    "\n",
    "if 'daily_market_sentiment_df' in locals() and not daily_market_sentiment_df.empty:\n",
    "    \n",
    "    # Set 'Period' as the index to perform time-series operations\n",
    "    daily_market_sentiment_df = daily_market_sentiment_df.set_index('Period')\n",
    "    \n",
    "    # Create the full daily date range again\n",
    "    full_date_range = pd.date_range(start=f'{START_YEAR}-01-01', end=f'{END_YEAR}-12-31', freq='D')\n",
    "    \n",
    "    # Reindex the DataFrame to include all days in the range\n",
    "    imputed_market_sentiment_df = daily_market_sentiment_df.reindex(full_date_range)\n",
    "    \n",
    "    # Use forward-fill to propagate the last known sentiment to missing days\n",
    "    imputed_market_sentiment_df['market_average_sentiment'].ffill(inplace=True)\n",
    "\n",
    "    # For any remaining NaNs at the beginning of the series, back-fill them\n",
    "    imputed_market_sentiment_df['market_average_sentiment'].bfill(inplace=True)\n",
    "    \n",
    "    imputed_market_sentiment_df = imputed_market_sentiment_df.reset_index().rename(columns={'index': 'Period'})\n",
    "\n",
    "    print(f\"Imputation complete. DataFrame now contains {len(imputed_market_sentiment_df)} days.\")\n",
    "    print(\"\\n--- Sample of Final Imputed Market Sentiment ---\")\n",
    "    display(imputed_market_sentiment_df.head())\n",
    "\n",
    "else:\n",
    "    print(\"ERROR: 'daily_market_sentiment_df' is not available. Skipping imputation.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "39ebe4c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.2.5: Save Final Market Sentiment DataFrame\n",
      "------------------------------\n",
      "Saving the final market sentiment DataFrame to 'Data\\market_sentiment_df.pkl'...\n",
      "\n",
      "DataFrame successfully saved to 'Data\\market_sentiment_df.pkl'.\n"
     ]
    }
   ],
   "source": [
    "print(\"3.2.5: Save Final Market Sentiment DataFrame\")\n",
    "print(\"-\"*30)\n",
    "\n",
    "import pickle\n",
    "import os\n",
    "\n",
    "if 'imputed_market_sentiment_df' in locals() and not imputed_market_sentiment_df.empty:\n",
    "    \n",
    "    output_dir = \"Data\"\n",
    "    output_filename = \"market_sentiment_df.pkl\"\n",
    "    output_path = os.path.join(output_dir, output_filename)\n",
    "    \n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    print(f\"Saving the final market sentiment DataFrame to '{output_path}'...\")\n",
    "\n",
    "    try:\n",
    "        with open(output_path, 'wb') as f:\n",
    "            pickle.dump(imputed_market_sentiment_df, f)\n",
    "        \n",
    "        print(f\"\\nDataFrame successfully saved to '{output_path}'.\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"\\nAn error occurred while saving the DataFrame: {e}\")\n",
    "else:\n",
    "    print(\"ERROR: 'imputed_market_sentiment_df' not found. Nothing to save.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe84931a",
   "metadata": {},
   "source": [
    "### 3.3: NLP Pipeline for `filings_df`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42d1d406",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
